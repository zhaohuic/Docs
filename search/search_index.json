{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPC","text":"<p>Welcome to HPC at NJIT.</p> <p>NJIT provides High Performance Computing resources to support scientific computing for faculty and students. These resources include CPU nodes, GPU nodes, parallel storage, high speed, low latency Infiniband networking and a fully optimized scientific software stack.</p>"},{"location":"#hpc-highlights","title":"HPC Highlights","text":"<p>Contact Us</p> <p>To create a ticket or request for software installation, visit Contact Us.</p> <p>Clusters</p> <p>See the list of availble clusters, their specifications, and how to access them in Clusters.</p> <p>Software Modules</p> <p>See Software Modules for list of software packages installed on our cluster.</p> <p>Lochness to Wulver Migration</p> <p>For details in migrating the Lochness users to Wulver, please visit Migration.</p> <p>Cluster Maintenance Updates</p> <p>To see the latest maintenance updates on NJIT cluster, please visit Cluster Maintenance.</p>"},{"location":"#hpc-latest-news","title":"HPC latest News!","text":"<p>Wulver Scheduled Maintenance</p> <p>Wulver will be out of service for maintenance once a month for updates, repairs, and upgrades.  The schedule is 9 a.m. to 9 p.m. the second Tuesday of every month.  During the maintenance period, the logins will be disabled and the jobs that do not end before the maintenance window begins will be held until the maintenance is complete and the cluster is returned to production. For example, if you submit a job the day before maintenance, your job will enter a pending state (you will see job status <code>PD</code> when using <code>squeue -u $LOGNAME</code>). You can either adjust the walltime or wait until maintenance ends. Please stay informed about maintenance updates at Cluster Maintenance Updates and News.</p>"},{"location":"Policies/lochness_policies/","title":"Lochness User Policies","text":"<p>Access and usages policies for the lochness cluster</p> <ul> <li> <p>Access</p> <ul> <li>Faculty members can request access to lochness for themselves and their students by sending an email to hpc@njit.edu.  There is no charge for using lochness.</li> <li>For courses, send an email to hpc@njit.edu to discuss requirements</li> </ul> </li> <li> <p>Storage</p> <ul> <li>Users are given a quota of 100GB</li> <li>Shared research directory with 500 GB quota is available upon request</li> </ul> </li> <li> <p>Scheduling</p> <ul> <li>The SLURM scheduler uses a \u201cfair share\u201d algorithm to assign priority to jobs. The \u201cfair share\u201d algorithm takes into account factors such as user history, wall time request, etc \u2026 to assign priority. The \u201cfair share\u201d algorithm is configure to favor smaller jobs. </li> <li>Limits<ul> <li>Wall time: 30 Days. Note that shorter wall time receive higher priority.</li> </ul> </li> <li>Cores/RAM: No limit but we request users be considerate. There are limited resources for public access.</li> </ul> </li> <li> <p>Condo</p> <ul> <li>Lochness has a private pool ownership model. Users buy nodes which are then incorporated into the cluster as a private partition.</li> </ul> </li> </ul> <p>Warning</p> <p>As of Jan 16, no new Lochness accounts will be created and Lochness will be decommissioned once the Wulver migration is complete.</p>"},{"location":"Policies/wulver_policies/","title":"Wulver Usage and Condo Policies","text":"<p>Proposed Wulver Usage and Condo Policies</p>"},{"location":"Policies/wulver_policies/#faculty-computing-allowance","title":"Faculty Computing Allowance","text":"<p>Faculty PIs are allocated 300,000 Service Units (SU) per year on request at no cost. An SU is equal to 1 core hour on a standard node. For more details on calculating SUs for GPUs, see Wulver SLURM. All users working as part of the PIs project will use this allocation. Multiple PIs working on the same project may pool SUs. The SUs can be renewed annually by providing a brief report describing how the SUs were used and a list of publications, presentations, and awards generated from research conducted. Additional SUs may be purchased at a cost of 0.005/SU. The minimum purchase is 50,000 SU (500). </p> <p>Note</p> <p>The 300,000 SUs are available on <code>--qos=standard</code> only. If PI does not want to buy more SUs, PI's group members can use <code>--qos=low</code> which does not have any SU charges. For more details, see SLURM QOS </p>"},{"location":"Policies/wulver_policies/#user-storage-allowance","title":"User Storage Allowance","text":"<p>Users will be provided with 50GB of <code>$HOME</code> directories. Home directories are backed up. PIs are additionally provided 2TB project directories. These project directories are backed up. Very fast NVME scratch is available to users. This scratch space is for temporary files generated during a run and will be deleted after 30 days. Additional project storage can be purchased if needed. This additional project space will also be backed up. Users need to manage data so that backed-up data fits in the project directory space. Transient, or rapidly changing data should be stored in the scratch directory. Long-term storage with backups or archival storage for research data will be stored in a yet to be determined campus wide storage resource.</p>"},{"location":"Policies/wulver_policies/#shared-condo-partnership","title":"Shared Condo Partnership","text":"<p>Faculty who routinely need more resources than the initial allocation may buy nodes and contribute to the cluster. A catalog of select hardware for inclusion in the cluster will be made available. The PI and associated users will be able to submit jobs with a higher priority up to the resources contributed. Note that these jobs may or may not run on the actual nodes purchased. The allocated resources will be available via a floating reservation for the amount of resources purchased. Contributors will be able to additionally submit jobs using SUs as well as lower priority. The university will subsidize all infrastructure costs for these nodes. This floating reservation will be available for five years.</p>"},{"location":"Policies/wulver_policies/#private-pool","title":"Private Pool","text":"<p>If the shared condo module does not satisfy the needs of the PI, a private pool may be set up. In addition to the nodes, the PI will be charged for all infrastructure costs, including but not limited to electricity, HVAC, system administration, etc. It is strongly recommended to first try the shared condo model. If the shared condo model does not work, the nodes can be converted to a private pool.</p>"},{"location":"Policies/wulver_policies/#job-priorities","title":"Job Priorities","text":"<ul> <li> <p>Standard Priority</p> <ul> <li>Faculty PIs are allocated 300,000 Service Units (SU) per year on request at no cost</li> <li>Wall time maximum - 72 hours</li> <li>Additional SUs may be purchased at a cost of $0.005/SU</li> <li>The minimum purchase is 50,000 SU ($500)</li> <li>Jobs can be superseded by those with higher priority jobs</li> </ul> </li> <li> <p>Low Priority</p> <ul> <li>Not charged against SU allocation</li> <li>Wall time maximum - 72 hours</li> <li>Jobs can be preempted by those with higher and standard priority jobs when they are in the queue</li> </ul> </li> <li> <p>High Priority</p> <ul> <li>Not charged against SU allocation</li> <li>Wall time: 72 hours (default), PI can request longer walltimes up to 30 days. ARCS HPC reserves the right to reboot nodes once a month for maintenance \u2014 the second Tuesday of each month. See Cluster Maintenance Updates and News for details</li> <li>Only available to contributors</li> </ul> </li> </ul>"},{"location":"Services/hpc-course/","title":"HPC Resources for Courses","text":""},{"location":"Services/hpc-course/#introduction","title":"Introduction","text":"<p>Instructors can utilize High-Performance Computing (HPC) resources for academic courses. Whether faculty are planning a course that involves computationally intensive tasks or introducing students to parallel computing concepts, the HPC environment can offer valuable resources.</p>"},{"location":"Services/hpc-course/#course-details","title":"Course Details","text":"<p>To request HPC resources for course, you need to fill out the form (the link to the form will be provided soon) and provide the following information. </p>"},{"location":"Services/hpc-course/#name-of-course","title":"Name of Course:","text":"<p>[Please provide the name of the course and a short description]</p>"},{"location":"Services/hpc-course/#estimated-number-of-students","title":"Estimated Number of Students:","text":"<p>[Enter the estimated number of students for the course]</p>"},{"location":"Services/hpc-course/#activities-on-hpc","title":"Activities on HPC:","text":"<p>[Describe the specific activities that will involve the use of HPC resources. For example, simulations, data analysis, modeling, etc.]</p>"},{"location":"Services/hpc-course/#software-needed","title":"Software Needed:","text":"<p>[Specify the software required for the course. Check the list of software installed on Wulver in Software. Include any specific versions or software not already available on the HPC cluster] </p> <p>Notice Period</p> <p>A minimum of 30 days' notice is required for requesting specific software installations or substantial resource allocations.</p>"},{"location":"Services/hpc-course/#utilizing-hpc-in-a-course","title":"Utilizing HPC in a Course","text":"<p>HPC resources are good for the courses if they satisfy the following requirements.</p> <ul> <li>Simulations and Modeling: Perform complex simulations and modeling exercises that require significant computational power.</li> <li>Data Analysis: Conduct large-scale data analysis projects, exploring real-world datasets with efficiency.</li> <li>Parallel Computing: Teach parallel computing concepts and applications by leveraging the cluster's parallel processing capabilities.</li> <li>Optimization Problems: Solve optimization problems that benefit from parallel processing and distributed computing.</li> <li>Scientific Research Projects: Enable students to work on scientific research projects that demand high-performance computing resources.</li> </ul>"},{"location":"Services/hpc-course/#hpc-introduction","title":"HPC Introduction","text":"<p>The HPC Facilitator is available to provide an introduction to High-Performance Computing. This introduction can be conducted in person or online based on the preferences and requirements of the course. The session covers:</p> <ul> <li>Overview of HPC concepts</li> <li>Accessing and navigating the HPC cluster</li> <li>Basic job submission and monitoring</li> <li>Filesystems on HPC cluster</li> <li>Conclusion</li> </ul> <p>By incorporating HPC resources into your course, you provide students with the opportunity to engage in hands-on, real-world applications of computational concepts. The HPC environment enhances the learning experience and prepares students for challenges in data-driven and computationally intensive fields. For specific requests or to schedule an HPC introduction session, please contact the HPC Facilitator.</p>"},{"location":"Software/","title":"Software Environment","text":"<p>All software and numerical libraries available at the cluster can be found at <code>/opt/site/easybuild/software/</code> if you are using Lochness. In case of Wulver the applications are installed at <code>/apps/easybuild/software/</code>. We use EasyBuild to install, build and manage different version of packages. </p> <p>Note</p> <p>If you are using Lochness, add the following in <code>.modules</code> file located in <code>$HOME</code> directory <pre><code>module use /opt/site/easybuild/modules/all/MPI\nmodule use /opt/site/easybuild/modules/all/Core\nmodule use /opt/site/easybuild/modules/all/Compiler\n</code></pre></p> <p>If you could not find software or libraries on HPC cluster, please submit a request for HPC Software Installation by visiting the Service Catalog. The list of installed software or packages on HPC cluster can be found in Software List.</p>"},{"location":"Software/#modules","title":"Modules","text":"<p>We use Environment Modules to manage the user environment in HPC, which help users to easily load and unload software packages, switch between different versions of software, and manage complex software dependencies. Lmod is an extension of the Environment Modules system, implemented in Lua. It enhances the functionality of traditional Environment Modules by introducing features such as hierarchical module naming, module caching, and improved flexibility in managing environment variables.</p>"},{"location":"Software/#access-to-all-available-modules","title":"Access to All Available Modules","text":"<p>The list of software and libraries installed on Lochness can be accessed by using the following command</p> <pre><code>module av\n</code></pre>"},{"location":"Software/#search-for-specific-package","title":"Search for Specific Package","text":"<p>You can check specific packages and list of their versions using <code>module av</code>. For example, the list of different versions of Python installed on cluster can be checked by using <pre><code>module av Python\n</code></pre> The above command gives the following output <pre><code>------------------------------------------------------------------------------------------------------------------- /opt/site/easybuild/modules/all/Compiler -------------------------------------------------------------------------------------------------------------------\n   GCCcore/10.2.0/Python/2.7.18             GCCcore/11.2.0/IPython/7.26.0                GCCcore/8.3.0/Meson/0.51.2-Python-3.7.4           GCCcore/9.3.0/Flask/1.1.2-Python-3.8.2                         GCCcore/9.3.0/archspec/0.1.0-Python-3.8.2\n   GCCcore/10.2.0/Python/3.8.6       (D)    GCCcore/11.2.0/Python/2.7.18-bare            GCCcore/8.3.0/Meson/0.59.1-Python-3.7.4    (D)    GCCcore/9.3.0/GObject-Introspection/1.64.0-Python-3.8.2        GCCcore/9.3.0/pkgconfig/1.5.1-Python-3.8.2\n   GCCcore/10.3.0/Python/2.7.18-bare        GCCcore/11.2.0/Python/3.9.6-bare             GCCcore/8.3.0/Python/2.7.16                       GCCcore/9.3.0/Meson/0.55.1-Python-3.8.2                        GCCcore/9.3.0/pybind11/2.4.3-Python-3.8.2\n   GCCcore/10.3.0/Python/3.9.5-bare         GCCcore/11.2.0/Python/3.9.6           (D)    GCCcore/8.3.0/Python/3.7.4                 (D)    GCCcore/9.3.0/Python/2.7.18\n   GCCcore/10.3.0/Python/3.9.5       (D)    GCCcore/11.2.0/protobuf-python/3.17.3        GCCcore/8.3.0/pkgconfig/1.5.1-Python-3.7.4        GCCcore/9.3.0/Python/3.8.2                              (D)\n</code></pre></p> <p>Note</p> <p>The above display message is for Lochness only. In Wulver, most of the appications are built based on the toolchain. For more details see Toolchain. Therefore to see available software, you need to load the toolchain first. </p> <p>To see how to load the modules (for example <code>Python/3.9.6</code>) the following command needs to used. <pre><code>module spider Python/3.9.6\n</code></pre> This will show the which prerequisite modules need to loaded prior to loading <code>Python</code></p> <p><pre><code>----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Python: Python/3.9.6\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n\n    You will need to load all module(s) on any one of the lines below before the \"Python/3.9.6\" module is available to load.\n\n      Core/GCCcore/11.2.0\n      GCCcore/11.2.0\n\n    Help:\n\n      Description\n      ===========\n      Python is a programming language that lets you work more quickly and integrate your systems\n       more effectively.\n\n\n      More information\n      ================\n       - Homepage: https://python.org/\n</code></pre> If you are unsure about the version, you can also use <code>module spider Python</code> to see the different versions of Python and prerequisite modules to be loaded. </p>"},{"location":"Software/#load-modules","title":"Load Modules","text":"<p>To use specific package, you need to use <code>module load</code> command which modified the environment to load the software package(s).</p> <p>Note</p> <ul> <li>The <code>module load</code> command will load dependencies automatically as needed, however you may still need to load prerequisite modules to load specific software package(s). For that you need to use <code>module spider</code> command as described above.</li> <li>For running jobs via batch script, you need to add module load command(s) to your submission script.</li> </ul> <p>For example, to load <code>Python</code> version <code>3.9.6</code> as shown in the above example, you need to load <code>GCCcore/.11.2.0</code> module first before loading the Python module is available to load. To use <code>Python 3.9.6</code>, use the following command <pre><code>module load GCCcore/11.2.0 Python\n</code></pre> You can verify whether Python is loaded using</p> <p><pre><code>module li\n</code></pre> and this will result is the following output <pre><code>Currently Loaded Modules:\n  1) GCCcore/11.2.0 (H)   3) binutils/2.37 (H)   5) ncurses/6.2     (H)   7) Tcl/8.6.11    9) XZ/.5.2.5  (H)  11) libffi/3.4.2 (H)  13) Python/3.9.6\n  2) zlib/1.2.11    (H)   4) bzip2/1.0.8   (H)   6) libreadline/8.1 (H)   8) SQLite/3.36  10) GMP/.6.2.1 (H)  12) OpenSSL/1.1  (H)\n\n  Where:\n   H:  Hidden Module\n</code></pre></p>"},{"location":"Software/#module-unload","title":"Module unload","text":"<p>To unload a specific module that you've previously loaded: <pre><code>module unload Python\n</code></pre> You can unload all modules at once with <pre><code>module purge\n</code></pre></p>"},{"location":"Software/#module-save-collections","title":"Module Save Collections","text":"<p>Since some package(s) require to load prerequisite modules to load, every time it might be inconvenient to users to load those modules everytime. Therefore, you can save those modules in particular environment after loading those modules. For example <pre><code>module save environment_name\n</code></pre> This will save the collection of  modules in <code>environment_name</code>. </p>"},{"location":"Software/#module-list-collections","title":"Module List Collections","text":"<p>To get a list of your collections, run: <pre><code>module savelist\n</code></pre></p>"},{"location":"Software/#module-command-summary","title":"Module Command Summary","text":"<p>Here are the list of <code>module</code> commands</p> Command Description <code>module list</code> Show active modules loaded in the user environment <code>module av [module]</code> Show list of available modules in MODULEPATH <code>module spider [module]</code> Query all modules in MODULEPATH and any module hierarchy <code>module overview [module]</code> List all modules with count of each module <code>module load [module]</code> Load a module file in the users environment <code>module unload [module]</code> Remove a loaded module from the user environment <code>module purge</code> Remove all modules from the user environment <code>module swap [module1] [module2]</code> Replace module1 with module2 <code>module show [module]</code> Show content of commands performed by loading module file <code>module --raw show [module]</code> Show raw content of module file <code>module help [module]</code> Show help for a given module <code>module whatis [module]</code> A brief description of the module, generally single line <code>module savelist</code> List all user collections <code>module save [collection]</code> Save active modules in a user collection <code>module describe [collection]</code> Show content of user collection <code>module restore [collection]</code> Load modules from a collection <code>module disable [collection]</code> Disable a user collection <code>module --config</code> Show Lmod configuration <code>module use [-a] [path]</code> Prepend or Append path to MODULEPATH <code>module unuse [path]</code> Remove path from MODULEPATH <code>module --show_hidden av</code> Show all available modules in MODULEPATH including hidden modules <code>module --show_hidden spider</code> Show all possible modules in MODULEPATH and module hierarchy including hidden modules"},{"location":"Software/#further-reading","title":"Further Reading","text":"<p>You can check the documentation of <code>module</code> on the cluster using the following command: <pre><code>man module\n</code></pre></p>"},{"location":"Software/#software-list","title":"Software List","text":"<p>The following applications are installed on Wulver and Lochness.</p> WulverLochness <p> Software Version Dependent Toolchain Module Load Command zlib 1.2.13 - <code>module load zlib/1.2.13</code> zlib 1.2.12 - <code>module load zlib/1.2.12</code> zlib 1.2.11 - <code>module load zlib/1.2.11</code> ffnvcodec 12.1.14.0 - <code>module load ffnvcodec/12.1.14.0</code> ffnvcodec 11.1.5.2 - <code>module load ffnvcodec/11.1.5.2</code> M4 1.4.18 - <code>module load M4/1.4.18</code> M4 1.4.19 - <code>module load M4/1.4.19</code> CUDA 12.0.0 - <code>module load CUDA/12.0.0</code> CUDA 11.4.1 - <code>module load CUDA/11.4.1</code> gettext 0.21 - <code>module load gettext/0.21</code> gettext 0.21.1 - <code>module load gettext/0.21.1</code> gettext 0.22 - <code>module load gettext/0.22</code> NVHPC 23.1-CUDA-12.0.0 - <code>module load NVHPC/23.1-CUDA-12.0.0</code> OptiX 6.5.0 - <code>module load OptiX/6.5.0</code> Autoconf 2.71 - <code>module load Autoconf/2.71</code> Bison 3.8.2 - <code>module load Bison/3.8.2</code> Autotools 20220317 - <code>module load Autotools/20220317</code> apptainer 1.1.9 - <code>module load apptainer/1.1.9</code> intel 2022b - <code>module load intel/2022b</code> intel 2022a - <code>module load intel/2022a</code> intel 2021b - <code>module load intel/2021b</code> ParaView 5.11.2-egl - <code>module load ParaView/5.11.2-egl</code> ParaView 5.11.0-osmesa - <code>module load ParaView/5.11.0-osmesa</code> ParaView 5.11.0-egl - <code>module load ParaView/5.11.0-egl</code> Mamba 4.14.0-0 - <code>module load Mamba/4.14.0-0</code> PyCharm 2022.3.2 - <code>module load PyCharm/2022.3.2</code> DMTCP 3.0.0 - <code>module load DMTCP/3.0.0</code> DMTCP 2.6.0 - <code>module load DMTCP/2.6.0</code> flex 2.6.4 - <code>module load flex/2.6.4</code> foss 2022b - <code>module load foss/2022b</code> foss 2023b - <code>module load foss/2023b</code> foss 2021b - <code>module load foss/2021b</code> EasyBuild 4.8.2 - <code>module load EasyBuild/4.8.2</code> EasyBuild 4.9.0 - <code>module load EasyBuild/4.9.0</code> EasyBuild 4.8.0 - <code>module load EasyBuild/4.8.0</code> Avogadro2 1.97.0-linux-x86_64 - <code>module load Avogadro2/1.97.0-linux-x86_64</code> imkl 2022.1.0 - <code>module load imkl/2022.1.0</code> imkl 2022.2.1 - <code>module load imkl/2022.2.1</code> imkl 2021.4.0 - <code>module load imkl/2021.4.0</code> Automake 1.16.5 - <code>module load Automake/1.16.5</code> OpenSSL 1.1 - <code>module load OpenSSL/1.1</code> libtool 2.4.7 - <code>module load libtool/2.4.7</code> Gaussian 16.C.01-AVX2~ - <code>module load Gaussian/16.C.01-AVX2~</code> Gaussian 16.C.01-AVX2 - <code>module load Gaussian/16.C.01-AVX2</code> Spack 0.17.2 - <code>module load Spack/0.17.2</code> Spack 0.20.0 - <code>module load Spack/0.20.0</code> binutils 2.36.1 - <code>module load binutils/2.36.1</code> binutils 2.37 - <code>module load binutils/2.37</code> binutils 2.40 - <code>module load binutils/2.40</code> binutils 2.38 - <code>module load binutils/2.38</code> binutils 2.39 - <code>module load binutils/2.39</code> intel-compilers 2022.2.1 intel/2022b <code>module load intel-compilers/2022.2.1</code> intel-compilers 2021.4.0 intel/2021b <code>module load intel-compilers/2021.4.0</code> Java 11.0.16 - <code>module load Java/11.0.16</code> Java .modulerc - <code>module load Java/.modulerc</code> gompi 2022b - <code>module load gompi/2022b</code> gompi 2023b - <code>module load gompi/2023b</code> gompi 2021b - <code>module load gompi/2021b</code> ncurses 6.3 - <code>module load ncurses/6.3</code> ncurses 6.4 - <code>module load ncurses/6.4</code> ncurses 6.2 - <code>module load ncurses/6.2</code> gfbf 2022b - <code>module load gfbf/2022b</code> gfbf 2023b - <code>module load gfbf/2023b</code> p7zip 17.04 - <code>module load p7zip/17.04</code> libevent 2.1.12 - <code>module load libevent/2.1.12</code> git-lfs 3.2.0 - <code>module load git-lfs/3.2.0</code> GCC 12.2.0 foss/2022b <code>module load GCC/12.2.0</code> GCC 13.2.0 foss/2023b <code>module load GCC/13.2.0</code> GCC 11.2.0 foss/2021b <code>module load GCC/11.2.0</code> opendihu 2204 - <code>module load opendihu/2204</code> git 2.33.1 - <code>module load git/2.33.1</code> rclone 1.57.0 - <code>module load rclone/1.57.0</code> tmux 3.3a - <code>module load tmux/3.3a</code> iimpi 2022b - <code>module load iimpi/2022b</code> iimpi 2022a - <code>module load iimpi/2022a</code> iimpi 2021b - <code>module load iimpi/2021b</code> MATLAB 2023a - <code>module load MATLAB/2023a</code> ANSYS 2022 - <code>module load ANSYS/2022</code> Anaconda3 2023.09-0 - <code>module load Anaconda3/2023.09-0</code> Anaconda3 5.3.0 - <code>module load Anaconda3/5.3.0</code> GCCcore 12.2.0 - <code>module load GCCcore/12.2.0</code> GCCcore 11.3.0 - <code>module load GCCcore/11.3.0</code> GCCcore 13.2.0 - <code>module load GCCcore/13.2.0</code> GCCcore 11.2.0 - <code>module load GCCcore/11.2.0</code> COMSOL 5.6 - <code>module load COMSOL/5.6</code> tecplot360ex 2022R2 - <code>module load tecplot360ex/2022R2</code> Go 1.17.3 - <code>module load Go/1.17.3</code> Go 1.17.6 - <code>module load Go/1.17.6</code> pkgconf 1.8.0 - <code>module load pkgconf/1.8.0</code> matplotlib 3.4.3 intel/2021b <code>module load intel/2021b matplotlib/3.4.3</code> CP2K 8.2 intel/2021b <code>module load intel/2021b CP2K/8.2</code> PLUMED 2.8.0 intel/2021b <code>module load intel/2021b PLUMED/2.8.0</code> netCDF-Fortran 4.5.3 intel/2021b <code>module load intel/2021b netCDF-Fortran/4.5.3</code> netCDF 4.8.1 intel/2021b <code>module load intel/2021b netCDF/4.8.1</code> SciPy-bundle 2021.10 intel/2021b <code>module load intel/2021b SciPy-bundle/2021.10</code> HDF5 1.12.1 intel/2021b <code>module load intel/2021b HDF5/1.12.1</code> Libint 2.7.2-lmax-6-cp2k intel/2021b <code>module load intel/2021b Libint/2.7.2-lmax-6-cp2k</code> SuperLU 5.3.0 intel/2021b <code>module load intel/2021b SuperLU/5.3.0</code> ELPA 2021.11.001 intel/2021b <code>module load intel/2021b ELPA/2021.11.001</code> imkl-FFTW 2021.4.0 intel/2021b <code>module load intel/2021b imkl-FFTW/2021.4.0</code> HPL 2.3 intel/2022b <code>module load intel/2022b HPL/2.3</code> HDF5 1.14.0 intel/2022b <code>module load intel/2022b HDF5/1.14.0</code> imkl-FFTW 2022.2.1 intel/2022b <code>module load intel/2022b imkl-FFTW/2022.2.1</code> VASP 6.4.2 intel/2022b <code>module load intel/2022b VASP/6.4.2</code> GROMACS 2021.5 foss/2021b <code>module load foss/2021b GROMACS/2021.5</code> GROMACS 2021.5-CUDA-11.4.1 foss/2021b <code>module load foss/2021b GROMACS/2021.5-CUDA-11.4.1</code> matplotlib 3.4.3 foss/2021b <code>module load foss/2021b matplotlib/3.4.3</code> R 4.2.0 foss/2021b <code>module load foss/2021b R/4.2.0</code> OpenFOAM v2112 foss/2021b <code>module load foss/2021b OpenFOAM/v2112</code> PLUMED 2.7.3 foss/2021b <code>module load foss/2021b PLUMED/2.7.3</code> MDAnalysis 2.0.0 foss/2021b <code>module load foss/2021b MDAnalysis/2.0.0</code> LIGGGHTS-PUBLIC 3.8.0-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LIGGGHTS-PUBLIC/3.8.0-kokkos-CUDA-11.4.1</code> netCDF-Fortran 4.5.3 foss/2021b <code>module load foss/2021b netCDF-Fortran/4.5.3</code> Biopython 1.79 foss/2021b <code>module load foss/2021b Biopython/1.79</code> LAMMPS 23Jun2022-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos-CUDA-11.4.1</code> LAMMPS 23Jun2022-kokkos foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos</code> magma 2.6.2-CUDA-11.4.1 foss/2021b <code>module load foss/2021b magma/2.6.2-CUDA-11.4.1</code> OVITO 3.7.11-basic foss/2021b <code>module load foss/2021b OVITO/3.7.11-basic</code> netCDF 4.8.1 foss/2021b <code>module load foss/2021b netCDF/4.8.1</code> ParaView 5.9.1-mpi foss/2021b <code>module load foss/2021b ParaView/5.9.1-mpi</code> Boost.MPI 1.77.0 foss/2021b <code>module load foss/2021b Boost.MPI/1.77.0</code> CGAL 4.14.3 foss/2021b <code>module load foss/2021b CGAL/4.14.3</code> SciPy-bundle 2021.10 foss/2021b <code>module load foss/2021b SciPy-bundle/2021.10</code> QuantumESPRESSO 7.1 foss/2021b <code>module load foss/2021b QuantumESPRESSO/7.1</code> SCOTCH 6.1.2 foss/2021b <code>module load foss/2021b SCOTCH/6.1.2</code> RIP-MD master foss/2021b <code>module load foss/2021b RIP-MD/master</code> HDF5 1.12.1 foss/2021b <code>module load foss/2021b HDF5/1.12.1</code> AFNI 23.0.04-Python-3.9.6 foss/2021b <code>module load foss/2021b AFNI/23.0.04-Python-3.9.6</code> VTK 9.1.0 foss/2021b <code>module load foss/2021b VTK/9.1.0</code> perm-md-count main foss/2021b <code>module load foss/2021b perm-md-count/main</code> networkx 2.6.3 foss/2021b <code>module load foss/2021b networkx/2.6.3</code> FFTW 3.3.10 foss/2021b <code>module load foss/2021b FFTW/3.3.10</code> ELPA 2021.05.001 foss/2021b <code>module load foss/2021b ELPA/2021.05.001</code> GDAL 3.3.2 foss/2021b <code>module load foss/2021b GDAL/3.3.2</code> ScaFaCoS 1.0.1 foss/2021b <code>module load foss/2021b ScaFaCoS/1.0.1</code> ScaLAPACK 2.1.0-fb foss/2021b <code>module load foss/2021b ScaLAPACK/2.1.0-fb</code> GROMACS 2024.1 foss/2023b <code>module load foss/2023b GROMACS/2024.1</code> mpi4py 3.1.5 foss/2023b <code>module load foss/2023b mpi4py/3.1.5</code> FFTW.MPI 3.3.10 foss/2023b <code>module load foss/2023b FFTW.MPI/3.3.10</code> ScaLAPACK 2.2.0-fb foss/2023b <code>module load foss/2023b ScaLAPACK/2.2.0-fb</code> GROMACS 2023.1-CUDA-12.0.0 foss/2022b <code>module load foss/2022b GROMACS/2023.1-CUDA-12.0.0</code> R 4.2.2 foss/2022b <code>module load foss/2022b R/4.2.2</code> ncview 2.1.8 foss/2022b <code>module load foss/2022b ncview/2.1.8</code> CP2K 2023.1 foss/2022b <code>module load foss/2022b CP2K/2023.1</code> PLUMED 2.9.0 foss/2022b <code>module load foss/2022b PLUMED/2.9.0</code> MDAnalysis 2.4.2 foss/2022b <code>module load foss/2022b MDAnalysis/2.4.2</code> PnetCDF 1.12.3 foss/2022b <code>module load foss/2022b PnetCDF/1.12.3</code> netCDF-Fortran 4.6.0 foss/2022b <code>module load foss/2022b netCDF-Fortran/4.6.0</code> Biopython 1.81 foss/2022b <code>module load foss/2022b Biopython/1.81</code> PyTables 3.8.0 foss/2022b <code>module load foss/2022b PyTables/3.8.0</code> mpi4py 3.1.4 foss/2022b <code>module load foss/2022b mpi4py/3.1.4</code> netCDF 4.9.0 foss/2022b <code>module load foss/2022b netCDF/4.9.0</code> ParaView 5.11.0-mpi foss/2022b <code>module load foss/2022b ParaView/5.11.0-mpi</code> IQ-TREE 2.2.2.6 foss/2022b <code>module load foss/2022b IQ-TREE/2.2.2.6</code> chapel 1.33.0 foss/2022b <code>module load foss/2022b chapel/1.33.0</code> SuperLU_DIST 8.1.0 foss/2022b <code>module load foss/2022b SuperLU_DIST/8.1.0</code> Armadillo 11.4.3 foss/2022b <code>module load foss/2022b Armadillo/11.4.3</code> HPL 2.3 foss/2022b <code>module load foss/2022b HPL/2.3</code> HDF5 1.14.0 foss/2022b <code>module load foss/2022b HDF5/1.14.0</code> xtb 6.6.1 foss/2022b <code>module load foss/2022b xtb/6.6.1</code> perm-md-count main foss/2022b <code>module load foss/2022b perm-md-count/main</code> ParMETIS 4.0.3 foss/2022b <code>module load foss/2022b ParMETIS/4.0.3</code> MPAS 8.0.1 foss/2022b <code>module load foss/2022b MPAS/8.0.1</code> SuperLU 5.3.0 foss/2022b <code>module load foss/2022b SuperLU/5.3.0</code> ORCA 5.0.4 foss/2022b <code>module load foss/2022b ORCA/5.0.4</code> VMD 1.9.4a57 foss/2022b <code>module load foss/2022b VMD/1.9.4a57</code> arkouda 2023.11.15 foss/2022b <code>module load foss/2022b arkouda/2023.11.15</code> GDAL 3.6.2 foss/2022b <code>module load foss/2022b GDAL/3.6.2</code> ParallelIO 2.5.10 foss/2022b <code>module load foss/2022b ParallelIO/2.5.10</code> Grace 5.1.25 foss/2022b <code>module load foss/2022b Grace/5.1.25</code> FFTW.MPI 3.3.10 foss/2022b <code>module load foss/2022b FFTW.MPI/3.3.10</code> arpack-ng 3.8.0 foss/2022b <code>module load foss/2022b arpack-ng/3.8.0</code> ScaLAPACK 2.2.0-fb foss/2022b <code>module load foss/2022b ScaLAPACK/2.2.0-fb</code> Hypre 2.27.0 foss/2022b <code>module load foss/2022b Hypre/2.27.0</code> libxsmm 1.17 intel/2021b <code>module load intel/2021b libxsmm/1.17</code> impi 2021.4.0 intel/2021b <code>module load intel/2021b</code> GSL 2.7 intel/2021b <code>module load intel/2021b GSL/2.7</code> Boost 1.77.0 intel/2021b <code>module load intel/2021b Boost/1.77.0</code> libxc 5.1.6 intel/2021b <code>module load intel/2021b libxc/5.1.6</code> libxc 4.3.4 intel/2021b <code>module load intel/2021b libxc/4.3.4</code> libxsmm 1.16.2 intel/2022b <code>module load intel/2022b libxsmm/1.16.2</code> impi 2021.7.1 intel/2022b <code>module load intel/2022b</code> libxc 6.1.0 intel/2022b <code>module load intel/2022b libxc/6.1.0</code> libxc 5.2.3 intel/2022b <code>module load intel/2022b libxc/5.2.3</code> Boost.Python 1.77.0 foss/2021b <code>module load foss/2021b Boost.Python/1.77.0</code> poppler 22.01.0 foss/2021b <code>module load foss/2021b poppler/22.01.0</code> AutoDock-GPU 1.5.3-CUDA-11.4.1 foss/2021b <code>module load foss/2021b AutoDock-GPU/1.5.3-CUDA-11.4.1</code> OpenMPI 4.1.1 foss/2021b <code>module load foss/2021b</code> OpenBLAS 0.3.18 foss/2021b <code>module load foss/2021b OpenBLAS/0.3.18</code> GSL 2.7 foss/2021b <code>module load foss/2021b GSL/2.7</code> BLIS 0.8.1 foss/2021b <code>module load foss/2021b BLIS/0.8.1</code> Boost 1.79.0 foss/2021b <code>module load foss/2021b Boost/1.79.0</code> Boost 1.77.0 foss/2021b <code>module load foss/2021b Boost/1.77.0</code> POV-Ray 3.7.0.10 foss/2021b <code>module load foss/2021b POV-Ray/3.7.0.10</code> FlexiBLAS 3.0.4 foss/2021b <code>module load foss/2021b FlexiBLAS/3.0.4</code> libxc 5.1.6 foss/2021b <code>module load foss/2021b libxc/5.1.6</code> GEOS 3.9.1 foss/2021b <code>module load foss/2021b GEOS/3.9.1</code> SciPy-bundle 2023.11 foss/2023b <code>module load foss/2023b SciPy-bundle/2023.11</code> OpenMPI 4.1.6 foss/2023b <code>module load foss/2023b</code> OpenBLAS 0.3.24 foss/2023b <code>module load foss/2023b OpenBLAS/0.3.24</code> networkx 3.2.1 foss/2023b <code>module load foss/2023b networkx/3.2.1</code> BLIS 0.9.0 foss/2023b <code>module load foss/2023b BLIS/0.9.0</code> FFTW 3.3.10 foss/2023b <code>module load foss/2023b FFTW/3.3.10</code> FlexiBLAS 3.3.1 foss/2023b <code>module load foss/2023b FlexiBLAS/3.3.1</code> matplotlib 3.7.0 foss/2022b <code>module load foss/2022b matplotlib/3.7.0</code> libxsmm 1.17 foss/2022b <code>module load foss/2022b libxsmm/1.17</code> packmol 20.14.3 foss/2022b <code>module load foss/2022b packmol/20.14.3</code> NTL 11.5.1 foss/2022b <code>module load foss/2022b NTL/11.5.1</code> Arb 2.23.0 foss/2022b <code>module load foss/2022b Arb/2.23.0</code> SciPy-bundle 2023.02 foss/2022b <code>module load foss/2022b SciPy-bundle/2023.02</code> Libint 2.7.2-lmax-6-cp2k foss/2022b <code>module load foss/2022b Libint/2.7.2-lmax-6-cp2k</code> OpenMPI 4.1.4 foss/2022b <code>module load foss/2022b</code> xtb 6.6.1 foss/2022b <code>module load foss/2022b xtb/6.6.1</code> OpenBLAS 0.3.21 foss/2022b <code>module load foss/2022b OpenBLAS/0.3.21</code> GSL 2.7 foss/2022b <code>module load foss/2022b GSL/2.7</code> networkx 2.8.8 foss/2022b <code>module load foss/2022b networkx/2.8.8</code> networkx 3.0 foss/2022b <code>module load foss/2022b networkx/3.0</code> BLIS 0.9.0 foss/2022b <code>module load foss/2022b BLIS/0.9.0</code> CREST 2.12 foss/2022b <code>module load foss/2022b CREST/2.12</code> Arrow 11.0.0 foss/2022b <code>module load foss/2022b Arrow/11.0.0</code> Boost 1.81.0 foss/2022b <code>module load foss/2022b Boost/1.81.0</code> FFTW 3.3.10 foss/2022b <code>module load foss/2022b FFTW/3.3.10</code> POV-Ray 3.7.0.10 foss/2022b <code>module load foss/2022b POV-Ray/3.7.0.10</code> silo 4.10.2 foss/2022b <code>module load foss/2022b silo/4.10.2</code> FlexiBLAS 3.2.1 foss/2022b <code>module load foss/2022b FlexiBLAS/3.2.1</code> libxc 6.1.0 foss/2022b <code>module load foss/2022b libxc/6.1.0</code> FLINT 2.9.0 foss/2022b <code>module load foss/2022b FLINT/2.9.0</code> FLINT 3.0.1 foss/2022b <code>module load foss/2022b FLINT/3.0.1</code> GEOS 3.11.1 foss/2022b <code>module load foss/2022b GEOS/3.11.1</code> libsndfile 1.0.31 foss/2021b <code>module load foss/2021b libsndfile/1.0.31</code> nodejs 14.17.6 foss/2021b <code>module load foss/2021b nodejs/14.17.6</code> Rust 1.54.0 foss/2021b <code>module load foss/2021b Rust/1.54.0</code> zlib 1.2.11 foss/2021b <code>module load foss/2021b zlib/1.2.11</code> OpenEXR 3.1.1 foss/2021b <code>module load foss/2021b OpenEXR/3.1.1</code> jbigkit 2.1 foss/2021b <code>module load foss/2021b jbigkit/2.1</code> libgd 2.3.3 foss/2021b <code>module load foss/2021b libgd/2.3.3</code> lz4 1.9.3 foss/2021b <code>module load foss/2021b lz4/1.9.3</code> kim-api 2.3.0 foss/2021b <code>module load foss/2021b kim-api/2.3.0</code> motif 2.3.8 foss/2021b <code>module load foss/2021b motif/2.3.8</code> libarchive 3.5.1 foss/2021b <code>module load foss/2021b libarchive/3.5.1</code> Ninja 1.10.2 foss/2021b <code>module load foss/2021b Ninja/1.10.2</code> Perl 5.34.0 foss/2021b <code>module load foss/2021b Perl/5.34.0</code> M4 1.4.19 foss/2021b <code>module load foss/2021b M4/1.4.19</code> GLib 2.69.1 foss/2021b <code>module load foss/2021b GLib/2.69.1</code> ICU 69.1 foss/2021b <code>module load foss/2021b ICU/69.1</code> groff 1.22.4 foss/2021b <code>module load foss/2021b groff/1.22.4</code> Meson 0.58.2 foss/2021b <code>module load foss/2021b Meson/0.58.2</code> UDUNITS 2.2.28 foss/2021b <code>module load foss/2021b UDUNITS/2.2.28</code> Z3 4.8.12 foss/2021b <code>module load foss/2021b Z3/4.8.12</code> gettext 0.21 foss/2021b <code>module load foss/2021b gettext/0.21</code> libvorbis 1.3.7 foss/2021b <code>module load foss/2021b libvorbis/1.3.7</code> LittleCMS 2.12 foss/2021b <code>module load foss/2021b LittleCMS/2.12</code> scikit-build 0.11.1 foss/2021b <code>module load foss/2021b scikit-build/0.11.1</code> Szip 2.1.1 foss/2021b <code>module load foss/2021b Szip/2.1.1</code> libffi 3.4.2 foss/2021b <code>module load foss/2021b libffi/3.4.2</code> PCRE 8.45 foss/2021b <code>module load foss/2021b PCRE/8.45</code> Eigen 3.4.0 foss/2021b <code>module load foss/2021b Eigen/3.4.0</code> Eigen 3.3.9 foss/2021b <code>module load foss/2021b Eigen/3.3.9</code> GObject-Introspection 1.68.0 foss/2021b <code>module load foss/2021b GObject-Introspection/1.68.0</code> LAME 3.100 foss/2021b <code>module load foss/2021b LAME/3.100</code> snappy 1.1.9 foss/2021b <code>module load foss/2021b snappy/1.1.9</code> NSS 3.69 foss/2021b <code>module load foss/2021b NSS/3.69</code> expat 2.4.1 foss/2021b <code>module load foss/2021b expat/2.4.1</code> Autoconf 2.71 foss/2021b <code>module load foss/2021b Autoconf/2.71</code> Xvfb 1.20.13 foss/2021b <code>module load foss/2021b Xvfb/1.20.13</code> xxd 8.2.4220 foss/2021b <code>module load foss/2021b xxd/8.2.4220</code> XZ 5.2.5 foss/2021b <code>module load foss/2021b XZ/5.2.5</code> cppy 1.1.0 foss/2021b <code>module load foss/2021b cppy/1.1.0</code> X11 20210802 foss/2021b <code>module load foss/2021b X11/20210802</code> Mesa 21.1.7 foss/2021b <code>module load foss/2021b Mesa/21.1.7</code> GLPK 5.0 foss/2021b <code>module load foss/2021b GLPK/5.0</code> archspec 0.1.3 foss/2021b <code>module load foss/2021b archspec/0.1.3</code> Bison 3.7.6 foss/2021b <code>module load foss/2021b Bison/3.7.6</code> Lua 5.4.3 foss/2021b <code>module load foss/2021b Lua/5.4.3</code> Autotools 20210726 foss/2021b <code>module load foss/2021b Autotools/20210726</code> re2c 2.2 foss/2021b <code>module load foss/2021b re2c/2.2</code> libtirpc 1.3.2 foss/2021b <code>module load foss/2021b libtirpc/1.3.2</code> LibTIFF 4.3.0 foss/2021b <code>module load foss/2021b LibTIFF/4.3.0</code> gperf 3.1 foss/2021b <code>module load foss/2021b gperf/3.1</code> Tcl 8.6.11 foss/2021b <code>module load foss/2021b Tcl/8.6.11</code> ACTC 1.1 foss/2021b <code>module load foss/2021b ACTC/1.1</code> x265 3.5 foss/2021b <code>module load foss/2021b x265/3.5</code> libdrm 2.4.107 foss/2021b <code>module load foss/2021b libdrm/2.4.107</code> Tkinter 3.9.6 foss/2021b <code>module load foss/2021b Tkinter/3.9.6</code> gzip 1.10 foss/2021b <code>module load foss/2021b gzip/1.10</code> SQLite 3.36 foss/2021b <code>module load foss/2021b SQLite/3.36</code> libreadline 8.1 foss/2021b <code>module load foss/2021b libreadline/8.1</code> numactl 2.0.14 foss/2021b <code>module load foss/2021b numactl/2.0.14</code> intltool 0.51.0 foss/2021b <code>module load foss/2021b intltool/0.51.0</code> double-conversion 3.1.5 foss/2021b <code>module load foss/2021b double-conversion/3.1.5</code> libunwind 1.5.0 foss/2021b <code>module load foss/2021b libunwind/1.5.0</code> Python 3.9.6 foss/2021b <code>module load foss/2021b Python/3.9.6</code> Python 2.7.18-bare foss/2021b <code>module load foss/2021b Python/2.7.18-bare</code> Python 3.9.6-bare foss/2021b <code>module load foss/2021b Python/3.9.6-bare</code> Ghostscript 9.54.0 foss/2021b <code>module load foss/2021b Ghostscript/9.54.0</code> help2man 1.48.3 foss/2021b <code>module load foss/2021b help2man/1.48.3</code> GDRCopy 2.3 foss/2021b <code>module load foss/2021b GDRCopy/2.3</code> JasPer 2.0.33 foss/2021b <code>module load foss/2021b JasPer/2.0.33</code> elfutils 0.185 foss/2021b <code>module load foss/2021b elfutils/0.185</code> flex 2.6.4 foss/2021b <code>module load foss/2021b flex/2.6.4</code> FriBidi 1.0.10 foss/2021b <code>module load foss/2021b FriBidi/1.0.10</code> cURL 7.78.0 foss/2021b <code>module load foss/2021b cURL/7.78.0</code> util-linux 2.37 foss/2021b <code>module load foss/2021b util-linux/2.37</code> libfabric 1.13.2 foss/2021b <code>module load foss/2021b libfabric/1.13.2</code> tqdm 4.62.3 foss/2021b <code>module load foss/2021b tqdm/4.62.3</code> nettle 3.7.3 foss/2021b <code>module load foss/2021b nettle/3.7.3</code> libiconv 1.16 foss/2021b <code>module load foss/2021b libiconv/1.16</code> MPFR 4.1.0 foss/2021b <code>module load foss/2021b MPFR/4.1.0</code> xprop 1.2.5 foss/2021b <code>module load foss/2021b xprop/1.2.5</code> SWIG 4.0.2 foss/2021b <code>module load foss/2021b SWIG/4.0.2</code> CMake 3.21.1 foss/2021b <code>module load foss/2021b CMake/3.21.1</code> CMake 3.22.1 foss/2021b <code>module load foss/2021b CMake/3.22.1</code> fontconfig 2.13.94 foss/2021b <code>module load foss/2021b fontconfig/2.13.94</code> libogg 1.3.5 foss/2021b <code>module load foss/2021b libogg/1.3.5</code> libgit2 1.1.1 foss/2021b <code>module load foss/2021b libgit2/1.1.1</code> Mako 1.1.4 foss/2021b <code>module load foss/2021b Mako/1.1.4</code> libglvnd 1.3.3 foss/2021b <code>module load foss/2021b libglvnd/1.3.3</code> Clang 12.0.1 foss/2021b <code>module load foss/2021b Clang/12.0.1</code> freetype 2.11.0 foss/2021b <code>module load foss/2021b freetype/2.11.0</code> libgeotiff 1.7.0 foss/2021b <code>module load foss/2021b libgeotiff/1.7.0</code> PyQt5 5.15.4 foss/2021b <code>module load foss/2021b PyQt5/5.15.4</code> NLopt 2.7.0 foss/2021b <code>module load foss/2021b NLopt/2.7.0</code> bzip2 1.0.8 foss/2021b <code>module load foss/2021b bzip2/1.0.8</code> zstd 1.5.0 foss/2021b <code>module load foss/2021b zstd/1.5.0</code> cairo 1.16.0 foss/2021b <code>module load foss/2021b cairo/1.16.0</code> Automake 1.16.4 foss/2021b <code>module load foss/2021b Automake/1.16.4</code> libGLU 9.0.2 foss/2021b <code>module load foss/2021b libGLU/9.0.2</code> HarfBuzz 2.8.2 foss/2021b <code>module load foss/2021b HarfBuzz/2.8.2</code> GMP 6.2.1 foss/2021b <code>module load foss/2021b GMP/6.2.1</code> SDL2 2.0.20 foss/2021b <code>module load foss/2021b SDL2/2.0.20</code> libtool 2.4.6 foss/2021b <code>module load foss/2021b libtool/2.4.6</code> LLVM 12.0.1 foss/2021b <code>module load foss/2021b LLVM/12.0.1</code> libjpeg-turbo 2.0.6 foss/2021b <code>module load foss/2021b libjpeg-turbo/2.0.6</code> NSPR 4.32 foss/2021b <code>module load foss/2021b NSPR/4.32</code> Voro++ 0.4.6 foss/2021b <code>module load foss/2021b Voro++/0.4.6</code> x264 20210613 foss/2021b <code>module load foss/2021b x264/20210613</code> DBus 1.13.18 foss/2021b <code>module load foss/2021b DBus/1.13.18</code> FLAC 1.3.3 foss/2021b <code>module load foss/2021b FLAC/1.3.3</code> pybind11 2.7.1 foss/2021b <code>module load foss/2021b pybind11/2.7.1</code> xorg-macros 1.19.3 foss/2021b <code>module load foss/2021b xorg-macros/1.19.3</code> binutils 2.37 foss/2021b <code>module load foss/2021b binutils/2.37</code> Brotli 1.0.9 foss/2021b <code>module load foss/2021b Brotli/1.0.9</code> PCRE2 10.37 foss/2021b <code>module load foss/2021b PCRE2/10.37</code> tbb 2020.3 foss/2021b <code>module load foss/2021b tbb/2020.3</code> libpciaccess 0.16 foss/2021b <code>module load foss/2021b libpciaccess/0.16</code> FFmpeg 4.3.2 foss/2021b <code>module load foss/2021b FFmpeg/4.3.2</code> libcerf 1.17 foss/2021b <code>module load foss/2021b libcerf/1.17</code> make 4.3 foss/2021b <code>module load foss/2021b make/4.3</code> Pillow 8.3.2 foss/2021b <code>module load foss/2021b Pillow/8.3.2</code> pixman 0.40.0 foss/2021b <code>module load foss/2021b pixman/0.40.0</code> METIS 5.1.0 foss/2021b <code>module load foss/2021b METIS/5.1.0</code> pkg-config 0.29.2 foss/2021b <code>module load foss/2021b pkg-config/0.29.2</code> UCX 1.11.2 foss/2021b <code>module load foss/2021b UCX/1.11.2</code> PMIx 4.1.0 foss/2021b <code>module load foss/2021b PMIx/4.1.0</code> ncurses 6.2 foss/2021b <code>module load foss/2021b ncurses/6.2</code> pkgconfig 1.5.5-python foss/2021b <code>module load foss/2021b pkgconfig/1.5.5-python</code> libpng 1.6.37 foss/2021b <code>module load foss/2021b libpng/1.6.37</code> Pango 1.48.8 foss/2021b <code>module load foss/2021b Pango/1.48.8</code> libevent 2.1.12 foss/2021b <code>module load foss/2021b libevent/2.1.12</code> Yasm 1.3.0 foss/2021b <code>module load foss/2021b Yasm/1.3.0</code> Doxygen 1.9.1 foss/2021b <code>module load foss/2021b Doxygen/1.9.1</code> gnuplot 5.4.2 foss/2021b <code>module load foss/2021b gnuplot/5.4.2</code> hwloc 2.5.0 foss/2021b <code>module load foss/2021b hwloc/2.5.0</code> HDF 4.2.15 foss/2021b <code>module load foss/2021b HDF/4.2.15</code> libxml2 2.9.10 foss/2021b <code>module load foss/2021b libxml2/2.9.10</code> NASM 2.15.05 foss/2021b <code>module load foss/2021b NASM/2.15.05</code> git 2.33.1-nodocs foss/2021b <code>module load foss/2021b git/2.33.1-nodocs</code> tmux 3.2a foss/2021b <code>module load foss/2021b tmux/3.2a</code> UnZip 6.0 foss/2021b <code>module load foss/2021b UnZip/6.0</code> Qt5 5.15.2 foss/2021b <code>module load foss/2021b Qt5/5.15.2</code> graphite2 1.3.14 foss/2021b <code>module load foss/2021b graphite2/1.3.14</code> PROJ 8.1.0 foss/2021b <code>module load foss/2021b PROJ/8.1.0</code> hypothesis 6.14.6 foss/2021b <code>module load foss/2021b hypothesis/6.14.6</code> NCCL 2.10.3-CUDA-11.4.1 foss/2021b <code>module load foss/2021b NCCL/2.10.3-CUDA-11.4.1</code> OpenJPEG 2.4.0 foss/2021b <code>module load foss/2021b OpenJPEG/2.4.0</code> tcsh 6.24.01 foss/2021b <code>module load foss/2021b tcsh/6.24.01</code> ANSYS 2023R1 foss/2021b <code>module load foss/2021b ANSYS/2023R1</code> UCX-CUDA 1.11.2-CUDA-11.4.1 foss/2021b <code>module load foss/2021b UCX-CUDA/1.11.2-CUDA-11.4.1</code> FLTK 1.3.7 foss/2021b <code>module load foss/2021b FLTK/1.3.7</code> Qhull 2020.2 foss/2021b <code>module load foss/2021b Qhull/2020.2</code> DB 18.1.40 foss/2021b <code>module load foss/2021b DB/18.1.40</code> ImageMagick 7.1.0-4 foss/2021b <code>module load foss/2021b ImageMagick/7.1.0-4</code> Tk 8.6.11 foss/2021b <code>module load foss/2021b Tk/8.6.11</code> pkgconf 1.8.0 foss/2021b <code>module load foss/2021b pkgconf/1.8.0</code> Rust 1.73.0 foss/2023b <code>module load foss/2023b Rust/1.73.0</code> zlib 1.2.13 foss/2023b <code>module load foss/2023b zlib/1.2.13</code> libarchive 3.7.2 foss/2023b <code>module load foss/2023b libarchive/3.7.2</code> virtualenv 20.24.6 foss/2023b <code>module load foss/2023b virtualenv/20.24.6</code> Ninja 1.11.1 foss/2023b <code>module load foss/2023b Ninja/1.11.1</code> Perl 5.38.0 foss/2023b <code>module load foss/2023b Perl/5.38.0</code> M4 1.4.19 foss/2023b <code>module load foss/2023b M4/1.4.19</code> groff 1.23.0 foss/2023b <code>module load foss/2023b groff/1.23.0</code> Meson 1.2.3 foss/2023b <code>module load foss/2023b Meson/1.2.3</code> gettext 0.22 foss/2023b <code>module load foss/2023b gettext/0.22</code> scikit-build 0.17.6 foss/2023b <code>module load foss/2023b scikit-build/0.17.6</code> libffi 3.4.4 foss/2023b <code>module load foss/2023b libffi/3.4.4</code> Eigen 3.4.0 foss/2023b <code>module load foss/2023b Eigen/3.4.0</code> expat 2.5.0 foss/2023b <code>module load foss/2023b expat/2.5.0</code> Autoconf 2.71 foss/2023b <code>module load foss/2023b Autoconf/2.71</code> hatchling 1.18.0 foss/2023b <code>module load foss/2023b hatchling/1.18.0</code> flit 3.9.0 foss/2023b <code>module load foss/2023b flit/3.9.0</code> XZ 5.4.4 foss/2023b <code>module load foss/2023b XZ/5.4.4</code> Bison 3.8.2 foss/2023b <code>module load foss/2023b Bison/3.8.2</code> setuptools-rust 1.8.0 foss/2023b <code>module load foss/2023b setuptools-rust/1.8.0</code> Autotools 20220317 foss/2023b <code>module load foss/2023b Autotools/20220317</code> Tcl 8.6.13 foss/2023b <code>module load foss/2023b Tcl/8.6.13</code> patchelf 0.18.0 foss/2023b <code>module load foss/2023b patchelf/0.18.0</code> Perl-bundle-CPAN 5.38.0 foss/2023b <code>module load foss/2023b Perl-bundle-CPAN/5.38.0</code> SQLite 3.43.1 foss/2023b <code>module load foss/2023b SQLite/3.43.1</code> cryptography 41.0.5 foss/2023b <code>module load foss/2023b cryptography/41.0.5</code> libreadline 8.2 foss/2023b <code>module load foss/2023b libreadline/8.2</code> numactl 2.0.16 foss/2023b <code>module load foss/2023b numactl/2.0.16</code> Python 3.11.5 foss/2023b <code>module load foss/2023b Python/3.11.5</code> help2man 1.49.3 foss/2023b <code>module load foss/2023b help2man/1.49.3</code> flex 2.6.4 foss/2023b <code>module load foss/2023b flex/2.6.4</code> UCC 1.2.0 foss/2023b <code>module load foss/2023b UCC/1.2.0</code> cURL 8.3.0 foss/2023b <code>module load foss/2023b cURL/8.3.0</code> libfabric 1.19.0 foss/2023b <code>module load foss/2023b libfabric/1.19.0</code> libiconv 1.17 foss/2023b <code>module load foss/2023b libiconv/1.17</code> CMake 3.27.6 foss/2023b <code>module load foss/2023b CMake/3.27.6</code> bzip2 1.0.8 foss/2023b <code>module load foss/2023b bzip2/1.0.8</code> Automake 1.16.5 foss/2023b <code>module load foss/2023b Automake/1.16.5</code> libtool 2.4.7 foss/2023b <code>module load foss/2023b libtool/2.4.7</code> pybind11 2.11.1 foss/2023b <code>module load foss/2023b pybind11/2.11.1</code> xorg-macros 1.20.0 foss/2023b <code>module load foss/2023b xorg-macros/1.20.0</code> cffi 1.15.1 foss/2023b <code>module load foss/2023b cffi/1.15.1</code> binutils 2.40 foss/2023b <code>module load foss/2023b binutils/2.40</code> libpciaccess 0.17 foss/2023b <code>module load foss/2023b libpciaccess/0.17</code> make 4.4.1 foss/2023b <code>module load foss/2023b make/4.4.1</code> UCX 1.15.0 foss/2023b <code>module load foss/2023b UCX/1.15.0</code> PMIx 4.2.6 foss/2023b <code>module load foss/2023b PMIx/4.2.6</code> ncurses 6.4 foss/2023b <code>module load foss/2023b ncurses/6.4</code> Perl-bundle-njit 5.38.0 foss/2023b <code>module load foss/2023b Perl-bundle-njit/5.38.0</code> libevent 2.1.12 foss/2023b <code>module load foss/2023b libevent/2.1.12</code> hwloc 2.9.2 foss/2023b <code>module load foss/2023b hwloc/2.9.2</code> libxml2 2.11.5 foss/2023b <code>module load foss/2023b libxml2/2.11.5</code> git 2.42.0 foss/2023b <code>module load foss/2023b git/2.42.0</code> UnZip 6.0 foss/2023b <code>module load foss/2023b UnZip/6.0</code> poetry 1.6.1 foss/2023b <code>module load foss/2023b poetry/1.6.1</code> hypothesis 6.90.0 foss/2023b <code>module load foss/2023b hypothesis/6.90.0</code> Catch2 2.13.9 foss/2023b <code>module load foss/2023b Catch2/2.13.9</code> meson-python 0.15.0 foss/2023b <code>module load foss/2023b meson-python/0.15.0</code> Python-bundle-PyPI 2023.10 foss/2023b <code>module load foss/2023b Python-bundle-PyPI/2023.10</code> pkgconf 2.0.3 foss/2023b <code>module load foss/2023b pkgconf/2.0.3</code> libsndfile 1.2.0 foss/2022b <code>module load foss/2022b libsndfile/1.2.0</code> nodejs 18.12.1 foss/2022b <code>module load foss/2022b nodejs/18.12.1</code> Rust 1.65.0 foss/2022b <code>module load foss/2022b Rust/1.65.0</code> zlib 1.2.12 foss/2022b <code>module load foss/2022b zlib/1.2.12</code> OpenEXR 3.1.5 foss/2022b <code>module load foss/2022b OpenEXR/3.1.5</code> jbigkit 2.1 foss/2022b <code>module load foss/2022b jbigkit/2.1</code> libgd 2.3.3 foss/2022b <code>module load foss/2022b libgd/2.3.3</code> lz4 1.9.4 foss/2022b <code>module load foss/2022b lz4/1.9.4</code> motif 2.3.8 foss/2022b <code>module load foss/2022b motif/2.3.8</code> libarchive 3.6.1 foss/2022b <code>module load foss/2022b libarchive/3.6.1</code> libidn2 2.3.2 foss/2022b <code>module load foss/2022b libidn2/2.3.2</code> Ninja 1.11.1 foss/2022b <code>module load foss/2022b Ninja/1.11.1</code> Perl 5.36.0-minimal foss/2022b <code>module load foss/2022b Perl/5.36.0-minimal</code> Perl 5.36.0 foss/2022b <code>module load foss/2022b Perl/5.36.0</code> M4 1.4.19 foss/2022b <code>module load foss/2022b M4/1.4.19</code> GLib 2.75.0 foss/2022b <code>module load foss/2022b GLib/2.75.0</code> ICU 72.1 foss/2022b <code>module load foss/2022b ICU/72.1</code> giflib 5.2.1 foss/2022b <code>module load foss/2022b giflib/5.2.1</code> libsodium 1.0.18 foss/2022b <code>module load foss/2022b libsodium/1.0.18</code> groff 1.22.4 foss/2022b <code>module load foss/2022b groff/1.22.4</code> py-cpuinfo 9.0.0 foss/2022b <code>module load foss/2022b py-cpuinfo/9.0.0</code> Meson 0.64.0 foss/2022b <code>module load foss/2022b Meson/0.64.0</code> at-spi2-core 2.46.0 foss/2022b <code>module load foss/2022b at-spi2-core/2.46.0</code> UDUNITS 2.2.28 foss/2022b <code>module load foss/2022b UDUNITS/2.2.28</code> gettext 0.21.1 foss/2022b <code>module load foss/2022b gettext/0.21.1</code> libvorbis 1.3.7 foss/2022b <code>module load foss/2022b libvorbis/1.3.7</code> LittleCMS 2.14 foss/2022b <code>module load foss/2022b LittleCMS/2.14</code> scikit-build 0.17.2 foss/2022b <code>module load foss/2022b scikit-build/0.17.2</code> wxWidgets 3.2.2.1 foss/2022b <code>module load foss/2022b wxWidgets/3.2.2.1</code> Szip 2.1.1 foss/2022b <code>module load foss/2022b Szip/2.1.1</code> libffi 3.4.4 foss/2022b <code>module load foss/2022b libffi/3.4.4</code> PCRE 8.45 foss/2022b <code>module load foss/2022b PCRE/8.45</code> LZO 2.10 foss/2022b <code>module load foss/2022b LZO/2.10</code> Eigen 3.4.0 foss/2022b <code>module load foss/2022b Eigen/3.4.0</code> GObject-Introspection 1.74.0 foss/2022b <code>module load foss/2022b GObject-Introspection/1.74.0</code> LAME 3.100 foss/2022b <code>module load foss/2022b LAME/3.100</code> snappy 1.1.9 foss/2022b <code>module load foss/2022b snappy/1.1.9</code> NSS 3.85 foss/2022b <code>module load foss/2022b NSS/3.85</code> expat 2.4.9 foss/2022b <code>module load foss/2022b expat/2.4.9</code> Autoconf 2.71 foss/2022b <code>module load foss/2022b Autoconf/2.71</code> Xvfb 21.1.6 foss/2022b <code>module load foss/2022b Xvfb/21.1.6</code> xxd 9.0.1696 foss/2022b <code>module load foss/2022b xxd/9.0.1696</code> GStreamer 1.22.1 foss/2022b <code>module load foss/2022b GStreamer/1.22.1</code> utf8proc 2.8.0 foss/2022b <code>module load foss/2022b utf8proc/2.8.0</code> XZ 5.2.7 foss/2022b <code>module load foss/2022b XZ/5.2.7</code> cppy 1.2.1 foss/2022b <code>module load foss/2022b cppy/1.2.1</code> X11 20221110 foss/2022b <code>module load foss/2022b X11/20221110</code> Mesa 22.2.4 foss/2022b <code>module load foss/2022b Mesa/22.2.4</code> GLPK 5.0 foss/2022b <code>module load foss/2022b GLPK/5.0</code> Bison 3.8.2 foss/2022b <code>module load foss/2022b Bison/3.8.2</code> LSD2 2.4.1 foss/2022b <code>module load foss/2022b LSD2/2.4.1</code> Lua 5.4.4 foss/2022b <code>module load foss/2022b Lua/5.4.4</code> Autotools 20220317 foss/2022b <code>module load foss/2022b Autotools/20220317</code> ZeroMQ 4.3.4 foss/2022b <code>module load foss/2022b ZeroMQ/4.3.4</code> re2c 3.0 foss/2022b <code>module load foss/2022b re2c/3.0</code> libtirpc 1.3.3 foss/2022b <code>module load foss/2022b libtirpc/1.3.3</code> LibTIFF 4.4.0 foss/2022b <code>module load foss/2022b LibTIFF/4.4.0</code> gperf 3.1 foss/2022b <code>module load foss/2022b gperf/3.1</code> Tcl 8.6.12 foss/2022b <code>module load foss/2022b Tcl/8.6.12</code> at-spi2-atk 2.38.0 foss/2022b <code>module load foss/2022b at-spi2-atk/2.38.0</code> ACTC 1.1 foss/2022b <code>module load foss/2022b ACTC/1.1</code> x265 3.5 foss/2022b <code>module load foss/2022b x265/3.5</code> libdrm 2.4.114 foss/2022b <code>module load foss/2022b libdrm/2.4.114</code> Tkinter 3.10.8 foss/2022b <code>module load foss/2022b Tkinter/3.10.8</code> gzip 1.12 foss/2022b <code>module load foss/2022b gzip/1.12</code> libxslt 1.1.37 foss/2022b <code>module load foss/2022b libxslt/1.1.37</code> SQLite 3.39.4 foss/2022b <code>module load foss/2022b SQLite/3.39.4</code> Graphene 1.10.8 foss/2022b <code>module load foss/2022b Graphene/1.10.8</code> libreadline 8.2 foss/2022b <code>module load foss/2022b libreadline/8.2</code> numactl 2.0.16 foss/2022b <code>module load foss/2022b numactl/2.0.16</code> libvori 220621 foss/2022b <code>module load foss/2022b libvori/220621</code> intltool 0.51.0 foss/2022b <code>module load foss/2022b intltool/0.51.0</code> Blosc 1.21.3 foss/2022b <code>module load foss/2022b Blosc/1.21.3</code> double-conversion 3.2.1 foss/2022b <code>module load foss/2022b double-conversion/3.2.1</code> libunwind 1.6.2 foss/2022b <code>module load foss/2022b libunwind/1.6.2</code> Python 3.10.8-bare foss/2022b <code>module load foss/2022b Python/3.10.8-bare</code> Python 2.7.18-bare foss/2022b <code>module load foss/2022b Python/2.7.18-bare</code> Python 3.10.8 foss/2022b <code>module load foss/2022b Python/3.10.8</code> Brunsli 0.1 foss/2022b <code>module load foss/2022b Brunsli/0.1</code> Ghostscript 10.0.0 foss/2022b <code>module load foss/2022b Ghostscript/10.0.0</code> help2man 1.49.2 foss/2022b <code>module load foss/2022b help2man/1.49.2</code> GDRCopy 2.3 foss/2022b <code>module load foss/2022b GDRCopy/2.3</code> JasPer 4.0.0 foss/2022b <code>module load foss/2022b JasPer/4.0.0</code> elfutils 0.189 foss/2022b <code>module load foss/2022b elfutils/0.189</code> RapidJSON 1.1.0 foss/2022b <code>module load foss/2022b RapidJSON/1.1.0</code> flex 2.6.4 foss/2022b <code>module load foss/2022b flex/2.6.4</code> FriBidi 1.0.12 foss/2022b <code>module load foss/2022b FriBidi/1.0.12</code> UCC 1.1.0 foss/2022b <code>module load foss/2022b UCC/1.1.0</code> json-c 0.16 foss/2022b <code>module load foss/2022b json-c/0.16</code> cURL 7.86.0 foss/2022b <code>module load foss/2022b cURL/7.86.0</code> util-linux 2.38.1 foss/2022b <code>module load foss/2022b util-linux/2.38.1</code> libfabric 1.16.1 foss/2022b <code>module load foss/2022b libfabric/1.16.1</code> tqdm 4.64.1 foss/2022b <code>module load foss/2022b tqdm/4.64.1</code> nettle 3.8.1 foss/2022b <code>module load foss/2022b nettle/3.8.1</code> libiconv 1.17 foss/2022b <code>module load foss/2022b libiconv/1.17</code> MPFR 4.2.0 foss/2022b <code>module load foss/2022b MPFR/4.2.0</code> xprop 1.2.5 foss/2022b <code>module load foss/2022b xprop/1.2.5</code> Xerces-C++ 3.2.4 foss/2022b <code>module load foss/2022b Xerces-C++/3.2.4</code> CMake 3.24.3 foss/2022b <code>module load foss/2022b CMake/3.24.3</code> fontconfig 2.14.1 foss/2022b <code>module load foss/2022b fontconfig/2.14.1</code> libogg 1.3.5 foss/2022b <code>module load foss/2022b libogg/1.3.5</code> libgit2 1.5.0 foss/2022b <code>module load foss/2022b libgit2/1.5.0</code> Mako 1.2.4 foss/2022b <code>module load foss/2022b Mako/1.2.4</code> libglvnd 1.6.0 foss/2022b <code>module load foss/2022b libglvnd/1.6.0</code> freetype 2.12.1 foss/2022b <code>module load foss/2022b freetype/2.12.1</code> libgeotiff 1.7.1 foss/2022b <code>module load foss/2022b libgeotiff/1.7.1</code> NLopt 2.7.1 foss/2022b <code>module load foss/2022b NLopt/2.7.1</code> CFITSIO 4.2.0 foss/2022b <code>module load foss/2022b CFITSIO/4.2.0</code> ATK 2.38.0 foss/2022b <code>module load foss/2022b ATK/2.38.0</code> bzip2 1.0.8 foss/2022b <code>module load foss/2022b bzip2/1.0.8</code> zstd 1.5.2 foss/2022b <code>module load foss/2022b zstd/1.5.2</code> cairo 1.17.4 foss/2022b <code>module load foss/2022b cairo/1.17.4</code> GST-plugins-base 1.22.1 foss/2022b <code>module load foss/2022b GST-plugins-base/1.22.1</code> Automake 1.16.5 foss/2022b <code>module load foss/2022b Automake/1.16.5</code> libGLU 9.0.2 foss/2022b <code>module load foss/2022b libGLU/9.0.2</code> HarfBuzz 5.3.1 foss/2022b <code>module load foss/2022b HarfBuzz/5.3.1</code> Blosc2 2.8.0 foss/2022b <code>module load foss/2022b Blosc2/2.8.0</code> GMP 6.2.1 foss/2022b <code>module load foss/2022b GMP/6.2.1</code> SDL2 2.26.3 foss/2022b <code>module load foss/2022b SDL2/2.26.3</code> libtool 2.4.7 foss/2022b <code>module load foss/2022b libtool/2.4.7</code> LLVM 15.0.5 foss/2022b <code>module load foss/2022b LLVM/15.0.5</code> GSL 2.7 foss/2022b <code>module load foss/2022b GSL/2.7</code> libjpeg-turbo 2.1.4 foss/2022b <code>module load foss/2022b libjpeg-turbo/2.1.4</code> Gdk-Pixbuf 2.42.10 foss/2022b <code>module load foss/2022b Gdk-Pixbuf/2.42.10</code> NSPR 4.35 foss/2022b <code>module load foss/2022b NSPR/4.35</code> x264 20230226 foss/2022b <code>module load foss/2022b x264/20230226</code> libopus 1.3.1 foss/2022b <code>module load foss/2022b libopus/1.3.1</code> DBus 1.15.2 foss/2022b <code>module load foss/2022b DBus/1.15.2</code> FLAC 1.4.2 foss/2022b <code>module load foss/2022b FLAC/1.4.2</code> pybind11 2.10.3 foss/2022b <code>module load foss/2022b pybind11/2.10.3</code> xorg-macros 1.19.3 foss/2022b <code>module load foss/2022b xorg-macros/1.19.3</code> GTK3 3.24.35 foss/2022b <code>module load foss/2022b GTK3/3.24.35</code> binutils 2.39 foss/2022b <code>module load foss/2022b binutils/2.39</code> Brotli 1.0.9 foss/2022b <code>module load foss/2022b Brotli/1.0.9</code> PCRE2 10.40 foss/2022b <code>module load foss/2022b PCRE2/10.40</code> Imath 3.1.6 foss/2022b <code>module load foss/2022b Imath/3.1.6</code> libpciaccess 0.17 foss/2022b <code>module load foss/2022b libpciaccess/0.17</code> FFmpeg 5.1.2 foss/2022b <code>module load foss/2022b FFmpeg/5.1.2</code> FFmpeg 6.1.1 foss/2022b <code>module load foss/2022b FFmpeg/6.1.1</code> libcerf 2.3 foss/2022b <code>module load foss/2022b libcerf/2.3</code> Pillow 9.4.0 foss/2022b <code>module load foss/2022b Pillow/9.4.0</code> pixman 0.42.2 foss/2022b <code>module load foss/2022b pixman/0.42.2</code> googletest 1.12.1 foss/2022b <code>module load foss/2022b googletest/1.12.1</code> METIS 5.1.0 foss/2022b <code>module load foss/2022b METIS/5.1.0</code> UCX 1.13.1 foss/2022b <code>module load foss/2022b UCX/1.13.1</code> PMIx 4.2.2 foss/2022b <code>module load foss/2022b PMIx/4.2.2</code> ncurses 6.3 foss/2022b <code>module load foss/2022b ncurses/6.3</code> libpng 1.6.38 foss/2022b <code>module load foss/2022b libpng/1.6.38</code> Pango 1.50.12 foss/2022b <code>module load foss/2022b Pango/1.50.12</code> libevent 2.1.12 foss/2022b <code>module load foss/2022b libevent/2.1.12</code> Yasm 1.3.0 foss/2022b <code>module load foss/2022b Yasm/1.3.0</code> libepoxy 1.5.10 foss/2022b <code>module load foss/2022b libepoxy/1.5.10</code> Doxygen 1.9.5 foss/2022b <code>module load foss/2022b Doxygen/1.9.5</code> gnuplot 5.4.6 foss/2022b <code>module load foss/2022b gnuplot/5.4.6</code> hwloc 2.8.0 foss/2022b <code>module load foss/2022b hwloc/2.8.0</code> HDF 4.2.15 foss/2022b <code>module load foss/2022b HDF/4.2.15</code> libxml2 2.10.3 foss/2022b <code>module load foss/2022b libxml2/2.10.3</code> NASM 2.15.05 foss/2022b <code>module load foss/2022b NASM/2.15.05</code> git 2.38.1-nodocs foss/2022b <code>module load foss/2022b git/2.38.1-nodocs</code> PyZMQ 25.1.0 foss/2022b <code>module load foss/2022b PyZMQ/25.1.0</code> UnZip 6.0 foss/2022b <code>module load foss/2022b UnZip/6.0</code> Qt5 5.15.7 foss/2022b <code>module load foss/2022b Qt5/5.15.7</code> graphite2 1.3.14 foss/2022b <code>module load foss/2022b graphite2/1.3.14</code> PROJ 9.1.1 foss/2022b <code>module load foss/2022b PROJ/9.1.1</code> LERC 4.0.0 foss/2022b <code>module load foss/2022b LERC/4.0.0</code> hypothesis 6.68.2 foss/2022b <code>module load foss/2022b hypothesis/6.68.2</code> OpenJPEG 2.5.0 foss/2022b <code>module load foss/2022b OpenJPEG/2.5.0</code> RE2 2023-03-01 foss/2022b <code>module load foss/2022b RE2/2023-03-01</code> Highway 1.0.3 foss/2022b <code>module load foss/2022b Highway/1.0.3</code> UCX-CUDA 1.13.1-CUDA-12.0.0 foss/2022b <code>module load foss/2022b UCX-CUDA/1.13.1-CUDA-12.0.0</code> FLTK 1.3.8 foss/2022b <code>module load foss/2022b FLTK/1.3.8</code> Qhull 2020.2 foss/2022b <code>module load foss/2022b Qhull/2020.2</code> OpenPGM 5.2.122 foss/2022b <code>module load foss/2022b OpenPGM/5.2.122</code> DB 18.1.40 foss/2022b <code>module load foss/2022b DB/18.1.40</code> ImageMagick 7.1.0-53 foss/2022b <code>module load foss/2022b ImageMagick/7.1.0-53</code> Tk 8.6.12 foss/2022b <code>module load foss/2022b Tk/8.6.12</code> nlohmann_json 3.11.2 foss/2022b <code>module load foss/2022b nlohmann_json/3.11.2</code> libdeflate 1.15 foss/2022b <code>module load foss/2022b libdeflate/1.15</code> pkgconf 1.9.3 foss/2022b <code>module load foss/2022b pkgconf/1.9.3</code> </p> <p> Software Version Dependent Toolchain Module Load Command EasyBuild 4.3.3 - <code>module load EasyBuild/4.3.3</code> EasyBuild 4.4.0 - <code>module load EasyBuild/4.4.0</code> EasyBuild 4.4.2 - <code>module load EasyBuild/4.4.2</code> EasyBuild 4.5.0 - <code>module load EasyBuild/4.5.0</code> EasyBuild 4.5.2 - <code>module load EasyBuild/4.5.2</code> EasyBuild 4.5.3 - <code>module load EasyBuild/4.5.3</code> EasyBuild 4.5.4 - <code>module load EasyBuild/4.5.4</code> EasyBuild 4.5.5 - <code>module load EasyBuild/4.5.5</code> EasyBuild 4.6.0 - <code>module load EasyBuild/4.6.0</code> EasyBuild 4.7.0 - <code>module load EasyBuild/4.7.0</code> M4 .1.4.18 - <code>module load M4/.1.4.18</code> M4 .1.4.19 - <code>module load M4/.1.4.19</code> M4 .1.4.17 - <code>module load M4/.1.4.17</code> Bison .3.5.3 - <code>module load Bison/.3.5.3</code> Bison .3.3.2 - <code>module load Bison/.3.3.2</code> Bison .3.7.1 - <code>module load Bison/.3.7.1</code> Bison .3.7.6 - <code>module load Bison/.3.7.6</code> Bison .3.8.2 - <code>module load Bison/.3.8.2</code> zlib .1.2.11 - <code>module load zlib/.1.2.11</code> zlib .1.2.12 - <code>module load zlib/.1.2.12</code> help2man .1.47.4 - <code>module load help2man/.1.47.4</code> flex .2.6.4 - <code>module load flex/.2.6.4</code> binutils .2.34 - <code>module load binutils/.2.34</code> binutils .2.32 - <code>module load binutils/.2.32</code> binutils .2.35 - <code>module load binutils/.2.35</code> binutils .2.36.1 - <code>module load binutils/.2.36.1</code> binutils .2.37 - <code>module load binutils/.2.37</code> binutils .2.38 - <code>module load binutils/.2.38</code> binutils .2.39 - <code>module load binutils/.2.39</code> GCCcore .9.3.0 - <code>module load GCCcore/.9.3.0</code> GCCcore .8.3.0 - <code>module load GCCcore/.8.3.0</code> GCCcore .10.2.0 - <code>module load GCCcore/.10.2.0</code> GCCcore .10.3.0 - <code>module load GCCcore/.10.3.0</code> GCCcore .11.2.0 - <code>module load GCCcore/.11.2.0</code> GCCcore .12.2.0 - <code>module load GCCcore/.12.2.0</code> GCC 9.3.0 foss/2020a <code>module load GCC/9.3.0</code> GCC 10.2.0 foss/2020b <code>module load GCC/10.2.0</code> GCC 10.3.0 foss/2021a <code>module load GCC/10.3.0</code> GCC 11.2.0 foss/2021b <code>module load GCC/11.2.0</code> GCC 12.2.0 foss/2022b <code>module load GCC/12.2.0</code> ncurses .6.1 - <code>module load ncurses/.6.1</code> ncurses .6.0 - <code>module load ncurses/.6.0</code> ncurses .6.2 - <code>module load ncurses/.6.2</code> ncurses .6.3 - <code>module load ncurses/.6.3</code> gettext .0.20.1 - <code>module load gettext/.0.20.1</code> gettext .0.19.8.1 - <code>module load gettext/.0.19.8.1</code> gettext .0.21 - <code>module load gettext/.0.21</code> gettext .0.21.1 - <code>module load gettext/.0.21.1</code> gompi 2020a - <code>module load gompi/2020a</code> gompi 2019b - <code>module load gompi/2019b</code> gompi 2020b - <code>module load gompi/2020b</code> gompi 2021a - <code>module load gompi/2021a</code> gompi 2021b - <code>module load gompi/2021b</code> gompi 2022b - <code>module load gompi/2022b</code> foss 2020a - <code>module load foss/2020a</code> foss 2019b - <code>module load foss/2019b</code> foss 2020b - <code>module load foss/2020b</code> foss 2021a - <code>module load foss/2021a</code> foss 2021b - <code>module load foss/2021b</code> foss 2022b - <code>module load foss/2022b</code> Eigen 3.3.7 - <code>module load Eigen/3.3.7</code> Java 11.0.2 - <code>module load Java/11.0.2</code> Java .modulerc - <code>module load Java/.modulerc</code> Java 1.8.0_281 - <code>module load Java/1.8.0_281</code> Java .modulerc.bak_20211102084550_123535 - <code>module load Java/.modulerc.bak_20211102084550_123535</code> Java 17.0.2 - <code>module load Java/17.0.2</code> CUDAcore 11.1.1 - <code>module load CUDAcore/11.1.1</code> gcccuda 2020b - <code>module load gcccuda/2020b</code> gompic 2020b - <code>module load gompic/2020b</code> sbt 1.3.13-Java-1.8 - <code>module load sbt/1.3.13-Java-1.8</code> pkg-config .0.29.2 - <code>module load pkg-config/.0.29.2</code> OpenSSL .1.1 - <code>module load OpenSSL/.1.1</code> OpenSSL 1.1 - <code>module load OpenSSL/1.1</code> iimpi .2021b - <code>module load iimpi/.2021b</code> iimpi .2021a - <code>module load iimpi/.2021a</code> imkl 2021.4.0 - <code>module load imkl/2021.4.0</code> intel-compilers 2021.4.0 intel/2021b <code>module load intel-compilers/2021.4.0</code> intel-compilers 2021.2.0 intel/2021a <code>module load intel-compilers/2021.2.0</code> intel 2021b - <code>module load intel/2021b</code> intel 2021a - <code>module load intel/2021a</code> FreeSurfer 7.2.0-centos7_x86_64 - <code>module load FreeSurfer/7.2.0-centos7_x86_64</code> Maven 3.6.3 - <code>module load Maven/3.6.3</code> CUDA 11.4.1 - <code>module load CUDA/11.4.1</code> CUDA 11.7.0 - <code>module load CUDA/11.7.0</code> ANSYS 2022 - <code>module load ANSYS/2022</code> MATLAB 2022a - <code>module load MATLAB/2022a</code> Anaconda3 2022.05 - <code>module load Anaconda3/2022.05</code> Autoconf .2.69 - <code>module load Autoconf/.2.69</code> Automake .1.15 - <code>module load Automake/.1.15</code> libtool .2.4.6 - <code>module load libtool/.2.4.6</code> Autotools .20150215 - <code>module load Autotools/.20150215</code> DMTCP 2.6.0 - <code>module load DMTCP/2.6.0</code> CMake 3.18.4 - <code>module load CMake/3.18.4</code> cuDNN 8.2.2.26-CUDA-11.4.1 - <code>module load cuDNN/8.2.2.26-CUDA-11.4.1</code> pkgconf 1.8.0 - <code>module load pkgconf/1.8.0</code> PyCharm 2021.1.1 - <code>module load PyCharm/2021.1.1</code> ant 1.10.11-Java-17.0.2 - <code>module load ant/1.10.11-Java-17.0.2</code> ant 1.10.11-Java-11 - <code>module load ant/1.10.11-Java-11</code> Julia 1.7.3-linux-x86_64 - <code>module load Julia/1.7.3-linux-x86_64</code> AutoDock-Vina 1.1.2-linux_x86 - <code>module load AutoDock-Vina/1.1.2-linux_x86</code> Tecplot360 2022 - <code>module load Tecplot360/2022</code> Miniconda3 4.9.2 - <code>module load Miniconda3/4.9.2</code> Mamba 4.14.0-0 - <code>module load Mamba/4.14.0-0</code> gfbf 2022b - <code>module load gfbf/2022b</code> zlib .1.2.11 foss/2020a <code>module load foss/2020a zlib/.1.2.11</code> help2man .1.47.12 foss/2020a <code>module load foss/2020a help2man/.1.47.12</code> M4 .1.4.18 foss/2020a <code>module load foss/2020a M4/.1.4.18</code> Bison .3.5.3 foss/2020a <code>module load foss/2020a Bison/.3.5.3</code> flex .2.6.4 foss/2020a <code>module load foss/2020a flex/.2.6.4</code> binutils .2.34 foss/2020a <code>module load foss/2020a binutils/.2.34</code> pkg-config .0.29.2 foss/2020a <code>module load foss/2020a pkg-config/.0.29.2</code> libevent 2.1.11 foss/2020a <code>module load foss/2020a libevent/2.1.11</code> libfabric .1.11.0 foss/2020a <code>module load foss/2020a libfabric/.1.11.0</code> libtool .2.4.6 foss/2020a <code>module load foss/2020a libtool/.2.4.6</code> XZ .5.2.5 foss/2020a <code>module load foss/2020a XZ/.5.2.5</code> libxml2 .2.9.10 foss/2020a <code>module load foss/2020a libxml2/.2.9.10</code> expat .2.2.9 foss/2020a <code>module load foss/2020a expat/.2.2.9</code> makeinfo .6.7 foss/2020a <code>module load foss/2020a makeinfo/.6.7</code> groff .1.22.4 foss/2020a <code>module load foss/2020a groff/.1.22.4</code> ncurses .6.2 foss/2020a <code>module load foss/2020a ncurses/.6.2</code> libreadline .8.0 foss/2020a <code>module load foss/2020a libreadline/.8.0</code> DB 18.1.32 foss/2020a <code>module load foss/2020a DB/18.1.32</code> Perl 5.30.2 foss/2020a <code>module load foss/2020a Perl/5.30.2</code> Autoconf .2.69 foss/2020a <code>module load foss/2020a Autoconf/.2.69</code> Automake .1.16.1 foss/2020a <code>module load foss/2020a Automake/.1.16.1</code> Autotools .20180311 foss/2020a <code>module load foss/2020a Autotools/.20180311</code> xorg-macros .1.19.2 foss/2020a <code>module load foss/2020a xorg-macros/.1.19.2</code> numactl .2.0.13 foss/2020a <code>module load foss/2020a numactl/.2.0.13</code> UCX 1.8.0 foss/2020a <code>module load foss/2020a UCX/1.8.0</code> libpciaccess .0.16 foss/2020a <code>module load foss/2020a libpciaccess/.0.16</code> hwloc .2.2.0 foss/2020a <code>module load foss/2020a hwloc/.2.2.0</code> PMIx 3.1.5 foss/2020a <code>module load foss/2020a PMIx/3.1.5</code> bzip2 .1.0.8 foss/2020a <code>module load foss/2020a bzip2/.1.0.8</code> cURL .7.69.1 foss/2020a <code>module load foss/2020a cURL/.7.69.1</code> CMake 3.16.4 foss/2020a <code>module load foss/2020a CMake/3.16.4</code> libpng .1.6.37 foss/2020a <code>module load foss/2020a libpng/.1.6.37</code> UnZip 6.0 foss/2020a <code>module load foss/2020a UnZip/6.0</code> GMP .6.2.0 foss/2020a <code>module load foss/2020a GMP/.6.2.0</code> NASM .2.14.02 foss/2020a <code>module load foss/2020a NASM/.2.14.02</code> Tcl 8.6.10 foss/2020a <code>module load foss/2020a Tcl/8.6.10</code> libjpeg-turbo .2.0.4 foss/2020a <code>module load foss/2020a libjpeg-turbo/.2.0.4</code> SQLite 3.31.1 foss/2020a <code>module load foss/2020a SQLite/3.31.1</code> libffi .3.3 foss/2020a <code>module load foss/2020a libffi/.3.3</code> Python 3.8.2 foss/2020a <code>module load foss/2020a Python/3.8.2</code> Python 2.7.18 foss/2020a <code>module load foss/2020a Python/2.7.18</code> archspec 0.1.0-Python-3.8.2 foss/2020a <code>module load foss/2020a archspec/0.1.0-Python-3.8.2</code> gzip 1.10 foss/2020a <code>module load foss/2020a gzip/1.10</code> libiconv 1.16 foss/2020a <code>module load foss/2020a libiconv/1.16</code> Doxygen .1.8.17 foss/2020a <code>module load foss/2020a Doxygen/.1.8.17</code> tbb 2020.1 foss/2020a <code>module load foss/2020a tbb/2020.1</code> Szip .2.1.1 foss/2020a <code>module load foss/2020a Szip/.2.1.1</code> PCRE .8.44 foss/2020a <code>module load foss/2020a PCRE/.8.44</code> Voro++ 0.4.6 foss/2020a <code>module load foss/2020a Voro++/0.4.6</code> x264 20191217 foss/2020a <code>module load foss/2020a x264/20191217</code> LAME 3.100 foss/2020a <code>module load foss/2020a LAME/3.100</code> Eigen 3.3.7 foss/2020a <code>module load foss/2020a Eigen/3.3.7</code> Yasm 1.3.0 foss/2020a <code>module load foss/2020a Yasm/1.3.0</code> x265 3.3 foss/2020a <code>module load foss/2020a x265/3.3</code> freetype .2.10.1 foss/2020a <code>module load foss/2020a freetype/.2.10.1</code> gettext .0.20.1 foss/2020a <code>module load foss/2020a gettext/.0.20.1</code> intltool 0.51.0 foss/2020a <code>module load foss/2020a intltool/0.51.0</code> pkgconfig 1.5.1-Python-3.8.2 foss/2020a <code>module load foss/2020a pkgconfig/1.5.1-Python-3.8.2</code> FriBidi 1.0.9 foss/2020a <code>module load foss/2020a FriBidi/1.0.9</code> gperf 3.1 foss/2020a <code>module load foss/2020a gperf/3.1</code> Ninja 1.10.0 foss/2020a <code>module load foss/2020a Ninja/1.10.0</code> util-linux 2.35 foss/2020a <code>module load foss/2020a util-linux/2.35</code> pybind11 2.4.3-Python-3.8.2 foss/2020a <code>module load foss/2020a pybind11/2.4.3-Python-3.8.2</code> libunistring 0.9.10 foss/2020a <code>module load foss/2020a libunistring/0.9.10</code> fontconfig .2.13.92 foss/2020a <code>module load foss/2020a fontconfig/.2.13.92</code> Meson 0.55.1-Python-3.8.2 foss/2020a <code>module load foss/2020a Meson/0.55.1-Python-3.8.2</code> X11 .20200222 foss/2020a <code>module load foss/2020a X11/.20200222</code> FFmpeg 4.2.2 foss/2020a <code>module load foss/2020a FFmpeg/4.2.2</code> gc 7.6.12 foss/2020a <code>module load foss/2020a gc/7.6.12</code> Tk .8.6.10 foss/2020a <code>module load foss/2020a Tk/.8.6.10</code> Guile 1.8.8 foss/2020a <code>module load foss/2020a Guile/1.8.8</code> Tkinter 3.8.2 foss/2020a <code>module load foss/2020a Tkinter/3.8.2</code> libmatheval 1.1.11 foss/2020a <code>module load foss/2020a libmatheval/1.1.11</code> Mako 1.1.2 foss/2020a <code>module load foss/2020a Mako/1.1.2</code> libdrm 2.4.100 foss/2020a <code>module load foss/2020a libdrm/2.4.100</code> lz4 1.9.2 foss/2020a <code>module load foss/2020a lz4/1.9.2</code> zstd 1.4.4 foss/2020a <code>module load foss/2020a zstd/1.4.4</code> libglvnd 1.2.0 foss/2020a <code>module load foss/2020a libglvnd/1.2.0</code> libunwind 1.3.1 foss/2020a <code>module load foss/2020a libunwind/1.3.1</code> LLVM 9.0.1 foss/2020a <code>module load foss/2020a LLVM/9.0.1</code> Mesa 20.0.2 foss/2020a <code>module load foss/2020a Mesa/20.0.2</code> libGLU 9.0.1 foss/2020a <code>module load foss/2020a libGLU/9.0.1</code> METIS 5.1.0 foss/2020a <code>module load foss/2020a METIS/5.1.0</code> MPFR 4.0.2 foss/2020a <code>module load foss/2020a MPFR/4.0.2</code> re2c 1.3 foss/2020a <code>module load foss/2020a re2c/1.3</code> libgd 2.3.0 foss/2020a <code>module load foss/2020a libgd/2.3.0</code> double-conversion 3.1.5 foss/2020a <code>module load foss/2020a double-conversion/3.1.5</code> pixman .0.38.4 foss/2020a <code>module load foss/2020a pixman/.0.38.4</code> GLib .2.64.1 foss/2020a <code>module load foss/2020a GLib/.2.64.1</code> cairo .1.16.0 foss/2020a <code>module load foss/2020a cairo/.1.16.0</code> libcerf 1.13 foss/2020a <code>module load foss/2020a libcerf/1.13</code> PCRE2 10.34 foss/2020a <code>module load foss/2020a PCRE2/10.34</code> GObject-Introspection 1.64.0-Python-3.8.2 foss/2020a <code>module load foss/2020a GObject-Introspection/1.64.0-Python-3.8.2</code> Lua 5.3.5 foss/2020a <code>module load foss/2020a Lua/5.3.5</code> DBus 1.13.12 foss/2020a <code>module load foss/2020a DBus/1.13.12</code> ICU 66.1 foss/2020a <code>module load foss/2020a ICU/66.1</code> HarfBuzz 2.6.4 foss/2020a <code>module load foss/2020a HarfBuzz/2.6.4</code> Pango 1.44.7 foss/2020a <code>module load foss/2020a Pango/1.44.7</code> snappy 1.1.8 foss/2020a <code>module load foss/2020a snappy/1.1.8</code> NSPR 4.25 foss/2020a <code>module load foss/2020a NSPR/4.25</code> NSS 3.51 foss/2020a <code>module load foss/2020a NSS/3.51</code> JasPer 2.0.14 foss/2020a <code>module load foss/2020a JasPer/2.0.14</code> Qt5 5.14.1 foss/2020a <code>module load foss/2020a Qt5/5.14.1</code> gnuplot 5.2.8 foss/2020a <code>module load foss/2020a gnuplot/5.2.8</code> SCons 3.1.2 foss/2020a <code>module load foss/2020a SCons/3.1.2</code> Flask 1.1.2-Python-3.8.2 foss/2020a <code>module load foss/2020a Flask/1.1.2-Python-3.8.2</code> parallel 20200422 foss/2020a <code>module load foss/2020a parallel/20200422</code> FLTK 1.3.5 foss/2020a <code>module load foss/2020a FLTK/1.3.5</code> xprop 1.2.4 foss/2020a <code>module load foss/2020a xprop/1.2.4</code> help2man .1.47.16 foss/2020b <code>module load foss/2020b help2man/.1.47.16</code> M4 .1.4.18 foss/2020b <code>module load foss/2020b M4/.1.4.18</code> Bison .3.7.1 foss/2020b <code>module load foss/2020b Bison/.3.7.1</code> flex .2.6.4 foss/2020b <code>module load foss/2020b flex/.2.6.4</code> zlib .1.2.11 foss/2020b <code>module load foss/2020b zlib/.1.2.11</code> binutils .2.35 foss/2020b <code>module load foss/2020b binutils/.2.35</code> libtool .2.4.6 foss/2020b <code>module load foss/2020b libtool/.2.4.6</code> ncurses .6.2 foss/2020b <code>module load foss/2020b ncurses/.6.2</code> bzip2 .1.0.8 foss/2020b <code>module load foss/2020b bzip2/.1.0.8</code> cURL .7.72.0 foss/2020b <code>module load foss/2020b cURL/.7.72.0</code> expat .2.2.9 foss/2020b <code>module load foss/2020b expat/.2.2.9</code> libreadline .8.0 foss/2020b <code>module load foss/2020b libreadline/.8.0</code> pkg-config .0.29.2 foss/2020b <code>module load foss/2020b pkg-config/.0.29.2</code> DB 18.1.40 foss/2020b <code>module load foss/2020b DB/18.1.40</code> UnZip 6.0 foss/2020b <code>module load foss/2020b UnZip/6.0</code> Perl 5.32.0 foss/2020b <code>module load foss/2020b Perl/5.32.0</code> Autoconf .2.69 foss/2020b <code>module load foss/2020b Autoconf/.2.69</code> Automake .1.16.2 foss/2020b <code>module load foss/2020b Automake/.1.16.2</code> XZ .5.2.5 foss/2020b <code>module load foss/2020b XZ/.5.2.5</code> libarchive .3.4.3 foss/2020b <code>module load foss/2020b libarchive/.3.4.3</code> Autotools .20200321 foss/2020b <code>module load foss/2020b Autotools/.20200321</code> GMP .6.2.0 foss/2020b <code>module load foss/2020b GMP/.6.2.0</code> CMake 3.18.4 foss/2020b <code>module load foss/2020b CMake/3.18.4</code> Eigen 3.3.8 foss/2020b <code>module load foss/2020b Eigen/3.3.8</code> libffi .3.3 foss/2020b <code>module load foss/2020b libffi/.3.3</code> Tcl 8.6.10 foss/2020b <code>module load foss/2020b Tcl/8.6.10</code> SQLite 3.33.0 foss/2020b <code>module load foss/2020b SQLite/3.33.0</code> Python 3.8.6 foss/2020b <code>module load foss/2020b Python/3.8.6</code> Python 2.7.18 foss/2020b <code>module load foss/2020b Python/2.7.18</code> hypothesis 5.41.2 foss/2020b <code>module load foss/2020b hypothesis/5.41.2</code> pybind11 2.6.0 foss/2020b <code>module load foss/2020b pybind11/2.6.0</code> libevent 2.1.12 foss/2020b <code>module load foss/2020b libevent/2.1.12</code> numactl .2.0.13 foss/2020b <code>module load foss/2020b numactl/.2.0.13</code> UCX 1.9.0 foss/2020b <code>module load foss/2020b UCX/1.9.0</code> UCX 1.9.0-CUDA-11.1.1 foss/2020b <code>module load foss/2020b UCX/1.9.0-CUDA-11.1.1</code> libxml2 .2.9.10 foss/2020b <code>module load foss/2020b libxml2/.2.9.10</code> libfabric .1.11.0 foss/2020b <code>module load foss/2020b libfabric/.1.11.0</code> xorg-macros .1.19.2 foss/2020b <code>module load foss/2020b xorg-macros/.1.19.2</code> libpciaccess .0.16 foss/2020b <code>module load foss/2020b libpciaccess/.0.16</code> hwloc .2.2.0 foss/2020b <code>module load foss/2020b hwloc/.2.2.0</code> PMIx 3.1.5 foss/2020b <code>module load foss/2020b PMIx/3.1.5</code> Check 0.15.2 foss/2020b <code>module load foss/2020b Check/0.15.2</code> GDRCopy 2.1-CUDA-11.1.1 foss/2020b <code>module load foss/2020b GDRCopy/2.1-CUDA-11.1.1</code> zlib .1.2.11 foss/2021a <code>module load foss/2021a zlib/.1.2.11</code> help2man .1.48.3 foss/2021a <code>module load foss/2021a help2man/.1.48.3</code> M4 .1.4.18 foss/2021a <code>module load foss/2021a M4/.1.4.18</code> Bison .3.7.6 foss/2021a <code>module load foss/2021a Bison/.3.7.6</code> flex .2.6.4 foss/2021a <code>module load foss/2021a flex/.2.6.4</code> binutils .2.36.1 foss/2021a <code>module load foss/2021a binutils/.2.36.1</code> pixman .0.40.0 foss/2021a <code>module load foss/2021a pixman/.0.40.0</code> bzip2 .1.0.8 foss/2021a <code>module load foss/2021a bzip2/.1.0.8</code> pkg-config .0.29.2 foss/2021a <code>module load foss/2021a pkg-config/.0.29.2</code> UnZip 6.0 foss/2021a <code>module load foss/2021a UnZip/6.0</code> ncurses .6.2 foss/2021a <code>module load foss/2021a ncurses/.6.2</code> libtool .2.4.6 foss/2021a <code>module load foss/2021a libtool/.2.4.6</code> libpng .1.6.37 foss/2021a <code>module load foss/2021a libpng/.1.6.37</code> XZ .5.2.5 foss/2021a <code>module load foss/2021a XZ/.5.2.5</code> libreadline .8.1 foss/2021a <code>module load foss/2021a libreadline/.8.1</code> libxml2 .2.9.10 foss/2021a <code>module load foss/2021a libxml2/.2.9.10</code> libunwind 1.4.0 foss/2021a <code>module load foss/2021a libunwind/1.4.0</code> gettext .0.21 foss/2021a <code>module load foss/2021a gettext/.0.21</code> freetype .2.10.4 foss/2021a <code>module load foss/2021a freetype/.2.10.4</code> expat .2.2.9 foss/2021a <code>module load foss/2021a expat/.2.2.9</code> gperf 3.1 foss/2021a <code>module load foss/2021a gperf/3.1</code> PCRE2 10.36 foss/2021a <code>module load foss/2021a PCRE2/10.36</code> libffi .3.3 foss/2021a <code>module load foss/2021a libffi/.3.3</code> util-linux 2.36 foss/2021a <code>module load foss/2021a util-linux/2.36</code> Tcl 8.6.11 foss/2021a <code>module load foss/2021a Tcl/8.6.11</code> Perl 5.32.1-minimal foss/2021a <code>module load foss/2021a Perl/5.32.1-minimal</code> Perl 5.32.1 foss/2021a <code>module load foss/2021a Perl/5.32.1</code> SQLite 3.35.4 foss/2021a <code>module load foss/2021a SQLite/3.35.4</code> cURL .7.76.0 foss/2021a <code>module load foss/2021a cURL/.7.76.0</code> makeinfo .6.7-minimal foss/2021a <code>module load foss/2021a makeinfo/.6.7-minimal</code> libarchive .3.5.1 foss/2021a <code>module load foss/2021a libarchive/.3.5.1</code> DB 18.1.40 foss/2021a <code>module load foss/2021a DB/18.1.40</code> CMake 3.20.1 foss/2021a <code>module load foss/2021a CMake/3.20.1</code> Python 3.9.5-bare foss/2021a <code>module load foss/2021a Python/3.9.5-bare</code> Python 3.9.5 foss/2021a <code>module load foss/2021a Python/3.9.5</code> Python 2.7.18-bare foss/2021a <code>module load foss/2021a Python/2.7.18-bare</code> groff .1.22.4 foss/2021a <code>module load foss/2021a groff/.1.22.4</code> Rust 1.52.1 foss/2021a <code>module load foss/2021a Rust/1.52.1</code> fontconfig .2.13.93 foss/2021a <code>module load foss/2021a fontconfig/.2.13.93</code> Autoconf .2.71 foss/2021a <code>module load foss/2021a Autoconf/.2.71</code> intltool 0.51.0 foss/2021a <code>module load foss/2021a intltool/0.51.0</code> Automake .1.16.3 foss/2021a <code>module load foss/2021a Automake/.1.16.3</code> Automake 1.16.3 foss/2021a <code>module load foss/2021a Automake/1.16.3</code> Autotools .20210128 foss/2021a <code>module load foss/2021a Autotools/.20210128</code> GMP .6.2.1 foss/2021a <code>module load foss/2021a GMP/.6.2.1</code> xorg-macros .1.19.3 foss/2021a <code>module load foss/2021a xorg-macros/.1.19.3</code> nettle 3.7.2 foss/2021a <code>module load foss/2021a nettle/3.7.2</code> Ninja 1.10.2 foss/2021a <code>module load foss/2021a Ninja/1.10.2</code> Mako 1.1.4 foss/2021a <code>module load foss/2021a Mako/1.1.4</code> Meson 0.58.0 foss/2021a <code>module load foss/2021a Meson/0.58.0</code> GLib .2.68.2 foss/2021a <code>module load foss/2021a GLib/.2.68.2</code> libpciaccess .0.16 foss/2021a <code>module load foss/2021a libpciaccess/.0.16</code> gzip 1.10 foss/2021a <code>module load foss/2021a gzip/1.10</code> X11 .20210518 foss/2021a <code>module load foss/2021a X11/.20210518</code> libdrm 2.4.106 foss/2021a <code>module load foss/2021a libdrm/2.4.106</code> cairo .1.16.0 foss/2021a <code>module load foss/2021a cairo/.1.16.0</code> libglvnd 1.3.3 foss/2021a <code>module load foss/2021a libglvnd/1.3.3</code> LLVM 11.1.0 foss/2021a <code>module load foss/2021a LLVM/11.1.0</code> lz4 1.9.3 foss/2021a <code>module load foss/2021a lz4/1.9.3</code> NASM .2.15.05 foss/2021a <code>module load foss/2021a NASM/.2.15.05</code> zstd 1.4.9 foss/2021a <code>module load foss/2021a zstd/1.4.9</code> libjpeg-turbo .2.0.6 foss/2021a <code>module load foss/2021a libjpeg-turbo/.2.0.6</code> LibTIFF .4.2.0 foss/2021a <code>module load foss/2021a LibTIFF/.4.2.0</code> Mesa 21.1.1 foss/2021a <code>module load foss/2021a Mesa/21.1.1</code> libGLU 9.0.1 foss/2021a <code>module load foss/2021a libGLU/9.0.1</code> Xvfb 1.20.11 foss/2021a <code>module load foss/2021a Xvfb/1.20.11</code> Tk .8.6.11 foss/2021a <code>module load foss/2021a Tk/.8.6.11</code> NLopt 2.7.0 foss/2021a <code>module load foss/2021a NLopt/2.7.0</code> ICU 69.1 foss/2021a <code>module load foss/2021a ICU/69.1</code> libogg 1.3.4 foss/2021a <code>module load foss/2021a libogg/1.3.4</code> FLAC 1.3.3 foss/2021a <code>module load foss/2021a FLAC/1.3.3</code> libvorbis 1.3.7 foss/2021a <code>module load foss/2021a libvorbis/1.3.7</code> libsndfile 1.0.31 foss/2021a <code>module load foss/2021a libsndfile/1.0.31</code> UDUNITS 2.2.28 foss/2021a <code>module load foss/2021a UDUNITS/2.2.28</code> Szip .2.1.1 foss/2021a <code>module load foss/2021a Szip/.2.1.1</code> libevent 2.1.12 foss/2021a <code>module load foss/2021a libevent/2.1.12</code> numactl .2.0.14 foss/2021a <code>module load foss/2021a numactl/.2.0.14</code> hwloc .2.4.1 foss/2021a <code>module load foss/2021a hwloc/.2.4.1</code> UCX 1.10.0 foss/2021a <code>module load foss/2021a UCX/1.10.0</code> GLPK 5.0 foss/2021a <code>module load foss/2021a GLPK/5.0</code> libfabric .1.12.1 foss/2021a <code>module load foss/2021a libfabric/.1.12.1</code> Ghostscript 9.54.0 foss/2021a <code>module load foss/2021a Ghostscript/9.54.0</code> nodejs 14.17.0 foss/2021a <code>module load foss/2021a nodejs/14.17.0</code> PMIx 3.2.3 foss/2021a <code>module load foss/2021a PMIx/3.2.3</code> JasPer 2.0.28 foss/2021a <code>module load foss/2021a JasPer/2.0.28</code> LittleCMS 2.12 foss/2021a <code>module load foss/2021a LittleCMS/2.12</code> ImageMagick 7.0.11-14 foss/2021a <code>module load foss/2021a ImageMagick/7.0.11-14</code> PCRE .8.44 foss/2021a <code>module load foss/2021a PCRE/.8.44</code> libgit2 1.1.0 foss/2021a <code>module load foss/2021a libgit2/1.1.0</code> PROJ .8.0.1 foss/2021a <code>module load foss/2021a PROJ/.8.0.1</code> libiconv 1.16 foss/2021a <code>module load foss/2021a libiconv/1.16</code> Doxygen .1.9.1 foss/2021a <code>module load foss/2021a Doxygen/.1.9.1</code> libgeotiff 1.6.0 foss/2021a <code>module load foss/2021a libgeotiff/1.6.0</code> hypothesis 6.13.1 foss/2021a <code>module load foss/2021a hypothesis/6.13.1</code> libtirpc 1.3.2 foss/2021a <code>module load foss/2021a libtirpc/1.3.2</code> HDF 4.2.15 foss/2021a <code>module load foss/2021a HDF/4.2.15</code> Eigen 3.3.9 foss/2021a <code>module load foss/2021a Eigen/3.3.9</code> pybind11 2.6.2 foss/2021a <code>module load foss/2021a pybind11/2.6.2</code> SCons 4.1.0.post1 foss/2021a <code>module load foss/2021a SCons/4.1.0.post1</code> SWIG 4.0.2 foss/2021a <code>module load foss/2021a SWIG/4.0.2</code> libgd 2.3.1 foss/2021a <code>module load foss/2021a libgd/2.3.1</code> freeglut 3.2.1 foss/2021a <code>module load foss/2021a freeglut/3.2.1</code> mm-common 1.0.4 foss/2021a <code>module load foss/2021a mm-common/1.0.4</code> libxslt 1.1.34 foss/2021a <code>module load foss/2021a libxslt/1.1.34</code> pkgconfig 1.5.4-python foss/2021a <code>module load foss/2021a pkgconfig/1.5.4-python</code> re2c 2.1.1 foss/2021a <code>module load foss/2021a re2c/2.1.1</code> FriBidi 1.0.10 foss/2021a <code>module load foss/2021a FriBidi/1.0.10</code> double-conversion 3.1.5 foss/2021a <code>module load foss/2021a double-conversion/3.1.5</code> DBus 1.13.18 foss/2021a <code>module load foss/2021a DBus/1.13.18</code> GTS 0.7.6 foss/2021a <code>module load foss/2021a GTS/0.7.6</code> GObject-Introspection 1.68.0 foss/2021a <code>module load foss/2021a GObject-Introspection/1.68.0</code> Gdk-Pixbuf 2.42.6 foss/2021a <code>module load foss/2021a Gdk-Pixbuf/2.42.6</code> snappy 1.1.8 foss/2021a <code>module load foss/2021a snappy/1.1.8</code> NSPR 4.30 foss/2021a <code>module load foss/2021a NSPR/4.30</code> HarfBuzz 2.8.1 foss/2021a <code>module load foss/2021a HarfBuzz/2.8.1</code> NSS 3.65 foss/2021a <code>module load foss/2021a NSS/3.65</code> Pango 1.48.5 foss/2021a <code>module load foss/2021a Pango/1.48.5</code> zlib .1.2.11 foss/2021b <code>module load foss/2021b zlib/.1.2.11</code> zlib 1.2.11 foss/2021b <code>module load foss/2021b zlib/1.2.11</code> help2man .1.48.3 foss/2021b <code>module load foss/2021b help2man/.1.48.3</code> M4 .1.4.19 foss/2021b <code>module load foss/2021b M4/.1.4.19</code> M4 1.4.19 foss/2021b <code>module load foss/2021b M4/1.4.19</code> Bison .3.7.6 foss/2021b <code>module load foss/2021b Bison/.3.7.6</code> Bison 3.7.6 foss/2021b <code>module load foss/2021b Bison/3.7.6</code> flex .2.6.4 foss/2021b <code>module load foss/2021b flex/.2.6.4</code> flex 2.6.4 foss/2021b <code>module load foss/2021b flex/2.6.4</code> binutils .2.37 foss/2021b <code>module load foss/2021b binutils/.2.37</code> pkg-config .0.29.2 foss/2021b <code>module load foss/2021b pkg-config/.0.29.2</code> pkg-config 0.29.2 foss/2021b <code>module load foss/2021b pkg-config/0.29.2</code> libtool .2.4.6 foss/2021b <code>module load foss/2021b libtool/.2.4.6</code> groff .1.22.4 foss/2021b <code>module load foss/2021b groff/.1.22.4</code> expat .2.4.1 foss/2021b <code>module load foss/2021b expat/.2.4.1</code> ncurses .6.2 foss/2021b <code>module load foss/2021b ncurses/.6.2</code> libreadline .8.1 foss/2021b <code>module load foss/2021b libreadline/.8.1</code> DB 18.1.40 foss/2021b <code>module load foss/2021b DB/18.1.40</code> Perl 5.34.0 foss/2021b <code>module load foss/2021b Perl/5.34.0</code> Autoconf .2.71 foss/2021b <code>module load foss/2021b Autoconf/.2.71</code> Automake .1.16.4 foss/2021b <code>module load foss/2021b Automake/.1.16.4</code> Autotools .20210726 foss/2021b <code>module load foss/2021b Autotools/.20210726</code> Autotools 20210726 foss/2021b <code>module load foss/2021b Autotools/20210726</code> numactl .2.0.14 foss/2021b <code>module load foss/2021b numactl/.2.0.14</code> numactl 2.0.14 foss/2021b <code>module load foss/2021b numactl/2.0.14</code> UCX 1.11.2 foss/2021b <code>module load foss/2021b UCX/1.11.2</code> libevent 2.1.12 foss/2021b <code>module load foss/2021b libevent/2.1.12</code> libfabric .1.13.2 foss/2021b <code>module load foss/2021b libfabric/.1.13.2</code> bzip2 .1.0.8 foss/2021b <code>module load foss/2021b bzip2/.1.0.8</code> bzip2 1.0.8 foss/2021b <code>module load foss/2021b bzip2/1.0.8</code> XZ .5.2.5 foss/2021b <code>module load foss/2021b XZ/.5.2.5</code> libxml2 .2.9.10 foss/2021b <code>module load foss/2021b libxml2/.2.9.10</code> libxml2 2.9.10 foss/2021b <code>module load foss/2021b libxml2/2.9.10</code> cURL .7.78.0 foss/2021b <code>module load foss/2021b cURL/.7.78.0</code> UnZip 6.0 foss/2021b <code>module load foss/2021b UnZip/6.0</code> xorg-macros .1.19.3 foss/2021b <code>module load foss/2021b xorg-macros/.1.19.3</code> libpciaccess .0.16 foss/2021b <code>module load foss/2021b libpciaccess/.0.16</code> hwloc .2.5.0 foss/2021b <code>module load foss/2021b hwloc/.2.5.0</code> PMIx 4.1.0 foss/2021b <code>module load foss/2021b PMIx/4.1.0</code> libarchive .3.5.1 foss/2021b <code>module load foss/2021b libarchive/.3.5.1</code> CMake 3.21.1 foss/2021b <code>module load foss/2021b CMake/3.21.1</code> CMake 3.22.1 foss/2021b <code>module load foss/2021b CMake/3.22.1</code> gettext .0.21 foss/2021b <code>module load foss/2021b gettext/.0.21</code> git 2.33.1-nodocs foss/2021b <code>module load foss/2021b git/2.33.1-nodocs</code> GMP .6.2.1 foss/2021b <code>module load foss/2021b GMP/.6.2.1</code> GMP 6.2.1 foss/2021b <code>module load foss/2021b GMP/6.2.1</code> libffi .3.4.2 foss/2021b <code>module load foss/2021b libffi/.3.4.2</code> Tcl 8.6.11 foss/2021b <code>module load foss/2021b Tcl/8.6.11</code> SQLite 3.36 foss/2021b <code>module load foss/2021b SQLite/3.36</code> Python 3.9.6-bare foss/2021b <code>module load foss/2021b Python/3.9.6-bare</code> Python 3.9.6 foss/2021b <code>module load foss/2021b Python/3.9.6</code> Python 2.7.18-bare foss/2021b <code>module load foss/2021b Python/2.7.18-bare</code> Rust 1.54.0 foss/2021b <code>module load foss/2021b Rust/1.54.0</code> SCons 4.2.0 foss/2021b <code>module load foss/2021b SCons/4.2.0</code> Szip .2.1.1 foss/2021b <code>module load foss/2021b Szip/.2.1.1</code> Xerces-C++ 3.2.3 foss/2021b <code>module load foss/2021b Xerces-C++/3.2.3</code> PCRE .8.45 foss/2021b <code>module load foss/2021b PCRE/.8.45</code> SWIG 4.0.2 foss/2021b <code>module load foss/2021b SWIG/4.0.2</code> Qhull 2020.2 foss/2021b <code>module load foss/2021b Qhull/2020.2</code> Eigen 3.4.0 foss/2021b <code>module load foss/2021b Eigen/3.4.0</code> Eigen 3.3.9 foss/2021b <code>module load foss/2021b Eigen/3.3.9</code> PAPI 6.0.0.1 foss/2021b <code>module load foss/2021b PAPI/6.0.0.1</code> intltool 0.51.0 foss/2021b <code>module load foss/2021b intltool/0.51.0</code> libpng .1.6.37 foss/2021b <code>module load foss/2021b libpng/.1.6.37</code> Ninja 1.10.2 foss/2021b <code>module load foss/2021b Ninja/1.10.2</code> libiconv 1.16 foss/2021b <code>module load foss/2021b libiconv/1.16</code> Meson 0.58.2 foss/2021b <code>module load foss/2021b Meson/0.58.2</code> Doxygen .1.9.1 foss/2021b <code>module load foss/2021b Doxygen/.1.9.1</code> Doxygen 1.9.1 foss/2021b <code>module load foss/2021b Doxygen/1.9.1</code> JasPer 2.0.33 foss/2021b <code>module load foss/2021b JasPer/2.0.33</code> Mako 1.1.4 foss/2021b <code>module load foss/2021b Mako/1.1.4</code> NASM .2.15.05 foss/2021b <code>module load foss/2021b NASM/.2.15.05</code> gperf 3.1 foss/2021b <code>module load foss/2021b gperf/3.1</code> libjpeg-turbo .2.0.6 foss/2021b <code>module load foss/2021b libjpeg-turbo/.2.0.6</code> util-linux 2.37 foss/2021b <code>module load foss/2021b util-linux/2.37</code> jbigkit 2.1 foss/2021b <code>module load foss/2021b jbigkit/2.1</code> Brotli 1.0.9 foss/2021b <code>module load foss/2021b Brotli/1.0.9</code> gzip 1.10 foss/2021b <code>module load foss/2021b gzip/1.10</code> freetype .2.11.0 foss/2021b <code>module load foss/2021b freetype/.2.11.0</code> fontconfig .2.13.94 foss/2021b <code>module load foss/2021b fontconfig/.2.13.94</code> X11 .20210802 foss/2021b <code>module load foss/2021b X11/.20210802</code> X11 20210802 foss/2021b <code>module load foss/2021b X11/20210802</code> libdrm 2.4.107 foss/2021b <code>module load foss/2021b libdrm/2.4.107</code> libglvnd 1.3.3 foss/2021b <code>module load foss/2021b libglvnd/1.3.3</code> lz4 1.9.3 foss/2021b <code>module load foss/2021b lz4/1.9.3</code> zstd 1.5.0 foss/2021b <code>module load foss/2021b zstd/1.5.0</code> LibTIFF .4.3.0 foss/2021b <code>module load foss/2021b LibTIFF/.4.3.0</code> PROJ .8.1.0 foss/2021b <code>module load foss/2021b PROJ/.8.1.0</code> PROJ 8.1.0 foss/2021b <code>module load foss/2021b PROJ/8.1.0</code> libgeotiff 1.7.0 foss/2021b <code>module load foss/2021b libgeotiff/1.7.0</code> libunwind 1.5.0 foss/2021b <code>module load foss/2021b libunwind/1.5.0</code> hypothesis 6.14.6 foss/2021b <code>module load foss/2021b hypothesis/6.14.6</code> LLVM 12.0.1 foss/2021b <code>module load foss/2021b LLVM/12.0.1</code> libtirpc 1.3.2 foss/2021b <code>module load foss/2021b libtirpc/1.3.2</code> Mesa 21.1.7 foss/2021b <code>module load foss/2021b Mesa/21.1.7</code> libGLU 9.0.2 foss/2021b <code>module load foss/2021b libGLU/9.0.2</code> freeglut 3.2.1 foss/2021b <code>module load foss/2021b freeglut/3.2.1</code> HDF 4.2.15 foss/2021b <code>module load foss/2021b HDF/4.2.15</code> GL2PS 1.4.2 foss/2021b <code>module load foss/2021b GL2PS/1.4.2</code> pybind11 2.7.1 foss/2021b <code>module load foss/2021b pybind11/2.7.1</code> DCMTK 3.6.6 foss/2021b <code>module load foss/2021b DCMTK/3.6.6</code> OpenEXR 3.1.1 foss/2021b <code>module load foss/2021b OpenEXR/3.1.1</code> SDL2 2.0.20 foss/2021b <code>module load foss/2021b SDL2/2.0.20</code> pkgconf 1.8.0 foss/2021b <code>module load foss/2021b pkgconf/1.8.0</code> giflib 5.2.1 foss/2021b <code>module load foss/2021b giflib/5.2.1</code> NSPR 4.32 foss/2021b <code>module load foss/2021b NSPR/4.32</code> pixman .0.40.0 foss/2021b <code>module load foss/2021b pixman/.0.40.0</code> NSS 3.69 foss/2021b <code>module load foss/2021b NSS/3.69</code> GLib .2.69.1 foss/2021b <code>module load foss/2021b GLib/.2.69.1</code> GLib 2.69.1 foss/2021b <code>module load foss/2021b GLib/2.69.1</code> cairo .1.16.0 foss/2021b <code>module load foss/2021b cairo/.1.16.0</code> GObject-Introspection 1.68.0 foss/2021b <code>module load foss/2021b GObject-Introspection/1.68.0</code> Gdk-Pixbuf 2.42.6 foss/2021b <code>module load foss/2021b Gdk-Pixbuf/2.42.6</code> OpenJPEG 2.4.0 foss/2021b <code>module load foss/2021b OpenJPEG/2.4.0</code> re2c 2.2 foss/2021b <code>module load foss/2021b re2c/2.2</code> ICU 69.1 foss/2021b <code>module load foss/2021b ICU/69.1</code> HarfBuzz 2.8.2 foss/2021b <code>module load foss/2021b HarfBuzz/2.8.2</code> double-conversion 3.1.5 foss/2021b <code>module load foss/2021b double-conversion/3.1.5</code> FriBidi 1.0.10 foss/2021b <code>module load foss/2021b FriBidi/1.0.10</code> Pango 1.48.8 foss/2021b <code>module load foss/2021b Pango/1.48.8</code> librsvg 2.52.8 foss/2021b <code>module load foss/2021b librsvg/2.52.8</code> PCRE2 10.37 foss/2021b <code>module load foss/2021b PCRE2/10.37</code> graphite2 1.3.14 foss/2021b <code>module load foss/2021b graphite2/1.3.14</code> DBus 1.13.18 foss/2021b <code>module load foss/2021b DBus/1.13.18</code> snappy 1.1.9 foss/2021b <code>module load foss/2021b snappy/1.1.9</code> Qt5 5.15.2 foss/2021b <code>module load foss/2021b Qt5/5.15.2</code> x264 20210613 foss/2021b <code>module load foss/2021b x264/20210613</code> LAME 3.100 foss/2021b <code>module load foss/2021b LAME/3.100</code> Yasm 1.3.0 foss/2021b <code>module load foss/2021b Yasm/1.3.0</code> x265 3.5 foss/2021b <code>module load foss/2021b x265/3.5</code> FFmpeg 4.3.2 foss/2021b <code>module load foss/2021b FFmpeg/4.3.2</code> scikit-build 0.11.1 foss/2021b <code>module load foss/2021b scikit-build/0.11.1</code> GDRCopy 2.3 foss/2021b <code>module load foss/2021b GDRCopy/2.3</code> UCX-CUDA 1.11.2-CUDA-11.4.1 foss/2021b <code>module load foss/2021b UCX-CUDA/1.11.2-CUDA-11.4.1</code> Tk .8.6.11 foss/2021b <code>module load foss/2021b Tk/.8.6.11</code> Tk 8.6.11 foss/2021b <code>module load foss/2021b Tk/8.6.11</code> nettle 3.7.3 foss/2021b <code>module load foss/2021b nettle/3.7.3</code> Xvfb 1.20.13 foss/2021b <code>module load foss/2021b Xvfb/1.20.13</code> NLopt 2.7.0 foss/2021b <code>module load foss/2021b NLopt/2.7.0</code> UDUNITS 2.2.28 foss/2021b <code>module load foss/2021b UDUNITS/2.2.28</code> libogg 1.3.5 foss/2021b <code>module load foss/2021b libogg/1.3.5</code> FLAC 1.3.3 foss/2021b <code>module load foss/2021b FLAC/1.3.3</code> libvorbis 1.3.7 foss/2021b <code>module load foss/2021b libvorbis/1.3.7</code> libsndfile 1.0.31 foss/2021b <code>module load foss/2021b libsndfile/1.0.31</code> GLPK 5.0 foss/2021b <code>module load foss/2021b GLPK/5.0</code> Ghostscript 9.54.0 foss/2021b <code>module load foss/2021b Ghostscript/9.54.0</code> nodejs 14.17.6 foss/2021b <code>module load foss/2021b nodejs/14.17.6</code> LittleCMS 2.12 foss/2021b <code>module load foss/2021b LittleCMS/2.12</code> ImageMagick 7.1.0-4 foss/2021b <code>module load foss/2021b ImageMagick/7.1.0-4</code> MPFR 4.1.0 foss/2021b <code>module load foss/2021b MPFR/4.1.0</code> libgit2 1.1.1 foss/2021b <code>module load foss/2021b libgit2/1.1.1</code> protobuf 3.17.3 foss/2021b <code>module load foss/2021b protobuf/3.17.3</code> Zip 3.0 foss/2021b <code>module load foss/2021b Zip/3.0</code> Bazel 3.7.2 foss/2021b <code>module load foss/2021b Bazel/3.7.2</code> METIS 5.1.0 foss/2021b <code>module load foss/2021b METIS/5.1.0</code> ATK 2.36.0 foss/2021b <code>module load foss/2021b ATK/2.36.0</code> libepoxy 1.5.8 foss/2021b <code>module load foss/2021b libepoxy/1.5.8</code> at-spi2-core 2.40.3 foss/2021b <code>module load foss/2021b at-spi2-core/2.40.3</code> at-spi2-atk 2.38.0 foss/2021b <code>module load foss/2021b at-spi2-atk/2.38.0</code> GTK3 3.24.31 foss/2021b <code>module load foss/2021b GTK3/3.24.31</code> Emacs 27.2 foss/2021b <code>module load foss/2021b Emacs/27.2</code> cppy 1.1.0 foss/2021b <code>module load foss/2021b cppy/1.1.0</code> Tkinter 3.9.6 foss/2021b <code>module load foss/2021b Tkinter/3.9.6</code> Pillow 8.3.2 foss/2021b <code>module load foss/2021b Pillow/8.3.2</code> Pillow 8.2.0 foss/2021b <code>module load foss/2021b Pillow/8.2.0</code> make 4.3 foss/2021b <code>module load foss/2021b make/4.3</code> libgd 2.3.3 foss/2021b <code>module load foss/2021b libgd/2.3.3</code> libcerf 1.17 foss/2021b <code>module load foss/2021b libcerf/1.17</code> Lua 5.4.3 foss/2021b <code>module load foss/2021b Lua/5.4.3</code> gnuplot 5.4.2 foss/2021b <code>module load foss/2021b gnuplot/5.4.2</code> tbb 2020.3 foss/2021b <code>module load foss/2021b tbb/2020.3</code> protobuf-python 3.17.3 foss/2021b <code>module load foss/2021b protobuf-python/3.17.3</code> typing-extensions 3.10.0.2 foss/2021b <code>module load foss/2021b typing-extensions/3.10.0.2</code> typing-extensions 4.3.0 foss/2021b <code>module load foss/2021b typing-extensions/4.3.0</code> libyaml 0.2.5 foss/2021b <code>module load foss/2021b libyaml/0.2.5</code> PyYAML 5.4.1 foss/2021b <code>module load foss/2021b PyYAML/5.4.1</code> expecttest 0.1.3 foss/2021b <code>module load foss/2021b expecttest/0.1.3</code> NCCL 2.10.3-CUDA-11.4.1 foss/2021b <code>module load foss/2021b NCCL/2.10.3-CUDA-11.4.1</code> Pillow-SIMD 8.3.2 foss/2021b <code>module load foss/2021b Pillow-SIMD/8.3.2</code> Pillow-SIMD 8.2.0 foss/2021b <code>module load foss/2021b Pillow-SIMD/8.2.0</code> flatbuffers 2.0.0 foss/2021b <code>module load foss/2021b flatbuffers/2.0.0</code> pkgconfig 1.5.5-python foss/2021b <code>module load foss/2021b pkgconfig/1.5.5-python</code> JsonCpp 1.9.4 foss/2021b <code>module load foss/2021b JsonCpp/1.9.4</code> LMDB 0.9.29 foss/2021b <code>module load foss/2021b LMDB/0.9.29</code> nsync 1.24.0 foss/2021b <code>module load foss/2021b nsync/1.24.0</code> flatbuffers-python 2.0 foss/2021b <code>module load foss/2021b flatbuffers-python/2.0</code> tqdm 4.62.3 foss/2021b <code>module load foss/2021b tqdm/4.62.3</code> XML-LibXML 2.0207 foss/2021b <code>module load foss/2021b XML-LibXML/2.0207</code> DB_File 1.857 foss/2021b <code>module load foss/2021b DB_File/1.857</code> BioPerl 1.7.8 foss/2021b <code>module load foss/2021b BioPerl/1.7.8</code> glew 2.2.0-glx foss/2021b <code>module load foss/2021b glew/2.2.0-glx</code> GLFW 3.3.4 foss/2021b <code>module load foss/2021b GLFW/3.3.4</code> GLM 0.9.9.8 foss/2021b <code>module load foss/2021b GLM/0.9.9.8</code> OpenPGM 5.2.122 foss/2021b <code>module load foss/2021b OpenPGM/5.2.122</code> libsodium 1.0.18 foss/2021b <code>module load foss/2021b libsodium/1.0.18</code> ZeroMQ 4.3.4 foss/2021b <code>module load foss/2021b ZeroMQ/4.3.4</code> IPython 7.26.0 foss/2021b <code>module load foss/2021b IPython/7.26.0</code> JupyterLab 3.1.6 foss/2021b <code>module load foss/2021b JupyterLab/3.1.6</code> Qwt 6.2.0 foss/2021b <code>module load foss/2021b Qwt/6.2.0</code> mm-common 1.0.5 foss/2021b <code>module load foss/2021b mm-common/1.0.5</code> libxslt 1.1.34 foss/2021b <code>module load foss/2021b libxslt/1.1.34</code> GTS 0.7.6 foss/2021b <code>module load foss/2021b GTS/0.7.6</code> Graphviz 2.50.0 foss/2021b <code>module load foss/2021b Graphviz/2.50.0</code> libsigc++ 3.2.0 foss/2021b <code>module load foss/2021b libsigc++/3.2.0</code> tcsh 6.24.01 foss/2021b <code>module load foss/2021b tcsh/6.24.01</code> motif 2.3.8 foss/2021b <code>module load foss/2021b motif/2.3.8</code> PyQt5 5.15.4 foss/2021b <code>module load foss/2021b PyQt5/5.15.4</code> libwebp 1.2.0 foss/2021b <code>module load foss/2021b libwebp/1.2.0</code> archspec 0.1.3 foss/2021b <code>module load foss/2021b archspec/0.1.3</code> Voro++ 0.4.6 foss/2021b <code>module load foss/2021b Voro++/0.4.6</code> kim-api 2.3.0 foss/2021b <code>module load foss/2021b kim-api/2.3.0</code> xxd 8.2.4220 foss/2021b <code>module load foss/2021b xxd/8.2.4220</code> zlib .1.2.12 foss/2022b <code>module load foss/2022b zlib/.1.2.12</code> help2man .1.49.2 foss/2022b <code>module load foss/2022b help2man/.1.49.2</code> M4 .1.4.19 foss/2022b <code>module load foss/2022b M4/.1.4.19</code> Bison .3.8.2 foss/2022b <code>module load foss/2022b Bison/.3.8.2</code> flex .2.6.4 foss/2022b <code>module load foss/2022b flex/.2.6.4</code> binutils .2.39 foss/2022b <code>module load foss/2022b binutils/.2.39</code> groff .1.22.4 foss/2022b <code>module load foss/2022b groff/.1.22.4</code> expat .2.4.9 foss/2022b <code>module load foss/2022b expat/.2.4.9</code> pkgconf 1.9.3 foss/2022b <code>module load foss/2022b pkgconf/1.9.3</code> libevent 2.1.12 foss/2022b <code>module load foss/2022b libevent/2.1.12</code> ncurses .6.3 foss/2022b <code>module load foss/2022b ncurses/.6.3</code> libtool .2.4.7 foss/2022b <code>module load foss/2022b libtool/.2.4.7</code> libreadline .8.2 foss/2022b <code>module load foss/2022b libreadline/.8.2</code> DB 18.1.40 foss/2022b <code>module load foss/2022b DB/18.1.40</code> Perl 5.36.0 foss/2022b <code>module load foss/2022b Perl/5.36.0</code> Autoconf .2.71 foss/2022b <code>module load foss/2022b Autoconf/.2.71</code> Automake .1.16.5 foss/2022b <code>module load foss/2022b Automake/.1.16.5</code> Autotools .20220317 foss/2022b <code>module load foss/2022b Autotools/.20220317</code> numactl .2.0.16 foss/2022b <code>module load foss/2022b numactl/.2.0.16</code> UCX 1.13.1 foss/2022b <code>module load foss/2022b UCX/1.13.1</code> libfabric .1.16.1 foss/2022b <code>module load foss/2022b libfabric/.1.16.1</code> xorg-macros .1.19.3 foss/2022b <code>module load foss/2022b xorg-macros/.1.19.3</code> libpciaccess .0.17 foss/2022b <code>module load foss/2022b libpciaccess/.0.17</code> UCC 1.1.0 foss/2022b <code>module load foss/2022b UCC/1.1.0</code> XZ .5.2.7 foss/2022b <code>module load foss/2022b XZ/.5.2.7</code> libxml2 .2.10.3 foss/2022b <code>module load foss/2022b libxml2/.2.10.3</code> hwloc .2.8.0 foss/2022b <code>module load foss/2022b hwloc/.2.8.0</code> PMIx 4.2.2 foss/2022b <code>module load foss/2022b PMIx/4.2.2</code> bzip2 .1.0.8 foss/2022b <code>module load foss/2022b bzip2/.1.0.8</code> cURL .7.86.0 foss/2022b <code>module load foss/2022b cURL/.7.86.0</code> UnZip 6.0 foss/2022b <code>module load foss/2022b UnZip/6.0</code> libarchive .3.6.1 foss/2022b <code>module load foss/2022b libarchive/.3.6.1</code> CMake 3.24.3 foss/2022b <code>module load foss/2022b CMake/3.24.3</code> libffi .3.4.4 foss/2022b <code>module load foss/2022b libffi/.3.4.4</code> Tcl 8.6.12 foss/2022b <code>module load foss/2022b Tcl/8.6.12</code> SQLite 3.39.4 foss/2022b <code>module load foss/2022b SQLite/3.39.4</code> Python 3.10.8-bare foss/2022b <code>module load foss/2022b Python/3.10.8-bare</code> Python 3.10.8 foss/2022b <code>module load foss/2022b Python/3.10.8</code> Rust 1.65.0 foss/2022b <code>module load foss/2022b Rust/1.65.0</code> GMP .6.2.1 foss/2022b <code>module load foss/2022b GMP/.6.2.1</code> gettext .0.21.1 foss/2022b <code>module load foss/2022b gettext/.0.21.1</code> git 2.38.1-nodocs foss/2022b <code>module load foss/2022b git/2.38.1-nodocs</code> hypothesis 6.68.2 foss/2022b <code>module load foss/2022b hypothesis/6.68.2</code> Ninja 1.11.1 foss/2022b <code>module load foss/2022b Ninja/1.11.1</code> Meson 0.64.0 foss/2022b <code>module load foss/2022b Meson/0.64.0</code> Eigen 3.4.0 foss/2022b <code>module load foss/2022b Eigen/3.4.0</code> pybind11 2.10.3 foss/2022b <code>module load foss/2022b pybind11/2.10.3</code> OpenBLAS 0.3.9 foss/2020a <code>module load foss/2020a OpenBLAS/0.3.9</code> OpenMPI 4.0.3 foss/2020a <code>module load foss/2020a</code> GSL 2.6 foss/2020a <code>module load foss/2020a GSL/2.6</code> libxsmm 1.16.1 foss/2020b <code>module load foss/2020b libxsmm/1.16.1</code> GSL 2.6 foss/2020b <code>module load foss/2020b GSL/2.6</code> OpenBLAS 0.3.12 foss/2020b <code>module load foss/2020b OpenBLAS/0.3.12</code> Boost 1.74.0 foss/2020b <code>module load foss/2020b Boost/1.74.0</code> libxc 4.3.4 foss/2020b <code>module load foss/2020b libxc/4.3.4</code> Libint 2.6.0-lmax-6-cp2k foss/2020b <code>module load foss/2020b Libint/2.6.0-lmax-6-cp2k</code> OpenMPI 4.0.5 foss/2020b <code>module load foss/2020b</code> CUDA 11.1.1 foss/2020b <code>module load foss/2020b CUDA/11.1.1</code> GSL 2.7 foss/2021a <code>module load foss/2021a GSL/2.7</code> OpenMPI 4.1.1 foss/2021a <code>module load foss/2021a</code> GEOS 3.9.1 foss/2021a <code>module load foss/2021a GEOS/3.9.1</code> OpenBLAS 0.3.15 foss/2021a <code>module load foss/2021a OpenBLAS/0.3.15</code> FlexiBLAS 3.0.4 foss/2021a <code>module load foss/2021a FlexiBLAS/3.0.4</code> Boost 1.76.0 foss/2021a <code>module load foss/2021a Boost/1.76.0</code> OpenMPI 4.1.1 foss/2021b <code>module load foss/2021b</code> OpenBLAS 0.3.18 foss/2021b <code>module load foss/2021b OpenBLAS/0.3.18</code> OpenBLAS 0.3.20 foss/2021b <code>module load foss/2021b OpenBLAS/0.3.20</code> BLIS 0.8.1 foss/2021b <code>module load foss/2021b BLIS/0.8.1</code> FlexiBLAS 3.0.4 foss/2021b <code>module load foss/2021b FlexiBLAS/3.0.4</code> FoX 4.1.2 foss/2021b <code>module load foss/2021b FoX/4.1.2</code> GEOS 3.9.1 foss/2021b <code>module load foss/2021b GEOS/3.9.1</code> Boost 1.77.0 foss/2021b <code>module load foss/2021b Boost/1.77.0</code> poppler 22.01.0 foss/2021b <code>module load foss/2021b poppler/22.01.0</code> GSL 2.7 foss/2021b <code>module load foss/2021b GSL/2.7</code> LAPACK 3.10.1 foss/2021b <code>module load foss/2021b LAPACK/3.10.1</code> AutoDock-GPU 1.5.3-CUDA-11.4.1 foss/2021b <code>module load foss/2021b AutoDock-GPU/1.5.3-CUDA-11.4.1</code> Bowtie2 2.4.4 foss/2021b <code>module load foss/2021b Bowtie2/2.4.4</code> SAMtools 1.15.1 foss/2021b <code>module load foss/2021b SAMtools/1.15.1</code> SAMtools 0.1.17 foss/2021b <code>module load foss/2021b SAMtools/0.1.17</code> Bowtie 1.3.1 foss/2021b <code>module load foss/2021b Bowtie/1.3.1</code> libxc 5.1.6 foss/2021b <code>module load foss/2021b libxc/5.1.6</code> texlive 20220321 foss/2021b <code>module load foss/2021b texlive/20220321</code> Boost.Python 1.77.0 foss/2021b <code>module load foss/2021b Boost.Python/1.77.0</code> OpenMPI 4.1.4 foss/2022b <code>module load foss/2022b</code> FFTW 3.3.10 foss/2022b <code>module load foss/2022b FFTW/3.3.10</code> BLIS 0.9.0 foss/2022b <code>module load foss/2022b BLIS/0.9.0</code> OpenBLAS 0.3.21 foss/2022b <code>module load foss/2022b OpenBLAS/0.3.21</code> FlexiBLAS 3.2.1 foss/2022b <code>module load foss/2022b FlexiBLAS/3.2.1</code> impi 2021.4.0 intel/2021b <code>module load intel/2021b</code> xmlf90 1.5.4 intel/2021b <code>module load intel/2021b xmlf90/1.5.4</code> libpsml 1.1.10 intel/2021b <code>module load intel/2021b libpsml/1.1.10</code> libxc 5.1.6 intel/2021b <code>module load intel/2021b libxc/5.1.6</code> impi 2021.2.0 intel/2021a <code>module load intel/2021a</code> libxc 5.1.5 intel/2021a <code>module load intel/2021a libxc/5.1.5</code> libxsmm 1.16.2 intel/2021a <code>module load intel/2021a libxsmm/1.16.2</code> Boost 1.76.0 intel/2021a <code>module load intel/2021a Boost/1.76.0</code> GSL 2.7 intel/2021a <code>module load intel/2021a GSL/2.7</code> FFTW 3.3.8 foss/2020a <code>module load foss/2020a FFTW/3.3.8</code> ScaLAPACK 2.1.0 foss/2020a <code>module load foss/2020a ScaLAPACK/2.1.0</code> LAMMPS 3Mar2020-Python-3.8.2-kokkos foss/2020a <code>module load foss/2020a LAMMPS/3Mar2020-Python-3.8.2-kokkos</code> HDF5 1.10.6 foss/2020a <code>module load foss/2020a HDF5/1.10.6</code> netCDF .4.7.4 foss/2020a <code>module load foss/2020a netCDF/.4.7.4</code> kim-api 2.1.3 foss/2020a <code>module load foss/2020a kim-api/2.1.3</code> ScaFaCoS 1.0.1 foss/2020a <code>module load foss/2020a ScaFaCoS/1.0.1</code> Boost 1.72.0 foss/2020a <code>module load foss/2020a Boost/1.72.0</code> SciPy-bundle 2020.03-Python-3.8.2 foss/2020a <code>module load foss/2020a SciPy-bundle/2020.03-Python-3.8.2</code> h5py 2.10.0-Python-3.8.2 foss/2020a <code>module load foss/2020a h5py/2.10.0-Python-3.8.2</code> matplotlib 3.2.1-Python-3.8.2 foss/2020a <code>module load foss/2020a matplotlib/3.2.1-Python-3.8.2</code> PLUMED 2.6.0-Python-3.8.2 foss/2020a <code>module load foss/2020a PLUMED/2.6.0-Python-3.8.2</code> molmod 1.4.5-Python-3.8.2 foss/2020a <code>module load foss/2020a molmod/1.4.5-Python-3.8.2</code> yaff 1.6.0-Python-3.8.2 foss/2020a <code>module load foss/2020a yaff/1.6.0-Python-3.8.2</code> VTK 8.2.0-Python-3.8.2 foss/2020a <code>module load foss/2020a VTK/8.2.0-Python-3.8.2</code> imkl 2020.1.217 foss/2020a <code>module load foss/2020a imkl/2020.1.217</code> scikit-build 0.10.0-Python-3.8.2 foss/2020a <code>module load foss/2020a scikit-build/0.10.0-Python-3.8.2</code> networkx 2.4-Python-3.8.2 foss/2020a <code>module load foss/2020a networkx/2.4-Python-3.8.2</code> GROMACS 2020.4-Python-3.8.2 foss/2020a <code>module load foss/2020a GROMACS/2020.4-Python-3.8.2</code> fluent 19.4.0 foss/2020a <code>module load foss/2020a fluent/19.4.0</code> fluent 20.1.0 foss/2020a <code>module load foss/2020a fluent/20.1.0</code> SCOTCH 6.0.9 foss/2020a <code>module load foss/2020a SCOTCH/6.0.9</code> CGAL 4.14.3-Python-3.8.2 foss/2020a <code>module load foss/2020a CGAL/4.14.3-Python-3.8.2</code> ParaView 5.8.0-Python-3.8.2-mpi foss/2020a <code>module load foss/2020a ParaView/5.8.0-Python-3.8.2-mpi</code> OpenFOAM 8 foss/2020a <code>module load foss/2020a OpenFOAM/8</code> Arrow 0.17.1-Python-3.8.2 foss/2020a <code>module load foss/2020a Arrow/0.17.1-Python-3.8.2</code> Spark 3.1.2-Python-3.8.2 foss/2020a <code>module load foss/2020a Spark/3.1.2-Python-3.8.2</code> GULP 6.0 foss/2020a <code>module load foss/2020a GULP/6.0</code> netCDF-Fortran 4.5.2 foss/2020a <code>module load foss/2020a netCDF-Fortran/4.5.2</code> ELPA 2019.11.001 foss/2020a <code>module load foss/2020a ELPA/2019.11.001</code> Siesta 4.1.5 foss/2020a <code>module load foss/2020a Siesta/4.1.5</code> ASE 3.21.1-Python-3.8.2 foss/2020a <code>module load foss/2020a ASE/3.21.1-Python-3.8.2</code> spglib-python 1.16.0-Python-3.8.2 foss/2020a <code>module load foss/2020a spglib-python/1.16.0-Python-3.8.2</code> ScaLAPACK 2.1.0 foss/2020b <code>module load foss/2020b ScaLAPACK/2.1.0</code> FFTW 3.3.8 foss/2020b <code>module load foss/2020b FFTW/3.3.8</code> SciPy-bundle 2020.11 foss/2020b <code>module load foss/2020b SciPy-bundle/2020.11</code> PLUMED 2.6.2 foss/2020b <code>module load foss/2020b PLUMED/2.6.2</code> CP2K 7.1 foss/2020b <code>module load foss/2020b CP2K/7.1</code> HDF5 1.10.7 foss/2021a <code>module load foss/2021a HDF5/1.10.7</code> FFTW 3.3.9 foss/2021a <code>module load foss/2021a FFTW/3.3.9</code> netCDF .4.8.0 foss/2021a <code>module load foss/2021a netCDF/.4.8.0</code> ScaLAPACK 2.1.0-fb foss/2021a <code>module load foss/2021a ScaLAPACK/2.1.0-fb</code> SciPy-bundle 2021.05 foss/2021a <code>module load foss/2021a SciPy-bundle/2021.05</code> GDAL .3.3.0 foss/2021a <code>module load foss/2021a GDAL/.3.3.0</code> R 4.1.0 foss/2021a <code>module load foss/2021a R/4.1.0</code> VTK 9.0.1 foss/2021a <code>module load foss/2021a VTK/9.0.1</code> h5py 3.2.1 foss/2021a <code>module load foss/2021a h5py/3.2.1</code> FFTW 3.3.10 foss/2021b <code>module load foss/2021b FFTW/3.3.10</code> ScaLAPACK 2.1.0-fb foss/2021b <code>module load foss/2021b ScaLAPACK/2.1.0-fb</code> HDF5 1.10.8 foss/2021b <code>module load foss/2021b HDF5/1.10.8</code> HDF5 1.12.1 foss/2021b <code>module load foss/2021b HDF5/1.12.1</code> ORCA 5.0.3 foss/2021b <code>module load foss/2021b ORCA/5.0.3</code> Boost 1.77.0 foss/2021b <code>module load foss/2021b Boost/1.77.0</code> Boost 1.79.0 foss/2021b <code>module load foss/2021b Boost/1.79.0</code> netCDF .4.8.1 foss/2021b <code>module load foss/2021b netCDF/.4.8.1</code> netCDF 4.8.1 foss/2021b <code>module load foss/2021b netCDF/4.8.1</code> SciPy-bundle 2021.10 foss/2021b <code>module load foss/2021b SciPy-bundle/2021.10</code> GDAL .3.3.2 foss/2021b <code>module load foss/2021b GDAL/.3.3.2</code> GDAL 3.3.2 foss/2021b <code>module load foss/2021b GDAL/3.3.2</code> SUMO 1.12.0 foss/2021b <code>module load foss/2021b SUMO/1.12.0</code> OpenSceneGraph 3.6.5 foss/2021b <code>module load foss/2021b OpenSceneGraph/3.6.5</code> networkx 2.6.3 foss/2021b <code>module load foss/2021b networkx/2.6.3</code> GROMACS 2021.5-CUDA-11.4.1 foss/2021b <code>module load foss/2021b GROMACS/2021.5-CUDA-11.4.1</code> GROMACS 2021.5 foss/2021b <code>module load foss/2021b GROMACS/2021.5</code> R 4.2.0 foss/2021b <code>module load foss/2021b R/4.2.0</code> matplotlib 3.4.3 foss/2021b <code>module load foss/2021b matplotlib/3.4.3</code> netCDF-Fortran 4.5.3 foss/2021b <code>module load foss/2021b netCDF-Fortran/4.5.3</code> PnetCDF 1.12.3 foss/2021b <code>module load foss/2021b PnetCDF/1.12.3</code> SCOTCH 6.1.2 foss/2021b <code>module load foss/2021b SCOTCH/6.1.2</code> CGAL 4.14.3 foss/2021b <code>module load foss/2021b CGAL/4.14.3</code> ParaView 5.9.1-mpi foss/2021b <code>module load foss/2021b ParaView/5.9.1-mpi</code> OpenFOAM v2112 foss/2021b <code>module load foss/2021b OpenFOAM/v2112</code> VTK 9.1.0 foss/2021b <code>module load foss/2021b VTK/9.1.0</code> VTK 8.2.0 foss/2021b <code>module load foss/2021b VTK/8.2.0</code> OpenCASCADE 7.6.0 foss/2021b <code>module load foss/2021b OpenCASCADE/7.6.0</code> OpenCASCADE 7.4.0 foss/2021b <code>module load foss/2021b OpenCASCADE/7.4.0</code> blaze 3.8.1 foss/2021b <code>module load foss/2021b blaze/3.8.1</code> irrlicht 1.8.5 foss/2021b <code>module load foss/2021b irrlicht/1.8.5</code> Chrono 7.0.3 foss/2021b <code>module load foss/2021b Chrono/7.0.3</code> magma 2.6.2-CUDA-11.4.1 foss/2021b <code>module load foss/2021b magma/2.6.2-CUDA-11.4.1</code> PyTorch 1.11.0-CUDA-11.4.1 foss/2021b <code>module load foss/2021b PyTorch/1.11.0-CUDA-11.4.1</code> h5py 3.6.0 foss/2021b <code>module load foss/2021b h5py/3.6.0</code> tensorboard 2.11.0 foss/2021b <code>module load foss/2021b tensorboard/2.11.0</code> torchvision 0.12.0-CUDA-11.4.1 foss/2021b <code>module load foss/2021b torchvision/0.12.0-CUDA-11.4.1</code> PyTorch-Lightning 1.7.7-CUDA-11.4.1 foss/2021b <code>module load foss/2021b PyTorch-Lightning/1.7.7-CUDA-11.4.1</code> Horovod 0.26.1-CUDA-11.4.1-PyTorch-1.11.0 foss/2021b <code>module load foss/2021b Horovod/0.26.1-CUDA-11.4.1-PyTorch-1.11.0</code> Bio-SamTools 1.43-Perl-5.34.0 foss/2021b <code>module load foss/2021b Bio-SamTools/1.43-Perl-5.34.0</code> Trinity 2.14.0 foss/2021b <code>module load foss/2021b Trinity/2.14.0</code> bitarray 2.6.0-Python-3.9.6 foss/2021b <code>module load foss/2021b bitarray/2.6.0-Python-3.9.6</code> ELPA 2021.05.001 foss/2021b <code>module load foss/2021b ELPA/2021.05.001</code> QuantumESPRESSO 7.1 foss/2021b <code>module load foss/2021b QuantumESPRESSO/7.1</code> AFNI 23.0.04-Python-3.9.6 foss/2021b <code>module load foss/2021b AFNI/23.0.04-Python-3.9.6</code> Biopython 1.79 foss/2021b <code>module load foss/2021b Biopython/1.79</code> MDAnalysis 2.0.0 foss/2021b <code>module load foss/2021b MDAnalysis/2.0.0</code> perm-md-count main foss/2021b <code>module load foss/2021b perm-md-count/main</code> RIP-MD master foss/2021b <code>module load foss/2021b RIP-MD/master</code> scikit-learn 1.0.1 foss/2021b <code>module load foss/2021b scikit-learn/1.0.1</code> OpenCV 4.5.5-contrib foss/2021b <code>module load foss/2021b OpenCV/4.5.5-contrib</code> psutil 5.9.3-Python-3.9.6 foss/2021b <code>module load foss/2021b psutil/5.9.3-Python-3.9.6</code> mmcv-full 1.7.1-CUDA-11.4.1 foss/2021b <code>module load foss/2021b mmcv-full/1.7.1-CUDA-11.4.1</code> Seaborn 0.11.2 foss/2021b <code>module load foss/2021b Seaborn/0.11.2</code> OVITO 3.7.11-basic foss/2021b <code>module load foss/2021b OVITO/3.7.11-basic</code> Boost.MPI 1.77.0 foss/2021b <code>module load foss/2021b Boost.MPI/1.77.0</code> pyh5md 1.0.0 foss/2021b <code>module load foss/2021b pyh5md/1.0.0</code> espresso 3.0.0 foss/2021b <code>module load foss/2021b espresso/3.0.0</code> ScaFaCoS 1.0.1 foss/2021b <code>module load foss/2021b ScaFaCoS/1.0.1</code> PLUMED 2.7.3 foss/2021b <code>module load foss/2021b PLUMED/2.7.3</code> LAMMPS 23Jun2022-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos-CUDA-11.4.1</code> FFTW.MPI 3.3.10 foss/2022b <code>module load foss/2022b FFTW.MPI/3.3.10</code> ScaLAPACK 2.2.0-fb foss/2022b <code>module load foss/2022b ScaLAPACK/2.2.0-fb</code> imkl-FFTW 2021.4.0 intel/2021b <code>module load intel/2021b imkl-FFTW/2021.4.0</code> HPL 2.3 intel/2021b <code>module load intel/2021b HPL/2.3</code> ELPA 2021.11.001 intel/2021b <code>module load intel/2021b ELPA/2021.11.001</code> libGridXC 0.9.6 intel/2021b <code>module load intel/2021b libGridXC/0.9.6</code> HDF5 1.12.1 intel/2021b <code>module load intel/2021b HDF5/1.12.1</code> netCDF .4.8.1 intel/2021b <code>module load intel/2021b netCDF/.4.8.1</code> netCDF-Fortran 4.5.3 intel/2021b <code>module load intel/2021b netCDF-Fortran/4.5.3</code> Siesta master-4.1.5 intel/2021b <code>module load intel/2021b Siesta/master-4.1.5</code> blaze 3.8.1 intel/2021b <code>module load intel/2021b blaze/3.8.1</code> imkl 2021.2.0 intel/2021a <code>module load intel/2021a imkl/2021.2.0</code> HDF5 1.10.7 intel/2021a <code>module load intel/2021a HDF5/1.10.7</code> netCDF .4.8.0 intel/2021a <code>module load intel/2021a netCDF/.4.8.0</code> netCDF-Fortran 4.5.3 intel/2021a <code>module load intel/2021a netCDF-Fortran/4.5.3</code> Yambo 5.0.4 intel/2021a <code>module load intel/2021a Yambo/5.0.4</code> ELPA 2021.05.001 intel/2021a <code>module load intel/2021a ELPA/2021.05.001</code> QuantumESPRESSO 6.8 intel/2021a <code>module load intel/2021a QuantumESPRESSO/6.8</code> FFTW 3.3.9 intel/2021a <code>module load intel/2021a FFTW/3.3.9</code> Libint 2.6.0-lmax-6-cp2k intel/2021a <code>module load intel/2021a Libint/2.6.0-lmax-6-cp2k</code> SciPy-bundle 2021.05 intel/2021a <code>module load intel/2021a SciPy-bundle/2021.05</code> PLUMED 2.7.2 intel/2021a <code>module load intel/2021a PLUMED/2.7.2</code> CP2K 8.2 intel/2021a <code>module load intel/2021a CP2K/8.2</code> </p>"},{"location":"Software/CFD/ansys/","title":"ANSYS","text":"<p>ANSYS is a computer-aided engineering (CAE) software suite used to simulate, analyze, and design products and systems in various industries such as aerospace, automotive, energy, and healthcare. ANSYS provides a wide range of simulation tools that can be used to model and analyze various physical phenomena such as fluid dynamics, structural mechanics, electromagnetics, and thermal analysis.</p> <p>The software suite is known for its high level of accuracy and versatility, and is widely used by engineers and designers to optimize product performance, reduce costs, and improve time to market. ANSYS offers a wide range of modules and add-ons, which can be customized to suit specific engineering needs and requirements.</p>"},{"location":"Software/CFD/ansys/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command ANSYS 2022 - <code>module load ANSYS/2022</code> ANSYS 2023R1 foss/2021b <code>module load foss/2021b ANSYS/2023R1</code> </p> <p> Software Version Dependent Toolchain Module Load Command ANSYS 2022 - <code>module load ANSYS/2022</code> </p>"},{"location":"Software/CFD/ansys/#application-information-documentation","title":"Application Information, Documentation","text":"<p>Please download ANSYS and follow the instructions to install ANSYS on your local machine.</p>"},{"location":"Software/CFD/ansys/#using-ansys","title":"Using ANSYS","text":"<p>ANSYS workbench will be available on Wulver via Open OnDemand. We will provide the instructions soon.</p>"},{"location":"Software/CFD/ansys/#related-applications","title":"Related Applications","text":"<ul> <li>COMSOL</li> </ul>"},{"location":"Software/CFD/ansys/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/comsol/","title":"COMSOL","text":"<p>COMSOL is a commercial finite element analysis (FEA) software package used for modeling and simulation of multi-physics systems. It allows users to build and solve complex multiphysics models involving various physical phenomena such as fluid dynamics, structural mechanics, heat transfer, electromagnetics, and chemical reactions.</p> <p>The software uses a graphical user interface to create models, and offers a wide range of pre-built modeling components that can be used to quickly create models. It also supports user-defined models and can handle a variety of boundary conditions and physics.</p> <p>COMSOL is widely used in engineering and science fields, such as mechanical engineering, chemical engineering, electrical engineering, physics, and materials science, among others. Its multiphysics capabilities make it a powerful tool for simulating complex systems and optimizing designs.</p>"},{"location":"Software/CFD/comsol/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command COMSOL 5.6 - <code>module load COMSOL/5.6</code> </p> <p> Software Version Dependent Toolchain Module Load Command </p>"},{"location":"Software/CFD/comsol/#application-information-documentation","title":"Application Information, Documentation","text":""},{"location":"Software/CFD/comsol/#using-comsol","title":"Using COMSOL","text":""},{"location":"Software/CFD/comsol/#related-applications","title":"Related Applications","text":"<ul> <li>ANSYS </li> </ul>"},{"location":"Software/CFD/comsol/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/fluent/","title":"FLUENT","text":""},{"location":"Software/CFD/fluent/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command ANSYS 2022 - <code>module load ANSYS/2022</code> ANSYS 2023R1 foss/2021b <code>module load foss/2021b ANSYS/2023R1</code> </p> <p> Software Version Dependent Toolchain Module Load Command ANSYS 2022 - <code>module load ANSYS/2022</code> fluent 19.4.0 foss/2020a <code>module load foss/2020a fluent/19.4.0</code> fluent 20.1.0 foss/2020a <code>module load foss/2020a fluent/20.1.0</code> </p>"},{"location":"Software/CFD/fluent/#application-information-documentation","title":"Application Information, Documentation","text":"<p>To use Fluent on cluster, Users need to prepare the case using ANSYS on their local machine first. Please download ANSYS and follow the instructions to install ANSYS on your local machine.</p>"},{"location":"Software/CFD/fluent/#using-fluent","title":"Using Fluent","text":"<p>To use Fluent in cluster, users first need to prepare the fluent case (meshing, setting boundary conditions) on their local machine  and save the case and data in <code>.cas</code> and <code>.dat</code> format respectively.  If you are running transient problem and want to save the data at particular timestep or time interval, please see the steps below.</p> <ul> <li>Go to <code>Calculation Activities</code> option in the left pane and double-click the <code>Autosave (Every  Flow Time)</code> option, you will notice a separate dialogue box <code>Autosave</code>, where you need to specify how frequently you want to save the data, you can choose eiter <code>timestep</code> or <code>Flow Time</code> interval. In the <code>File name</code> option you need to specify the subdirectory where you want to save the data and the case name. In the example shown below the subdirectory is <code>data</code> and the problem name is heatpipe. You need to make sure to create the subdirectoy (<code>data</code> in this example) in the cluster where you want to intend to submit the job script.</li> </ul> <p></p> <p>This is required if your job is somehow cancelled, or you need to restart from specific flow time.</p> <ul> <li>If you want  to ppstprocess the data using a different software , e.g. Tecplot or ParaView, you need to save the data in different file format at certain flow time or time step interval.  To set up the postprocessing configuration, select <code>File --&gt; Export --&gt; During Calculation --&gt; Solution Data</code> option (see the figure below)</li> </ul> <p></p> <ul> <li>Once you open the configuration, you will notice separate dialogue box <code>Automatic Export</code>. You need the file format in <code>File Type</code> option. Select the drop-down option to see different options. In this example above, <code>ensight</code> format has been selected. In right pane, you need to select which parameters you want to visualize. In the example above, <code>Volume Fraction</code> for different phases have been selected. </li> <li>Next, you to select the file name and the subdirectory on cluster where you intend to write the post processed data. In this example, we choose to write the files in <code>ensi</code> subdirectory. Please make sure to create this subdirectory inside case directory on cluster before running the simulation. </li> <li>You can also set how often you want to write your data. You need to select <code>Export Data Everty (s)</code> option in terms of time step interval or flow time interval.  </li> <li>Once you set up your case and initialize the problem, you need to go to <code>File --&gt; write --&gt; case and data</code> option to write the case and data in <code>.cas</code> and <code>.dat</code> respectively. </li> <li>Transfer the <code>.cas</code> and <code>.dat</code> files to the cluster. See the steps to transfer the data for transferring the files from local machine to cluster.</li> <li>Once you transfer the files, log on the cluster and go the directory where you transferred the <code>.cas</code> and <code>.dat</code> files. </li> <li>Create a journal file which defines the input data file and some additional settings required by Fluent.  You can use the following journal file</li> </ul> Sample journal file : journal.JOU <pre><code>   /file/set-tui-version \"21.1\"\n   /file/read-case-data tube_vof.cas.h5\n   solve/dual-time-iterate 20 50\n   (print-case-timer)\n   parallel/timer/usage\n</code></pre> <p>In the above <code>Journal</code> script, the full name of case file (<code>tube_vof.cas.h5</code>) is mentioned. You need to modify based on the case file based on the problem.  The <code>solve/dual-time-iterate</code> specifies the end flow time and number of iterations. In the above example, <code>20</code> is the end flow time while the maximum number of iterations are <code>50</code>. The \"dual-time\" approach allows for a larger time step size by introducing an additional iteration loop within each time step. Users can select different approach based on their problems and need to modify it accordingly.  For more details on journal commands, see the Fluent text user interface (TUI) commands from Fluent documentation.</p> Sample Batch Script to Run FLUENT : fluent.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n#SBATCH --job-name=fluent\n#SBATCH --output=%x.%j.out # i%x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n# Use \"sinfo\" to see what partitions are available to you\n#SBATCH --partition=general\n#SBATCH --ntasks=8\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n# Memory required; lower amount gets scheduling priority\n#SBATCH --mem-per-cpu=2G\n\n# Time required in d-hh:mm:ss format; lower time gets scheduling priority\n#SBATCH --time=71:59:00\n\n# Purge and load the correct modules\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load the slurm, easybuild \nmodule load ANSYS\n\n# Run the mpi program\n\nmachines=hosts.$SLURM_JOB_ID\ntouch $machines\nfor node in `scontrol show hostnames`\n    do\n        echo \"$node\"  &gt;&gt; $machines\n    done\n\nfluent 3ddp -affinity=off -ssh -t$SLURM_NTASKS -pib -mpi=intel -cnf=\"$machines\" -g -i journal.JOU\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=fluent\n#SBATCH --output=%x.%j.out # i%x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --ntasks=8\n# Use \"sinfo\" to see what partitions are available to you\n#SBATCH --partition=public\n\n# Memory required; lower amount gets scheduling priority\n#SBATCH --mem-per-cpu=5G\n\n# Time required in d-hh:mm:ss format; lower time gets scheduling priority\n#SBATCH --time=5-24:59:00\n\n# Purge and load the correct modules\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load ANSYS\n\n# Run the mpi program\n\nmachines=hosts.$SLURM_JOB_ID\ntouch $machines\nfor node in `scontrol show hostnames`\n    do\n        echo \"$node\"  &gt;&gt; $machines\n    done\n\nfluent 3ddp -affinity=off -ssh -t$SLURM_NTASKS -pib -mpi=intel -cnf=\"$machines\" -g -i journal.JOU\n</code></pre> <p>Submit the job using <code>sbatch fluent.submit.sh</code> command.</p>"},{"location":"Software/CFD/fluent/#related-applications","title":"Related Applications","text":"<ul> <li>OpenFOAM</li> </ul>"},{"location":"Software/CFD/fluent/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/openfoam/","title":"OpenFOAM","text":"<p>OpenFOAM (Open Field Operation and Manipulation) is a free and open-source computational fluid dynamics (CFD) software package that is used to simulate and analyze fluid flow, heat transfer, and other related phenomena. It is written primarily in C++ and can be used on various operating systems such as Linux, Windows, and macOS.</p> <p>OpenFOAM offers a wide range of solvers, turbulence models, and physical models that can be used to model various engineering applications such as automotive, aerospace, chemical processing, and power generation. It also allows users to customize and extend its functionality to meet specific needs.</p> <p>The software is widely used in academia, research, and industry, and is known for its robustness, flexibility, and scalability. It offers a high degree of parallelization, which allows it to be used on large clusters of computers for complex simulations. Because it is open source, it is often used by researchers and developers to create new solvers, models, and add-ons for specific applications.</p>"},{"location":"Software/CFD/openfoam/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command OpenFOAM v2112 foss/2021b <code>module load foss/2021b OpenFOAM/v2112</code> </p> <p> Software Version Dependent Toolchain Module Load Command OpenFOAM 8 foss/2020a <code>module load foss/2020a OpenFOAM/8</code> OpenFOAM v2112 foss/2021b <code>module load foss/2021b OpenFOAM/v2112</code> </p>"},{"location":"Software/CFD/openfoam/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of OpenFOAM is available at OpenFOAM Documentation, where you can find the tutorials in OpenFOAM meshing (blockMesh), postprocessing, setting boundary conditions etc. </p>"},{"location":"Software/CFD/openfoam/#using-openfoam","title":"Using OpenFOAM","text":"<p>OpenFOAM can be used for both serial and parallel jobs. To run OpenFOAM in parallel, you need to use the following job script.</p> Sample Batch Script to Run OpenFOAM in parallel: openfoam_parallel.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n#SBATCH --job-name=openfoam_parallel\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2021b OpenFOAM\n################################################\n#\n# Source OpenFOAM bashrc\n# The modulefile doesn't do this\n#\n################################################\nsource $FOAM_BASH\n################################################\n#\n# copy into cavity directory from /opt/site/examples/openFoam/parallel \n# run blockMesh and\n# icoFoam. Note: this is running on one node and\n# using all 32 cores on the node\n#\n################################################\ncp -r /apps/easybuild/examples/openFoam/parallel/cavity /path/to/destination\n# /path/to/destination is destination path where user wants to copy the cavity directory\ncd cavity\nblockMesh\ndecomposePar -force\nsrun icoFoam -parallel\nreconstructPar\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=openfoam_parallel\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --partition=public\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --mem-per-cpu=10G # Adjust as necessary\n#SBATCH --time=00:01:00  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load foss/2021b OpenFOAM\n################################################\n#\n# Source OpenFOAM bashrc\n# The modulefile doesn't do this\n#\n################################################\nsource $FOAM_BASH\n################################################\n#\n# cd into cavity directory and run blockMesh and\n# icoFoam. Note: this is running on one node and\n# using all 32 cores on the node\n#\n################################################\ncd cavity\nblockMesh\ndecomposePar -force\nsrun icoFoam -parallel\nreconstructPar\n</code></pre> <p>Note</p> WulverLochness <p>You can copy the tutorial <code>cavity</code> mentioned in the above job script from the <code>/apps/easybuild/examples/openFoam/parallel</code> directory.  </p> <p>You can copy the tutorial <code>cavity</code> mentioned in the above job script from the <code>/opt/site/examples/openFoam/parallel</code> directory.</p> <p>To run OpenFOAM in serial, the following job script can be used.</p> Sample Batch Script to Run OpenFOAM in serial: openfoam_serial.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n#SBATCH --job-name=openfoam_serial\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2021b OpenFOAM\n################################################\n#\n# Source OpenFOAM bashrc\n# The modulefile doesn't do this\n#\n################################################\nsource $FOAM_BASH\n################################################\n#\n# copy into cavity directory from /apps/easybuild/examples/openFoam/serial \n# run blockMesh and\n# icoFoam. Note: this is running on one node and\n# using all 32 cores on the node\n#\n################################################\ncp -r /apps/easybuild/examples/openFoam/serial/cavity /path/to/destination\n# /path/to/destination is destination path where user wants to copy the cavity directory\ncd cavity\nblockMesh\nicoFoam\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=openfoam_serial\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --partition=public\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --mem-per-cpu=10G # Adjust as necessary\n#SBATCH --time=00:01:00  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load foss/2021b OpenFOAM\n################################################\n#\n# Source OpenFOAM bashrc\n# The modulefile doesn't do this\n#\n################################################\nsource $FOAM_BASH\n################################################\n#\n# copy into cavity directory from /opt/site/examples/openFoam/parallel and run blockMesh and\n# icoFoam. Note: this is running on one node and\n# using all 32 cores on the node\n#\n################################################\ncp -r /opt/site/examples/openFoam/parallel/cavity /path/to/destination\n# /path/to/destination is destination path where user wants to copy the cavity directory\ncd cavity\nblockMesh\nicoFoam\n</code></pre> <p>Submit the job script using the sbatch command: <code>sbatch openfoam_parallel.submit.sh</code> or <code>sbatch openfoam_serial.submit.sh</code>.</p>"},{"location":"Software/CFD/openfoam/#building-openfoam-from-source","title":"Building OpenFOAM from source","text":"<p>Sometimes, users need to create a new solver or modify the existing solver by adding different functions for their research. In that case, users need to build openFOAM from source since user do not have the permission to add libraries in the root directory where OpenFOAM is installed. The following instructions are provided on how to build openFOAM from source on cluster. If you have any queries or issues regarding building OpenFOAM, please contact us at hpc@njit.edu.</p> <pre><code>  # This is to build a completly self contained OpenFOAM using MPICH mpi. Everything from GCC on up will be built.\n\n  # purge all loaded modules\n  module purge\n  # Download the latest version of OpenFOAM, visit https://develop.openfoam.com/Development/openfoam/-/blob/master/doc/Build.md for details\n  # Download the source\n  wget https://dl.openfoam.com/source/v2212/OpenFOAM-v2212.tgz \n  # Download ThirdParty\n  wget https://dl.openfoam.com/source/v2212/ThirdParty-v2212.tgz\n\n  cd ThirdParty-v2212\n\n  #Packages to download :\n  wget https://ftp.gnu.org/gnu/gcc/gcc-4.8.5/gcc-4.8.5.tar.bz2\n  wget https://src.fedoraproject.org/repo/pkgs/metis/metis-5.1.0.tar.gz/5465e67079419a69e0116de24fce58fe/metis-5.1.0.tar.gz\n  wget ftp://ftp.gnu.org/gnu/gmp/gmp-6.2.0.tar.bz2\n  wget ftp://ftp.gnu.org/gnu/mpfr/mpfr-4.0.2.tar.bz2\n  wget ftp://ftp.gnu.org/gnu/mpc/mpc-1.1.0.tar.gz\n  wget http://www.mpich.org/static/downloads/3.3/mpich-3.3.tar.gz\n\n  # unpack the above packages\n\n  vi ../OpenFOAM-v2212/etc/bashrc\n  # Change the following in bashrc \n  # User needs to specify the full path of the project directory below, it can be either the research directory or $HOME directory.\n    projectDir=\"path/to/OpenFOAM/2212/OpenFOAM-$WM_PROJECT_VERSION\" \n    export WM_MPLIB=MPICH\n    export WM_LABEL_SIZE=64\n\n  vi ../OpenFOAM-v2212/etc/config.sh/compiler\n  # Change the following in compiler\n    default_gmp_version=gmp-system\n    default_mpfr_version=mpfr-system\n    default_mpc_version=mpc-system\n\n    gmp_version=\"gmp-6.2.0\"\n    mpfr_version=\"mpfr-4.0.2\"\n    mpc_version=\"mpc-1.1.0\"\n  # Source the RC script\n  source ../OpenFOAM-v2212/etc/bashrc FOAMY_HEX_MESH=yes\n  # You might see the following warning message\n  ===============================================================================\n  Warning in /opt/site/apps/OpenFOAM/2212/OpenFOAM-v2212/etc/config.sh/settings:\n  Cannot find 'Gcc' compiler installation\n    /opt/site/apps/OpenFOAM/2212/ThirdParty-v2212/platforms/linux64/gcc-4.8.5\n\n  Either install this compiler version, or use the system compiler by setting\n  WM_COMPILER_TYPE to 'system' in $WM_PROJECT_DIR/etc/bashrc.\n  ===============================================================================\n  No completions for /opt/site/apps/OpenFOAM/2212/OpenFOAM-v2212/platforms/linux64GccDPInt64Opt/bin\n  [ignore if OpenFOAM is not yet compiled]\n\n  ./makeGcc\n  wmRefresh\n  # make MPICH\n  ./makeMPICH\n  wmRefresh\n\n  # We should be able to make the rest of the utilities.\n  # load the cmake module\n  module load cmake\n\n  /Allwmake -j 8\n\n  cd ../OpenFOAM-v2212\n  wmRefresh\n\n  ./Allwmake -j 16\n</code></pre>"},{"location":"Software/CFD/openfoam/#related-applications","title":"Related Applications","text":"<ul> <li>FLUENT</li> </ul>"},{"location":"Software/CFD/openfoam/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/IDE/VSCode/","title":"VS Code","text":"<p>Visual Studio Code (often abbreviated as VS Code) is an Integrated Development Environment (IDE). It is a lightweight and highly extensible code editor developed by Microsoft. Although referred to as a code editor, VS Code offers many features. One of the important feature of VS Code is an integrated terminal that allows developers to execute commands, run scripts, and interact with the command-line interface without leaving the editor. VS Code also provides Git integration, enabling developers to manage version control operations, such as committing, branching, and merging, without switching to a separate Git client. </p>"},{"location":"Software/IDE/VSCode/#availability","title":"Availability","text":"<p>VS Code is not installed on the cluster. To use VS Code, you need to install it on your computer and connect remotely to the cluster using NJIT VPN.</p>"},{"location":"Software/IDE/VSCode/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of VS Code is available at VS Code documentation. You can download the VS Code from VS Code download page</p>"},{"location":"Software/IDE/VSCode/#using-vs-code-on-cluster","title":"Using VS Code on Cluster","text":"<p>Warning</p> <p>If you want to use VS Code on NJIT cluster, don't use VS Code installed on your machine to connect to cluster! Please use the method described belelow so that you can use VS Code not only to edit scripts, but also run your script on the cluster.</p> <p>Use the following slurm script and submit the job script using <code>sbatch vs-code.submit.sh</code> command.</p> Batch Script to use VS Code : vs-code.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n#SBATCH --job-name=vs-code\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\nset -e\n\nmodule purge\nmodule load wulver # load slurn, easybuild\n\n# add any required module loads here, e.g. a specific Python\n\nCLI_PATH=\"${HOME}/vscode_cli\"\n\n# Install the VS Code CLI command if it doesn't exist\nif [[ ! -e ${CLI_PATH}/code ]]; then\n    echo \"Downloading and installing the VS Code CLI command\"\n    mkdir -p \"${HOME}/vscode_cli\"\n    pushd \"${HOME}/vscode_cli\"\n    # Process from: https://code.visualstudio.com/docs/remote/tunnels#_using-the-code-cli\n    curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&amp;os=cli-alpine-x64' --output vscode_cli.tar.gz\n    # unpack the code binary file\n    tar -xf vscode_cli.tar.gz\n    # clean-up\n    rm vscode_cli.tar.gz\n    popd\nfi\n\n# run the code tunnel command and accept the licence\n${CLI_PATH}/code tunnel --accept-server-license-terms\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=vs-code\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --partition=datasci\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=24:59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4G\n\nset -e\n\nmodule purge\n\n# add any required module loads here, e.g. a specific Python\n\nCLI_PATH=\"${HOME}/vscode_cli\"\n\n# Install the VS Code CLI command if it doesn't exist\nif [[ ! -e ${CLI_PATH}/code ]]; then\n    echo \"Downloading and installing the VS Code CLI command\"\n    mkdir -p \"${HOME}/vscode_cli\"\n    pushd \"${HOME}/vscode_cli\"\n    # Process from: https://code.visualstudio.com/docs/remote/tunnels#_using-the-code-cli\n    curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&amp;os=cli-alpine-x64' --output vscode_cli.tar.gz\n    # unpack the code binary file\n    tar -xf vscode_cli.tar.gz\n    # clean-up\n    rm vscode_cli.tar.gz\n    popd\nfi\n\n# run the code tunnel command and accept the licence\n${CLI_PATH}/code tunnel --accept-server-license-terms\n</code></pre> <p>Once you submit the job, you will see an output file with <code>.out</code> extension. Once you open the file, you will see the following <pre><code>*\n\n* Visual Studio Code Server\n\n*\n\n* By using the software, you agree to\n\n* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license)and\n\n* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).\n\n*\n\nTo grant access to the server, please log into https://github.com/login/device and use code XXXX-XXXX\n</code></pre> You need to have the GitHub account, please open the GitHub profile and use the code printed in the output file. Once you authorize GitHub, you will see the following in the output file</p> <pre><code>Open this link in your browser https://vscode.dev/tunnel/nodeXXX\n</code></pre> <p>Now copy and paste this link in your browser and VS Code is ready to use.</p>"},{"location":"Software/IDE/VSCode/#related-applications","title":"Related Applications","text":"<ul> <li>PyCharm</li> </ul>"},{"location":"Software/IDE/VSCode/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/cp2k/","title":"CP2K","text":"<p>CP2K is a free and open-source quantum chemistry and solid-state physics software package that is designed to perform atomistic simulations of a wide range of materials. It is capable of carrying out a variety of quantum mechanical calculations, including density functional theory (DFT), time-dependent DFT, and many-body perturbation theory (MBPT).</p> <p>CP2K is optimized for running on massively parallel supercomputers, making it well-suited for large-scale simulations of complex systems. It includes a number of advanced algorithms and techniques for efficiently calculating electronic properties of materials, such as linear scaling algorithms for DFT and MBPT calculations.</p> <p>CP2K is highly modular and flexible, allowing users to customize and extend its functionality by writing their own input files or by modifying the source code. It also includes a graphical user interface (GUI) for setting up and running simulations, as well as a number of built-in tools for analyzing simulation output.</p> <p>CP2K is widely used in the fields of materials science, chemistry, and physics for studying a wide range of materials, including biomolecules, polymers, and solids.</p>"},{"location":"Software/chemistry/cp2k/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command CP2K 8.2 intel/2021b <code>module load intel/2021b CP2K/8.2</code> CP2K 2023.1 foss/2022b <code>module load foss/2022b CP2K/2023.1</code> </p> <p> Software Version Dependent Toolchain Module Load Command CP2K 7.1 foss/2020b <code>module load foss/2020b CP2K/7.1</code> CP2K 8.2 intel/2021a <code>module load intel/2021a CP2K/8.2</code> </p>"},{"location":"Software/chemistry/cp2k/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of CP2K is available at CP2K Documentation. For any issues CP2K simulation, users can contact at CP2K Forum. </p>"},{"location":"Software/chemistry/cp2k/#using-cp2k","title":"Using CP2K","text":"<p>CP2K MPI/OpenMP-hybrid Execution (PSMP), CP2K with Population Analysis capabilities- CP2K-popt</p>"},{"location":"Software/chemistry/cp2k/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/cp2k/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/gaussian/","title":"Gaussian","text":""},{"location":"Software/chemistry/gaussian/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command Gaussian 16.C.01-AVX2~ - <code>module load Gaussian/16.C.01-AVX2~</code> Gaussian 16.C.01-AVX2 - <code>module load Gaussian/16.C.01-AVX2</code> </p> <p> Software Version Dependent Toolchain Module Load Command </p>"},{"location":"Software/chemistry/gaussian/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/gaussian/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/orca/","title":"ORCA","text":""},{"location":"Software/chemistry/orca/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command ORCA 5.0.4 foss/2022b <code>module load foss/2022b ORCA/5.0.4</code> </p> <p> Software Version Dependent Toolchain Module Load Command ORCA 5.0.3 foss/2021b <code>module load foss/2021b ORCA/5.0.3</code> </p>"},{"location":"Software/chemistry/orca/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/orca/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/qe/","title":"Quantum Espresso","text":""},{"location":"Software/chemistry/qe/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command QuantumESPRESSO 7.1 foss/2021b <code>module load foss/2021b QuantumESPRESSO/7.1</code> </p> <p> Software Version Dependent Toolchain Module Load Command QuantumESPRESSO 7.1 foss/2021b <code>module load foss/2021b QuantumESPRESSO/7.1</code> QuantumESPRESSO 6.8 intel/2021a <code>module load intel/2021a QuantumESPRESSO/6.8</code> </p>"},{"location":"Software/chemistry/qe/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/qe/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/siesta/","title":"SIESTA","text":"<p>SIESTA (Spanish Initiative for Electronic Simulations with Thousands of Atoms) is a free and open-source software package for performing electronic structure calculations of materials using density functional theory (DFT). It is particularly well-suited for simulating materials containing large numbers of atoms, such as nanomaterials and surfaces.</p> <p>SIESTA uses a localized basis set approach that reduces the computational cost of DFT calculations by approximating the electronic wavefunction using a small number of basis functions centered around each atom. This allows SIESTA to efficiently simulate systems containing thousands of atoms with reasonable accuracy.</p> <p>SIESTA is actively developed and maintained by a team of researchers at the Universidad Aut\u00f3noma de Madrid in Spain, and is available as a free download under an open-source license. It is widely used in the fields of material science, chemistry, and physics for studying a wide range of materials, including metals, semiconductors, and biological molecules.</p>"},{"location":"Software/chemistry/siesta/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command </p> <p> Software Version Dependent Toolchain Module Load Command Siesta 4.1.5 foss/2020a <code>module load foss/2020a Siesta/4.1.5</code> Siesta master-4.1.5 intel/2021b <code>module load intel/2021b Siesta/master-4.1.5</code> </p>"},{"location":"Software/chemistry/siesta/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/siesta/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/math/MATLAB/","title":"MATLAB","text":"<p>MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages. Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.</p>"},{"location":"Software/math/MATLAB/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command MATLAB 2023a - <code>module load MATLAB/2023a</code> </p> <p> Software Version Dependent Toolchain Module Load Command MATLAB 2022a - <code>module load MATLAB/2022a</code> </p>"},{"location":"Software/math/MATLAB/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of MATLAB is available at MATLAB Tutorial</p>"},{"location":"Software/math/MATLAB/#using-matlab","title":"Using MATLAB","text":""},{"location":"Software/math/MATLAB/#serial-job","title":"Serial Job","text":"Sample Batch Script to run MATLAB: matlab-serial.sh WulverLochness <pre><code>#!/bin/bash\n#SBATCH -J test_matlab\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Load matlab module\nmodule purge\nmodule load wulver # Load the slurm, easybuild \nmodule load MATLAB\n\nmatlab -nodisplay -nosplash -r test\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH -J test_matlab\n#SBATCH --partition=public\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH - t 30:00\n\n# Load matlab module\nmodule purge\nmodule load MATLAB/2022a\n\nmatlab -nodisplay -nosplash -r test\n</code></pre> Sample MATLAB script <pre><code>A = [ 1 2; 3 4]\nA.**2\n</code></pre>"},{"location":"Software/math/MATLAB/#parallel-job","title":"Parallel Job","text":""},{"location":"Software/math/MATLAB/#single-node-parallelization","title":"Single node parallelization","text":"Sample Batch Script to run MATLAB: matlab_parallel.sh WulverLochness <pre><code>#!/bin/bash\n#SBATCH -J test_matlab\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Load matlab module\nmodule purge\nmodule load wulver # Load the slurm, easybuild\nmodule load MATLAB\n\n# Run matlab\nmatlab -nodisplay -nosplash -r 'for_loop; quit'\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH -J test_matlab\n#SBATCH --partition=public\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --time=30:00\n\n# Load matlab module\nmodule purge\nmodule load MATLAB/2022a\n\n# Run matlab\nmatlab -nodisplay -nosplash -r 'for_loop; quit'\n</code></pre> Sample Parallel MATLAB script: for_loop.m <pre><code>poolobj = parpool('local',32);\nfprintf('Number of workers: %g\\n', poolobj.NumWorkers);\n\ntic\nn = 2000;\nA = 500;\na = zeros(n);\nparfor i = 1:n\n    a(i) = max(abs(eig(rand(A))));\nend\ntoc\n</code></pre>"},{"location":"Software/math/MATLAB/#multi-node-parallelization","title":"Multi node parallelization","text":"<p>If you want to learn how to install a version of MATLAB on your local system and use it to run jobs on Wulver, please see Using Local MATLAB on Wulver</p>"},{"location":"Software/math/MATLAB/matlab_local/","title":"Use MATLAB on NJIT HPC","text":"<p>Warning</p> <p>Please note that the following instructions are applicatible for Lochness only. We will soon update the instructions for Wulver.</p>"},{"location":"Software/math/MATLAB/matlab_local/#installation-steps-of-matlab-on-local-machine","title":"Installation steps of MATLAB on local machine","text":"<ul> <li>Go to Mathworks Download and register with your NJIT email address.</li> <li>Select the R2022a version to download. </li> <li>User needs to select the correct installer based on the OS (Mac or Windows). </li> <li>Run the installer.</li> </ul> <ul> <li>Make sure to check Parallel Computing Toolbox option.</li> </ul> <ul> <li>Continue by selecting Next and MATLAB will be installed on your computer.</li> </ul>"},{"location":"Software/math/MATLAB/matlab_local/#setup-slurm-profile-to-run-matlab-on-lochness","title":"Setup Slurm profile to run MATLAB on Lochness","text":"<p>Following this procedure a user will be able to submit jobs to lochness or stheno from Matlab running locally on the user's computer.</p>"},{"location":"Software/math/MATLAB/matlab_local/#installing-the-add-on","title":"Installing the Add-On","text":"<p>From the Matlab window, click on \"Add-ons\" and select \"Get Add-Ons.\"</p> <p></p> <p>In the search box enter \"slurm\" and click on the magnifying glass icon. Select \"Parallel Computing Toolbox plugin for MATLAB Parallel Server with Slurm\". Alternatively, this Add-On can be downloaded directly from the Mathworks site.</p> <p></p> <p>Click on \"Install.\"</p> <p></p>"},{"location":"Software/math/MATLAB/matlab_local/#creating-a-profile-for-lochness-or-stheno","title":"Creating a Profile for Lochness or Stheno","text":"<p>The following steps will create a profile for lochness (or stheno). Click Next to begin.</p> <p></p> <p>In the \"Operating System\" screen <code>Unix</code> is already selected. Click Next to continue.</p> <p></p> <p>This \"Submission Mode\" screen determines whether or not to use a <code>shared</code> or <code>nonshared</code> submission mode. Since Matlab installed on your personal computer or laptop does not use a shared job location storage, select \"No\" where indicated and click Next to continue.</p> <p></p> <p>Click Next to continue.</p> <p></p> <p>In the \"Connection Details\" screen, enter the cluster host, either \"lochness.njit.edu\" or \"stheno.njit.edu.\" Enter your UCID for the username. Select \"No\" for the \"Do you want to use an identity file to log in to the cluster\" option and click next to continue.</p> <p></p> <p>In the \"Cluster Details\" screen enter the full path to the directory on lochness to store the Matlab job files. In the case the directory is $HOME/MDCS. MDCS stands for Matlab Distributed Computing Server. It is not necessary to name this directory MDCS. This directory can be named anything you wish. To determine the value of $HOME, logon to lochness. For details on how to Logon to Lochness from local computer please see this link. Once connected to Lochness run the following:</p> <pre><code>  login-1-45 ~ &gt;: echo $HOME\n  /home/g/guest24\n</code></pre> <p>Make sure to check the box Use unique subfolders . Click Next to continue.</p> <p></p> <p>In the \"Workers\" screen enter <code>512</code> for the number of workers and <code>/opt/site/easybuild/software/MATLAB/2022a</code> for <code>MATLAB installation folders for workers</code>. Click Next to continue.</p> <p></p> <p>In the \"License\" screen make sure to select \"Network license manager\" and click Next to continue.</p> <p></p> <p>In the \"Profile Details\" screen enter either \"Lochness\" or \"Stheno\" depending on which cluster you are making a profile for. The \"Cluster description\" is optional and may be left blank. Click Next to continue.</p> <p></p> <p>In the \"Summary\" screen make sure everything is correct and click \"Create.\"</p> <p></p> <p>In the \"Profile Created Successfully\" screen, check the \"Set the new profile as default\" box and click on \"Finish.\"</p> <p></p>"},{"location":"Software/math/MATLAB/matlab_local/#submitting-a-serial-job","title":"Submitting a Serial Job","text":"<p>This section will demonstrate how to create a cluster object and submit a simple job to the cluster. The job will run the 'hostname' command on the node assigned to the job. The output will indicate clearly that the job ran on the cluster and not on the local computer.</p> <p>The hostname.m file used in this demonstration can be downloaded here.</p> <p><pre><code> &gt;&gt; c=parcluster \n</code></pre> </p> <p>Certain arguments need to be passed to SLURM in order for the job to run properly. Here we will set values for partion, mem-per-cpu and time. In the Matlab window enter: <pre><code> &gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs=['--partition=public --mem-per-cpu=10G --time=2-00:00:00'] \n</code></pre> To make this persistent between Matlab sessions these arguments need to be saved to the profile. In the Matlab window enter: <pre><code> &gt;&gt; c.saveProfile \n</code></pre> </p> <p>We will now submit the hostname.m function to the cluster. In the Matlab window enter the following: <pre><code>&gt;&gt; j=c.batch(@hostname, 1, {}, 'AutoAddClientPath', false); \n</code></pre> @: Submitting a function.\\ 1: The number of output arguments from the evaluated function.\\ {}: Cell array of input arguments to the function. In this case empty.\\ 'AutoAddClientPath', false: The client path is not available on the cluster.</p> <p>When the job is submitted, you will be prompted for your password.</p> <p>For more information see the Mathworks page: batch </p> <p></p> <p>To wait for the job to finish, enter the following in the Matlab window: <pre><code> &gt;&gt;j.wait\n</code></pre> Finally, to get the results: <pre><code> &gt;&gt;fetchOutputs(j)\n</code></pre> As can be seen, this job ran on node720</p> <p></p>"},{"location":"Software/math/MATLAB/matlab_local/#submitting-a-parallel-function","title":"Submitting a Parallel Function","text":"<p>The \"Job Monitor\" is a convenient way to monitor jobs submitted to the cluster. In the Matlab window select \"Parallel\" and then \"Monitor Jobs.\"</p> <p>For more information see the Mathworks page: Job Monitor</p> <p></p> <p>Here we will submit a simple function using a \"parfor\" loop. The code for this example is as follows: <pre><code>function t = parallel_example\n\nt0 = tic;\nparfor idx = 1:16\n        A(idx) = idx;\n        pause (2)\nend\n\nt=toc(t0);\n</code></pre> To submit this job: <pre><code> &gt;&gt; j=c.batch(@parallel_example, 1, {}, 'AutoAddClientPath', false, 'Pool', 7)\n</code></pre> Since this is a parallel job a 'Pool' must be started. The actual number of tasks started will be one more than requested in the pool. I this case, the batch command calls for a pool of seven. Eight tasks will be started on the cluster.</p> <p>Also see that the state of the job in the \"Job Monitor\" is \"running.\" </p> <p>The job takes a few minutes to run and the state of the job changes to \"finished.\" </p> <p>Once again to get the results enter: <pre><code> &gt;&gt; fetchOutputs(j) \n</code></pre> As can be seen the parfor loop was completed in 6.7591 seconds. </p>"},{"location":"Software/math/MATLAB/matlab_local/#submitting-a-script-requiring-a-gpu","title":"Submitting a Script Requiring a GPU","text":"<p>In this section we will submit a matlab script using a GPU. The results will be written to the job diary. The code for this example is as follows: <pre><code>% MATLAB script that defines a random matrix and does FFT\n%\n% The first FFT is without a GPU\n% The second is with the GPU\n%\n% MATLAB knows to use the GPU the second time because it\n%   is passed a type gpuArray as an argument to FFT\n% We do the FFT a bunch of times to make using the GPU worth it,\n%   or else it spends more time offloading to the GPU\n%   than performning the calculation\n%\n% This example is meant to provide a general understanding\n%   of MATLAB GPU usage\n% Meaningful performance measurements depend on many factors\n%   beyond the scope of this example\n% Downloaded from https://projects.ncsu.edu/hpc/Software/examples/matlab/gpu/gpu_m\n\n% Define a matrix\nA1 = rand(3000,3000);\n\n% Just use the compute node, no GPU\ntic;\n% Do 1000 FFT's\nfor i = 1:1000\n      B2 = fft(A1);\nend\ntime1 = toc;\nfprintf('%s\\n',\"Time to run FFT on the node:\")\ndisp(time1);\n\n% Use GPU\ntic;\nA2 = gpuArray(A1);\n% Do 1000 FFT's\nfor i = 1:1000\n      % MALAB knows to use GPU FFT because A2 is defined by gpuArray\n        B2 = fft(A2);\nend\ntime2 = toc;\nfprintf('%s\\n',\"Time to run FFT on the GPU:\")\ndisp(time2);\n\n% Will be greater than 1 if GPU is faster\nspeedup = time1/time2 \n</code></pre> We will need to change the partition to datasci and request a gpu. In the Matlab window enter: <pre><code> &gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs=['--partition=datasci --gres=gpu:1 --mem-per-cpu=10G --time=2-00:00:00'] \n</code></pre> </p> <p>Submit the job as before. Since a script is submitted as opposed to a function, only the name of the script is included in the batch command. Do not include the '@' symbol. In a script there are no inputs or ouptuts. <pre><code> &gt;&gt; j=c.batch('gpu', 'AutoAddClientPath', false) \n</code></pre> </p> <p>To get the result: <pre><code> &gt;&gt; j.diary \n</code></pre> </p>"},{"location":"Software/math/MATLAB/matlab_local/#load-and-plot-results-from-a-job","title":"Load and Plot Results from A Job","text":"<p>In this section we will run a job on the cluster and then load and plot the results in the local Matlab workspace. The code for this example is as follows: <pre><code>n=100;\ndisp(\"n = \" + n);\nA = gallery('poisson',n-2);\nb = convn(([1,zeros(1,n-2),1]'|[1,zeros(1,n-1)]), 0.5*ones(3,3),'valid')';\nx = reshape(A\\b(:),n-2,n-2)';%\n</code></pre> As before submit the job: <pre><code> &gt;&gt; j=c.batch('plot_demo', 'AutoAddClientPath', false);\n</code></pre> </p> <p>To load 'x' into the local Matlab workspace: <pre><code> &gt;&gt; load(j,'x') \n</code></pre> </p> <p>Finally, plot the results: <pre><code> &gt;&gt; plot(x) \n</code></pre> </p>"},{"location":"Software/md/gromacs/","title":"GROMACS","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.</p> <p>It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"Software/md/gromacs/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command GROMACS 2021.5 foss/2021b <code>module load foss/2021b GROMACS/2021.5</code> GROMACS 2021.5-CUDA-11.4.1 foss/2021b <code>module load foss/2021b GROMACS/2021.5-CUDA-11.4.1</code> GROMACS 2024.1 foss/2023b <code>module load foss/2023b GROMACS/2024.1</code> GROMACS 2023.1-CUDA-12.0.0 foss/2022b <code>module load foss/2022b GROMACS/2023.1-CUDA-12.0.0</code> </p> <p> Software Version Dependent Toolchain Module Load Command GROMACS 2020.4-Python-3.8.2 foss/2020a <code>module load foss/2020a GROMACS/2020.4-Python-3.8.2</code> GROMACS 2021.5-CUDA-11.4.1 foss/2021b <code>module load foss/2021b GROMACS/2021.5-CUDA-11.4.1</code> GROMACS 2021.5 foss/2021b <code>module load foss/2021b GROMACS/2021.5</code> </p>"},{"location":"Software/md/gromacs/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of GROMACS is available at GROMACS Manual, where you can find the tutorials in topologies, input file format, setting parameters, etc. </p>"},{"location":"Software/md/gromacs/#using-gromacs","title":"Using GROMACS","text":"<p>GROMACS can be used on CPU or GPU. When using GROMACS with GPUs (Graphics Processing Units), the calculations can be significantly accelerated, allowing for faster simulations. You can use GROMACS with GPU acceleration, but you need to use GPU nodes on our cluster. </p> Sample Batch Script to Run GROMACS on GPU gmx_gpu.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n# NOTE the -l (login) flag!\n#SBATCH -J gmx2023\n#SBATCH -o test.%x.%j.out\n#SBATCH -e test.%x.%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --time 72:00:00   # Max 3 days\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --cpus-per-task=2\n#SBATCH --gpus-per-node=4  \n#SBATCH --account=PI_ucid  # Replace PI_ucid with the UCID of PI, if you don't know PI's UCID use \"sacctmgr show user username\" on the login screen, replace \"username\" with your UCID\n\nmodule purge\nmodule load wulver\nmodule load foss/2022b GROMACS/2023.1-CUDA-12.0.0\n\nINPUT_DIR=${PWD}/INPUT\nOUTPUT_DIR=${PWD}/OUTPUT\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\nsrun gmx_mpi mdrun -deffnm run -cpi -v -ntomp 2 -pin on -tunepme -dlb yes -nb gpu -noappend\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=gpu-esculentin\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --partition=datasci\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --gres=gpu:1 # Number of GPUs per node\n#SBATCH --time=24:59:00  # D-HH:MM:SS\n#SBATCH --mail-type=ALL\n\n################################################\n#\n# Purge and load modules needed to run\n#\n################################################\nmodule purge\nmodule load foss/2021b CUDA/11.4.1\nmodule load GROMACS/2021.5-CUDA-11.4.1\n\n###############################################\n#\n# cd into case directory if required\n#\n################################################\nINPUT_DIR=${PWD}/input_files_v2020\nOUTPUT_DIR=${PWD}/output\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\n###############################################\n#\n# Run simulation\n#\n##############################################\ngmx mdrun -v -deffnm em -nt 16 -nb gpu\n</code></pre> Sample Batch Script to Run GROMACS on CPU gmx_cpu.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n# NOTE the -l (login) flag!\n#SBATCH -J gmx2021\n#SBATCH -o test.%x.%j.out\n#SBATCH -e test.%x.%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --time 72:00:00   # Max 3 days\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --account=PI_ucid  # Replace PI_ucid with the UCID of PI, if you don't know PI's UCID use \"sacctmgr show user username\" on the login screen, replace \"username\" with your UCID\n\nmodule purge\nmodule load wulver\nmodule load foss/2021b GROMACS/2021.5\n\nINPUT_DIR=${PWD}/INPUT\nOUTPUT_DIR=${PWD}/OUTPUT\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\nsrun gmx_mpi mdrun -v -deffnm em -cpi -v -ntomp 1 -pin on -tunepme -dlb yes -noappend\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=gpu-esculentin\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --partition=datasci\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --gres=gpu:1 # Number of GPUs per node\n#SBATCH --time=24:59:00  # D-HH:MM:SS\n#SBATCH --mail-type=ALL\n\n################################################\n#\n# Purge and load modules needed to run\n#\n################################################\nmodule purge\nmodule load foss/2021b \nmodule load GROMACS/2021.5\n\n###############################################\n#\n# cd into case directory if required\n#\n################################################\nINPUT_DIR=${PWD}/input_files_v2020\nOUTPUT_DIR=${PWD}/output\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\n###############################################\n#\n# Run simulation\n#\n##############################################\nsrun gmx_mpi mdrun -v -deffnm em -cpi -v -ntomp 2 -pin on -tunepme -dlb yes -nb gpu -noappend\n</code></pre> <p>The tutorial in the above-mentioned job script can be found in </p> WulverLochness <p><code>/apps/testjobs/gromacs</code></p> <p><code>/opt/site/examples/gromacs</code></p>"},{"location":"Software/md/gromacs/#related-applications","title":"Related Applications","text":"<ul> <li>LAMMPS</li> </ul>"},{"location":"Software/md/gromacs/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/md/lammps/","title":"LAMMPS","text":"<p>LAMMPS is a large scale classical molecular dynamics code, and stands for Large-scale Atomic/Molecular Massively Parallel Simulator.  LAMMPS has potentials for soft materials (biomolecules, polymers), solid-state materials (metals, semiconductors) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, meso, or continuum scale.</p>"},{"location":"Software/md/lammps/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command LAMMPS 23Jun2022-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos-CUDA-11.4.1</code> LAMMPS 23Jun2022-kokkos foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos</code> </p> <p> Software Version Dependent Toolchain Module Load Command LAMMPS 3Mar2020-Python-3.8.2-kokkos foss/2020a <code>module load foss/2020a LAMMPS/3Mar2020-Python-3.8.2-kokkos</code> LAMMPS 23Jun2022-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos-CUDA-11.4.1</code> </p> <p>Note</p> <p>To know the deatils about dependent toolchain please go to Toolchains</p>"},{"location":"Software/md/lammps/#application-information-documentation-and-support","title":"Application Information, Documentation and Support","text":"<p>The official LAMMPS is available at LAMMPS Online Manual. LAMMPS has a large user base and a good user support. Question related to using LAMMPS can be posted to the LAMMPS User forum. Archived user mailing list are also useful to resolve some of the common user issues. </p> <p>Tip</p> <p>If after checking the above forum, if you believe that there is an issue with the module, please file a ticket with Service Now</p>"},{"location":"Software/md/lammps/#using-lammps","title":"Using LAMMPS","text":"Sample Batch Script to Run LAMMPS WulverLochness <pre><code>#!/bin/bash\n#SBATCH -J test_lammps\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n###############################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2021b LAMMPS\n\nsrun -n $SLURM_NTASKS lmp -in test.in\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH -J test_lammps\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --partition=public\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=10G # Adjust as necessary\n#SBATCH --time=00:01:00  # D-HH:MM:SS\n\n###############################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load foss/2021b LAMMPS\n\nsrun -n $SLURM_NTASKS lmp -in test.in\n</code></pre> <p>Then submit the job script using the sbatch command, e.g., assuming the job script name is <code>test_lammps.slurm</code>:</p> <pre><code>sbatch test_lammps.slurm\n</code></pre>"},{"location":"Software/md/lammps/#building-lammps-from-source","title":"Building LAMMPS from source","text":"<p>Some users may be interested in building LAMMPS from source to enable more specific LAMMPS packages.  The source files for LAMMPS can be downloaded as either a tar file  or from the LAMMPS Github repository. </p> Building on Cluster <p>The following procedure was used to build LAMMPS on Wulver.  In the terminal:</p> WulverLochness <pre><code>module purge\nmodule load wulver\nmodule load foss\nmodule load CMake\n\ngit clone https://github.com/lammps/lammps.git\ncd lammps\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=$PWD/../install_hsw -DCMAKE_CXX_COMPILER=mpicxx \\\n            -DCMAKE_BUILD_TYPE=Release -D BUILD_MPI=yes -DKokkos_ENABLE_OPENMP=ON \\\n            -DKokkos_ARCH_HSW=ON -DCMAKE_CXX_STANDARD=17 -D PKG_MANYBODY=ON \\\n            -D PKG_MOLECULE=ON -D PKG_KSPACE=ON -D PKG_REPLICA=ON -D PKG_ASPHERE=ON \\\n            -D PKG_RIGID=ON -D PKG_KOKKOS=ON -D DOWNLOAD_KOKKOS=ON \\\n            -D CMAKE_POSITION_INDEPENDENT_CODE=ON -D CMAKE_EXE_FLAGS=\"-dynamic\" ../cmake\nmake -j16\nmake install\n</code></pre> <pre><code>module purge\nmodule load foss\nmodule load CMake\n\ngit clone https://github.com/lammps/lammps.git\ncd lammps\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=$PWD/../install_hsw -DCMAKE_CXX_COMPILER=mpicxx \\\n            -DCMAKE_BUILD_TYPE=Release -D BUILD_MPI=yes -DKokkos_ENABLE_OPENMP=ON \\\n            -DKokkos_ARCH_HSW=ON -DCMAKE_CXX_STANDARD=17 -D PKG_MANYBODY=ON \\\n            -D PKG_MOLECULE=ON -D PKG_KSPACE=ON -D PKG_REPLICA=ON -D PKG_ASPHERE=ON \\\n            -D PKG_RIGID=ON -D PKG_KOKKOS=ON -D DOWNLOAD_KOKKOS=ON \\\n            -D CMAKE_POSITION_INDEPENDENT_CODE=ON -D CMAKE_EXE_FLAGS=\"-dynamic\" ../cmake\nmake -j16\nmake install\n</code></pre>"},{"location":"Software/md/lammps/#related-applications","title":"Related Applications","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"Software/md/lammps/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/md/plumed/","title":"PLUMED","text":""},{"location":"Software/md/plumed/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command PLUMED 2.8.0 intel/2021b <code>module load intel/2021b PLUMED/2.8.0</code> PLUMED 2.7.3 foss/2021b <code>module load foss/2021b PLUMED/2.7.3</code> PLUMED 2.9.0 foss/2022b <code>module load foss/2022b PLUMED/2.9.0</code> </p> <p> Software Version Dependent Toolchain Module Load Command PLUMED 2.6.0-Python-3.8.2 foss/2020a <code>module load foss/2020a PLUMED/2.6.0-Python-3.8.2</code> PLUMED 2.6.2 foss/2020b <code>module load foss/2020b PLUMED/2.6.2</code> PLUMED 2.7.3 foss/2021b <code>module load foss/2021b PLUMED/2.7.3</code> PLUMED 2.7.2 intel/2021a <code>module load intel/2021a PLUMED/2.7.2</code> </p>"},{"location":"Software/md/plumed/#related-applications","title":"Related Applications","text":""},{"location":"Software/md/plumed/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/programming/compilers/","title":"Compilers and Toolchains","text":""},{"location":"Software/programming/compilers/#gnu-and-intel-compilers","title":"GNU and Intel Compilers","text":"<p>We offer both GNU and Intel compilers. Here is the list of compilers you can find on our cluster.</p> WulverLochness <p> Software Version Dependent Toolchain Module Load Command intel-compilers 2022.2.1 intel/2022b <code>module load intel-compilers/2022.2.1</code> intel-compilers 2021.4.0 intel/2021b <code>module load intel-compilers/2021.4.0</code> GCC 12.2.0 foss/2022b <code>module load GCC/12.2.0</code> GCC 13.2.0 foss/2023b <code>module load GCC/13.2.0</code> GCC 11.2.0 foss/2021b <code>module load GCC/11.2.0</code> </p> <p> Software Version Dependent Toolchain Module Load Command GCC 9.3.0 foss/2020a <code>module load GCC/9.3.0</code> GCC 10.2.0 foss/2020b <code>module load GCC/10.2.0</code> GCC 10.3.0 foss/2021a <code>module load GCC/10.3.0</code> GCC 11.2.0 foss/2021b <code>module load GCC/11.2.0</code> GCC 12.2.0 foss/2022b <code>module load GCC/12.2.0</code> intel-compilers 2021.4.0 intel/2021b <code>module load intel-compilers/2021.4.0</code> intel-compilers 2021.2.0 intel/2021a <code>module load intel-compilers/2021.2.0</code> </p>"},{"location":"Software/programming/compilers/#mpi-libraries","title":"MPI Libraries","text":"<p>MPI (Message Passing Interface) libraries are a set of software tools that allow for parallel computing on distributed memory systems, such as computer clusters. These libraries provide a standardized interface for communication between processes running on different nodes of the cluster. There are several implementations of MPI libraries available, such as OpenMPI, MPICH, and Intel MPI. Currently, the following MPI libraries on our cluster.</p> WulverLochness <p> Software Version Dependent Toolchain Module Load Command impi 2021.4.0 intel/2021b <code>module load intel/2021b</code> impi 2021.7.1 intel/2022b <code>module load intel/2022b</code> OpenMPI 4.1.1 foss/2021b <code>module load foss/2021b</code> OpenMPI 4.1.6 foss/2023b <code>module load foss/2023b</code> OpenMPI 4.1.4 foss/2022b <code>module load foss/2022b</code> </p> <p> Software Version Dependent Toolchain Module Load Command OpenMPI 4.0.3 foss/2020a <code>module load foss/2020a</code> OpenMPI 4.0.5 foss/2020b <code>module load foss/2020b</code> OpenMPI 4.1.1 foss/2021a <code>module load foss/2021a</code> OpenMPI 4.1.1 foss/2021b <code>module load foss/2021b</code> OpenMPI 4.1.4 foss/2022b <code>module load foss/2022b</code> impi 2021.4.0 intel/2021b <code>module load intel/2021b</code> impi 2021.2.0 intel/2021a <code>module load intel/2021a</code> </p>"},{"location":"Software/programming/compilers/#toolchains","title":"Toolchains","text":"<p>We use EasyBuild to install the packages as modules and to avoid too many packages tol load as a module, we use pre-defined build environment modules called toolchains which include a combination of tools such as compilers, libraries etc. We use <code>foss</code> and <code>intel</code> toolchains in Wulver. The advantage of using toolchains is that user can load either <code>foss</code> or <code>intel</code> as base package and the additional libraries such as MPI, LAPACK and other math libraries will be automatically loaded. </p>"},{"location":"Software/programming/compilers/#free-open-source-software-foss","title":"Free Open Source Software (foss)","text":"<p>The <code>foss</code> toolchains are versioned with a yearletter scheme, e.g. <code>foss/2021b</code> is the second foss toolchain composed in 2021. The <code>foss</code> toolchain comprises the following </p> <pre><code>\ud83d\udcc1 foss\n\u251c\u2500\u2500 \ud83d\udcc1 gompi \n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 GCC\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 GCCcore\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 zlib\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 binutils\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 OpenMPI\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 numactl\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 XZ\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 libxml2\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 libpciaccess\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 hwloc\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 OpenSSL\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 libevent\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 UCX\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 libfabric\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 PMIx\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 UCC\n\u251c\u2500\u2500 \ud83d\udcc4 FFTW\n\u251c\u2500\u2500 \ud83d\udcc4 OpenBLAS\n\u251c\u2500\u2500 \ud83d\udcc4 ScaLAPACK\n\u2514\u2500\u2500 \ud83d\udcc4 FlexiBLAS\n</code></pre> WulverLochness <p> Software Version Dependent Toolchain Module Load Command foss 2022b - <code>module load foss/2022b</code> foss 2023b - <code>module load foss/2023b</code> foss 2021b - <code>module load foss/2021b</code> </p> <p> Software Version Dependent Toolchain Module Load Command foss 2020a - <code>module load foss/2020a</code> foss 2019b - <code>module load foss/2019b</code> foss 2020b - <code>module load foss/2020b</code> foss 2021a - <code>module load foss/2021a</code> foss 2021b - <code>module load foss/2021b</code> foss 2022b - <code>module load foss/2022b</code> </p> <p>To see GCC and OpenMPI versions details in each toolchain, see the list of compiler versions and OpenMPI versions.</p>"},{"location":"Software/programming/compilers/#intel","title":"Intel","text":"<p>Like <code>foss</code>, <code>intel</code> toolchains are versioned with yearletter scheme, e.g. <code>intel/2021b</code> is the second intel toolchain composed in 2021.</p> <pre><code>\ud83d\udcc1 intel\n\u251c\u2500\u2500 \ud83d\udcc1 intel-compilers\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 GCCcore   \n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 zlib\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 binutils\n\u251c\u2500\u2500 \ud83d\udcc1 iimpi\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 iccifort\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 Intel MPI\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 numactl\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 UCX\n\u2514\u2500\u2500 \ud83d\udcc4 Intel Math Kernel Library\n</code></pre> WulverLochness <p> Software Version Dependent Toolchain Module Load Command intel 2022b - <code>module load intel/2022b</code> intel 2022a - <code>module load intel/2022a</code> intel 2021b - <code>module load intel/2021b</code> </p> <p> Software Version Dependent Toolchain Module Load Command intel 2021b - <code>module load intel/2021b</code> intel 2021a - <code>module load intel/2021a</code> </p> <p>The <code>intel-compilers</code> and <code>impi</code> versions in <code>intel</code> toolchains are tabulated in intel versions and impi versions. To see the versions of <code>GCCcore</code> and <code>mkl</code> libraries of <code>intel</code> toolchain, please load the intel toolchain module with yearletter version, e.g. <code>module load intel/2021b</code> and then use <code>module li</code>.</p>"},{"location":"Software/programming/python/","title":"Python","text":"<p>Python is a high-level, general-purpose programming language. Python supports multiple programming paradigms, including structured, object-oriented, and functional programming. It is used in a wide range of applications such as machine learning, molecular dynamics, scientific computing, automation, image processing, etc.</p>"},{"location":"Software/programming/python/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command Python 3.9.6 foss/2021b <code>module load foss/2021b Python/3.9.6</code> Python 3.11.5 foss/2023b <code>module load foss/2023b Python/3.11.5</code> Python 3.10.8 foss/2022b <code>module load foss/2022b Python/3.10.8</code> </p> <p> Software Version Dependent Toolchain Module Load Command Python 3.8.2 foss/2020a <code>module load foss/2020a Python/3.8.2</code> Python 2.7.18 foss/2020a <code>module load foss/2020a Python/2.7.18</code> Python 3.8.6 foss/2020b <code>module load foss/2020b Python/3.8.6</code> Python 2.7.18 foss/2020b <code>module load foss/2020b Python/2.7.18</code> Python 3.9.5 foss/2021a <code>module load foss/2021a Python/3.9.5</code> Python 3.9.6 foss/2021b <code>module load foss/2021b Python/3.9.6</code> Python 3.10.8 foss/2022b <code>module load foss/2022b Python/3.10.8</code> </p>"},{"location":"Software/programming/python/#python-libraries","title":"Python libraries","text":"<p>Apart from Python\u2019s standard library, Python offers a wide range of additional libraries that need to be loaded as modules before users can use these. Here, we list these additional libraries. Please contact us to file a ticket with Service Now in case you do not find the libraries you want to use.</p> Libraries Version Python Version Module load command NumPy 1.21.3 3.9.6 <code>module load foss/2021b SciPy-bundle</code> Matplotlib 3.4.3 3.9.6 <code>module load foss/2021b matplotlib</code> SciPy 2021.10 3.9.6 <code>module load foss/2021b SciPy-bundle</code> <p>For using multiple libraries, you simply need to add the library name in <code>module load</code> command. For example, to load NumPy, Matplotlib, and SciPy together, you need to use the following command. </p> <pre><code>module load foss/2021b SciPy-bundle matplotlib\n</code></pre>"},{"location":"Software/programming/python/#using-conda-or-pip-to-install-python-libraries","title":"Using Conda or <code>pip</code> to Install Python Libraries","text":"<p>Since sometimes users a specific version of Python libraries, it is advisable to use Conda so that users can create their own environment where they can install required packages based on their requirements. Conda is a cross-language package manager that excels at managing environments and dependencies, making it suitable for scientific computing and data science projects. It can handle non-Python libraries and binaries, offering a comprehensive solution. On the other hand, pip is the default Python package installer, known for its simplicity and compatibility with the Python Package Index (PyPI). While both tools serve the same purpose, it is generally recommended to choose one and remain consistent within a project to avoid potential conflicts. Please see Conda Documentation on how to install Python packages via Conda.</p>"},{"location":"Software/programming/python/conda/","title":"Conda","text":"<p>Since Python supports a wide range of additional libraries in machine learning or data science research, it is not always possible to install every package on HPC. Also, users sometimes need to use a specific version of Python or its libraries to conduct their research. Therefore, in that case, users can build their own Python version along with a specific library. One of the ways to accomplish this is to use Conda.</p>"},{"location":"Software/programming/python/conda/#conda","title":"Conda","text":"<p>Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. </p>"},{"location":"Software/programming/python/conda/#availability","title":"Availability","text":"<p>Conda can be accessed on the cluster as <code>Anaconda3</code> or <code>Miniconda3</code> module.</p> WulverLochness <p> Software Version Dependent Toolchain Module Load Command Anaconda3 2023.09-0 - <code>module load Anaconda3/2023.09-0</code> Anaconda3 5.3.0 - <code>module load Anaconda3/5.3.0</code> </p> <p> Software Version Dependent Toolchain Module Load Command Anaconda3 2022.05 - <code>module load Anaconda3/2022.05</code> Miniconda3 4.9.2 - <code>module load Miniconda3/4.9.2</code> </p> <p>Users can use conda after using any of the modules mentioned above</p> <p>module a <code>Anaconda3</code> module. Users can use <code>Anaconda3</code> to create virtual Python environments to manage Python modules.</p>"},{"location":"Software/programming/python/conda/#create-and-activate-a-conda-virtual-environment","title":"Create and Activate a Conda Virtual Environment","text":"<p>Load the Anaconda Module</p> <pre><code>module load Anaconda3\n</code></pre>"},{"location":"Software/programming/python/conda/#create-environment-with-conda","title":"Create Environment with <code>conda</code>","text":"<p>To create an environment use the <code>conda create</code> command. Once the environment is created, you need to use <code>source conda.sh</code> to activate the path. To create an environment use <code>conda create --name ENV python=3.9</code> where <code>ENV</code> is the name of the environment. You can choose any environment name of your choice.</p>"},{"location":"Software/programming/python/conda/#activate-and-deactivate-conda-environment","title":"Activate and Deactivate Conda Environment","text":"<p>Once you create an environment, you need to activate the environment to install python packages Use <code>conda activate ENV</code> to activate the Conda environment (<code>ENV</code> is the name of the environment). Following the activation of the conda environment, the name of the environment appears at the left of the hostname in the terminal. </p> <pre><code>login1-41 ~ &gt;: module load Anaconda3\nlogin1-41 ~ &gt;: source conda.sh\nlogin1-41 ~ &gt;: conda create --name ENV python=3.9\nlogin1-41 ~ &gt;: conda activate ENV\n(ENV) login-41 ~ &gt;:\n</code></pre> <p>Once you finish the installation of Python packages, deactivate the conda environment using <code>conda deactivate ENV</code>. </p> <p>Warning</p> <p>Please note that you may need to create multiple Conda environments, as some packages may not work in a single environment. For example, if you want to install PyTorch and TensorFlow, it's advisable to create separate environments as sometimes both packages in a single environment can cause errors. To create another environment make sure to deactivate the previous environment by using the <code>conda deactivate</code> command. </p>"},{"location":"Software/programming/python/conda/#install-python-packages-via-conda","title":"Install Python Packages Via Conda","text":"<p>Once Conda environment is activated, you can install packages via <code>conda install package_name</code> command. For example, if you want to install <code>matplotlib</code>, you need to use</p> <p><pre><code>(ENV) login-41 ~ &gt;: conda install matplotlib\n</code></pre> Make sure to activate the conda environment prior to installing Python packages. </p>"},{"location":"Software/programming/python/conda/#conda-channel","title":"Conda Channel","text":"<p>Conda Channel refers to a repository or collection of software packages that are available for installation using Conda. Conda Channels are used to organize and distribute packages, and they play a crucial role in the Conda ecosystem. Channels can be specified using the <code>--channel</code> or <code>-c</code> option with the conda install command i.e.  <code>conda install -c channel_name package_name</code>. In the above example, if you want to specify the channel name to install <code>matplotlib</code>, you need to use</p> <p><pre><code>(ENV) login-41 ~ &gt;: conda install -c conda-forge matplotlib\n</code></pre> This will install <code>matplotlib</code> from <code>conda-forge</code> channel which is a community-maintained collection of Conda packages where a wide range of packages contributed by the community are available.  Users can prioritize channels by listing them in a specific order, so that Conda searches channels in the order they are listed, installing the first version of a package that it finds. To list the channels, create a file <code>.condarc</code> in the <code>$HOME</code> directory and add the following</p> <p><pre><code>auto_activate_base: false\nchannels:\n  - conda-forge\n  - defaults\n</code></pre> The advantage of using <code>.condarc</code> is that you don't have to mention the channel name every time you install a package. However, please note that you still need to use the channel name if you want to install Python packages that require a specific channel other than the default channels (<code>conda-forge</code> and <code>defaults</code>).</p>"},{"location":"Software/programming/python/conda/#examples","title":"Examples","text":"<p>Here, we provide some examples of how to use <code>conda</code> to install application </p>"},{"location":"Software/programming/python/conda/#install-tensorflow-with-gpu","title":"Install TensorFlow with GPU","text":"<p>The following example will create a new conda environment based on Python 3.9 and install TensorFlow in the environment.</p> <pre><code>login1-41 ~ &gt;: module load Anaconda3\nlogin1-41 ~ &gt;: conda create --name tf python=3.9\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/g/guest24/.conda/envs/tf\n\n  added / updated specs:\n    - python=3.9\n\n\nThe following packages will be downloaded:\n\n &lt;output snipped&gt;\n\nProceed ([y]/n)?y\n\n &lt;output snipped&gt;\n#\n# To activate this environment, use\n#\n#     $ conda activate tf\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n</code></pre> <p>Activate the new 'tf' environment <pre><code>login1-41 ~ &gt;: source conda.sh\nlogin1-41 ~ &gt;: conda activate tf\n(tf) login-41 ~ &gt;:\n</code></pre> Install tensorflow-gpu <pre><code>(tf) node430-41 ~ &gt;: conda install -c anaconda tensorflow-gpu\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/g/guest24/miniconda3/envs/tf\n\n  added / updated specs:\n    - tensorflow-gpu\n\n&lt;output snipped&gt;\n\nThe following packages will be SUPERSEDED by a higher-priority channel:\n\n  ca-certificates                                 pkgs/main --&gt; anaconda\n  certifi                                         pkgs/main --&gt; anaconda\n  openssl                                         pkgs/main --&gt; anaconda\n\n\nProceed ([y]/n)?y\n\n&lt;output snipped&gt;\n\nmkl_fft-1.1.0        | 143 KB    | ####################################################################################### | 100%\nurllib3-1.25.9       | 98 KB     | ####################################################################################### | 100%\ncudatoolkit-10.1.243 | 513.2 MB  | ####################################################################################### | 100%\nprotobuf-3.12.3      | 711 KB    | ####################################################################################### | 100%\nblinker-1.4          | 21 KB     | ####################################################################################### | 100%\nrequests-2.24.0      | 54 KB     | ####################################################################################### | 100%\nwerkzeug-1.0.1       | 243 KB    | ####################################################################################### | 100%\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n</code></pre> Check to see if TensorFlow can be loaded <pre><code>(tf) login1-41 ~ &gt;: python\nPython 3.9.13 (main, Oct 13 2022, 21:15:33)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> Simple TensorFlow test program to make sure the virtual env can access a GPU. Program is called </p> tf.gpu.test.py <pre><code>import tensorflow as tf\n\nif tf.test.gpu_device_name():\n\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n\nelse:\n\n   print(\"Please install GPU version of TF\")\n</code></pre> Slurm script to submit the job WulverLochness <pre><code>#!/bin/bash -l\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Purge any module loaded by default\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load slurm, easybuild\nmodule load Anaconda3\nsource conda.sh\nconda activate tf\nsrun python tf.gpu.test.py\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=tf_test\n#SBATCH --output=%x.%j.out # %x.%j expands to JobName.JobID\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --partition=datasci\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n\n# Purge any module loaded by default\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load Anaconda3\nsource $HOME/conda3.sh\nconda activate tf\nsrun python tf.gpu.test.py\n</code></pre> <p>Result: <pre><code>Starting /home/g/guest24/.bash_profile ... standard AFS bash profile\n\nHome directory : /home/g/guest24 is not in AFS -- skipping quota check\n\nOn host node430 :\n         17:14:13 up 1 day,  1:17,  0 users,  load average: 0.01, 0.07, 0.06\n\n      Your Kerberos ticket and AFS token status \nklist: No credentials cache found (filename: /tmp/krb5cc_22967_HvCVvuvMMX)\nKerberos :\nAFS      :\n\nLoading default modules ...\nCreate file : \"/home/g/guest24/.modules\" to customize.\n\nNo modules loaded\n2020-07-29 17:14:19.047276: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2020-07-29 17:14:19.059941: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200070000 Hz\n2020-07-29 17:14:19.060093: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ea8ebfdb90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-07-29 17:14:19.060136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-07-29 17:14:19.061484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n\n&lt;ouput snipped&gt;\n\n2020-07-29 17:14:19.817386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n2020-07-29 17:14:19.817392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\n2020-07-29 17:14:19.817397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\n2020-07-29 17:14:19.819082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/device:GPU:0 with 15064 MB memory) -&gt; physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:02:00.0, compute capability: 6.0)\nDefault GPU Device: /device:GPU:0\n</code></pre> Next, deactivate the environment using <code>conda deactivate tf</code> command.</p>"},{"location":"Software/programming/python/conda/#install-pytorch-with-gpu","title":"Install PyTorch with GPU","text":"<p>To install PyTorch with GPU, load the <code>Anaconda3</code> module as described above and then use the following</p> <pre><code>conda create --name torch-cuda python=3.7\nsource conda.sh\nconda activate torch-cuda\nconda install -c \"nvidia/label/cuda-11.7.0\" cuda-toolkit\nconda install -c pytorch -c nvidia pytorch torchvision torchaudio pytorch-cuda=11.7\n</code></pre> <p>Note</p> <p>In the example above, we mentioned the channel name as we intend to install PyTorch and PyTorch-CUDA from a specific channel. For the default channel please see Channels.</p> <p>A simple PyTorch test program is given below to check whether PyTorch has been installed properly. Program is called</p> torch_tensor.py <pre><code># -*- coding: utf-8 -*-\n\nimport torch\nimport math\n\n\ndtype = torch.float\n#device = torch.device(\"cpu\")   # Uncomment this to run on CPU\ndevice = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n</code></pre> <p>User can use the following job script to run the script.</p> torch-cuda.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n#SBATCH --job-name=torch_test\n#SBATCH --output=%x.%j.out # %x.%j expands to JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Purge any module loaded by default\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load slurm, easybuild\nmodule load Anaconda3\nsource conda.sh\nconda activate torch-cuda\nsrun python touch_tensor.py\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=torch_test\n#SBATCH --output=%x.%j.out # %x.%j expands to JobName.JobID\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --partition=datasci\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n\n# Purge any module loaded by default\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load Anaconda3\nsource $HOME/conda3.sh\nconda activate torch-cuda\nsrun python touch_tensor.py\n</code></pre> <p>Warning</p> <p>When working with Python, it is generally advised to avoid mixing package management tools such as pip and conda within the same environment. Pip and Conda manage dependencies differently, and their conflict can lead to compatibility issues and unexpected behavior. Mixing the two can result in an environment where packages installed with one tool may not interact seamlessly with those installed using the other. </p>"},{"location":"Software/programming/python/conda/#mamba-the-conda-alternative","title":"Mamba: The Conda Alternative","text":"<p>Mamba is a fast, robust, and cross-platform package manager and particularly useful for building complicated environments, where <code>conda</code> is unable to 'solve' the required set of packages within a reasonable amount of time. Users can install packages with <code>mamba</code> in the same way as with <code>conda</code>. <pre><code>module load Mamba Anaconda3\n\n# create new environment\nmamba create --name env_name python numpy pandas \nsource conda.sh\n# install a new package into an existing environment\nconda activate env_name\nmamba install scipy\n</code></pre></p>"},{"location":"Software/programming/python/conda/#export-and-import-conda-environment","title":"Export and Import Conda Environment","text":"<p>Exporting and importing Conda environments allows users to capture and reproduce the exact set of dependencies for a project. With Conda, a popular package and environment management system, users can export an environment, including all installed packages, into a YAML file. This file can then be shared or version-controlled. Importing the environment from the YAML file on another system ensures consistent dependencies, making it easier to recreate the development or execution environment. </p> <p>Tips</p> <p>When installing Python packages via Conda, ensure that you perform the installation on the compute node rather than the login node. The CPU and memory resources on login nodes are limited, and installing Python packages on the login node can be time-consuming. To avoid this, initiate an tnteractive session with compute node.</p>"},{"location":"Software/programming/python/conda/#export-conda-environment","title":"Export Conda Environment","text":"<p>To export a conda environment to a new directory or a different machine, you need to activate the environment first that you intend to export. Please see Conda environment on how to activate the environment. Once your environment is activated, you can export it to a YAML file: <pre><code>conda env export &gt; my_environment.yml\n</code></pre> The YAML should look like this</p> <p><pre><code>name: my_env\nchannels:\n- defaults\ndependencies:\n- _libgcc_mutex=0.1=main\n- _openmp_mutex=5.1=1_gnu\n- blas=1.0=mkl\n\n&lt;ouput snipped&gt;\n\n#the last line is the path of the env\nprefix: /home/a/abc3/.conda/envs/my_env.\n</code></pre> Next, edit the <code>my_environment.yml</code> file to make sure it has the correct environment name and other settings. The last line of the file specifies the path of the environment.</p> <p>Once the YAML file is ready, you can transfer the <code>my_environment.yml</code> file to the new machine or directory where you want to replicate the environment. See cluster file transfer for details on transferring the files to clusters.</p>"},{"location":"Software/programming/python/conda/#import-environment-on-new-machine","title":"Import Environment on New Machine","text":"<p>On the new machine, first load Anaconda and initialize conda as before. Then, create the environment from the YAML file:</p> <p><pre><code>conda env create -f my_environment.yml\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n&lt;ouput snipped&gt;\n\nDownloading and Extracting Packages\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n# $ conda activate my_env\n#\n# To deactivate an active environment, use\n#\n# $ conda deactivate\n</code></pre> After running this command, Conda will set up the environment as it was on the original machine, including downloading and installing packages. To activate the New Environment use <code>conda activate my_env</code> where <code>my_env</code> is the environment name. You can check your current environments using <code>conda env list</code>.</p>"},{"location":"Software/programming/python/conda/#importing-to-a-different-location","title":"Importing to a Different Location","text":"<p>If you want to import the conda environment to a different location, use the <code>--prefix</code> or <code>-p</code> option <pre><code>conda env create -f my_environment.yml -p /project/hpcadmins/abc3/conda_env/my_env\n</code></pre> This will create the environment in the specified directory instead of the default conda environment directory. Please note that in that case, you need to provide the full path of the environment to activate it.</p> <p><pre><code>conda activate /project/hpcadmins/abc3/conda_env/my_env\n(/project/hpcadmins/abc3/conda_env/my_env) abc3@login01:~$ conda env list\n# conda environments:\n#\nbase /apps/easybuild/software/Anaconda3/2023.09-0\n* /project/hpcadmins/abc3/conda_env/my_env\n</code></pre> By following these steps, you can successfully export a conda environment from one machine and import it to another, ensuring a consistent working environment across different machines or directories.</p> <p>Warning</p> <p>It is advisable to use the <code>/project</code> directory to store the Conda environment rather than using the <code>$HOME</code> directory. On Wulver, the storage space on <code>$HOME</code> is limited (50G) and cannot be increased. See Wulver Filesystems for details. </p>"},{"location":"Software/programming/python/conda/#conda-user-commands","title":"Conda User Commands","text":"Task Command Activate environment: <code>conda activate [environment_name]</code> Deactivate environment: <code>conda deactivate [environment_name]</code> Show the list of environments: <code>conda env list</code> Delete environment: <code>conda remove [environment_name]</code> Export environment: <code>conda env export &gt; [environment_name].yml</code> Import environment from YAML: <code>conda env create -f [environment_name].yml</code> Import environment to different location: <code>conda env create -f [environment_name].yml -p [PATH]</code>"},{"location":"Software/programming/python/jupyter/","title":"Jupyter Notebooks","text":"<p>The Jupyter Notebook is a web-based interactive computing platform. The notebook combines live code, equations, narrative text, and visualizations. In our cluster, we have JupyterLab which is the next-generation user interface for Project Jupyter offering all the familiar building blocks of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and powerful user interface. </p>"},{"location":"Software/programming/python/jupyter/#availability","title":"Availability","text":"Lochness <p> Software Version Dependent Toolchain Module Load Command JupyterLab 3.1.6 foss/2021b <code>module load foss/2021b JupyterLab/3.1.6</code> </p>"},{"location":"Software/programming/python/jupyter/#using-jupyterlab","title":"Using JupyterLab","text":"Sample Batch Script to run Jupyter Notebook Lochness <pre><code>#!/bin/bash -l                                                                                                                                                                                                                          \n#SBATCH --job-name=jupyter_test                                                                                                                                                                                                         \n#SBATCH --output=%x.%j.out                                                                                                                                                                                                              \n#SBATCH --nodes=1                                                                                                                                                                                                                       \n#SBATCH --tasks-per-node=1\n#SBATCH --partition=datasci                                                                                                                                                                                                             \n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --time=0-01:00:00\n\n######################################                                                                                                                                                                                               \nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load GCCcore/11.2.0 JupyterLab\nport=$(shuf -i 6000-9999 -n 1)\n/usr/bin/ssh -N -f -R $port:localhost:$port login-1.tartan.njit.edu\n\ncat&lt;&lt;EOF\n\nJupyter server is running on: $(hostname)\nJob starts at: $(date)\nStep 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L $port:localhost:$port $USER@login-1.tartan.njit.edu\n\nStep 2: Connect to Jupyter\n\nKeep the terminal in the previouse step open. Now open browser, find the line with\n\nOr copy and paste one of these URLs:\n\nthe URL will be something like:\n\nhttp://localhost:${port}/?token=XXXXXXXX\n\nyou should be able to connect to Jupyter Notebook running remotely on a Lochness compute node with the above url\n\nEOF\n\njupyter notebook --no-browser --port $port --notebook-dir=$(pwd)\n</code></pre> <p>Once you submit this job script, you will see an output file indicating the port number that you need to use to connect to the HPC cluster in a new terminal window. Please follow further instructions from the output file.</p> <p>Warning</p> <p>Please note that JupyterLab or Jupyter Notebook is not available as a module on Wulver, as users can build those packages via Conda. Please see Conda Documentation for details. Jupyter Notebook can also be accessed on Wulver via Open OnDemand. We will provide the instructions soon.</p>"},{"location":"Software/programming/python/jupyter/#jupyter-notebook-on-wulver","title":"Jupyter Notebook on Wulver","text":"<p>As mentioned above, users need to install Jupyter Notebook on the Conda Environment. Once the Conda Environment is activated users can install Jupyter Notebook via <code>conda install -c conda-forge jupyter notebook</code> command. Here we provide a sample SLURM script on how to start Jupyter Notebook session on Wulver.</p> Sample Batch Script to run Jupyter Notebook Wulver <pre><code>#!/bin/bash -l                                                                                                                                                                                                                          \n#SBATCH --job-name=jupyter_test                                                                         \n#SBATCH --output=%x.%j.out                                                                                    \n#SBATCH --tasks-per-node=1\n#SBATCH --partition=gpu\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --qos=standard\n#SBATCH --time=71:59:59 # D-HH:MM:SS\n\n######################################                                                                                                                                                                                               \nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver\nmodule load Anaconda3\nsource conda.sh \nconda activate ENV # Replace the name of the environment with the environment you are using. For example, if your environment is torch-cuda then use cond to activate torch-cuda\n\nport=$(shuf -i 6000-9999 -n 1)\n/usr/bin/ssh -N -f -R $port:localhost:$port login01.tartan.njit.edu\n\ncat&lt;&lt;EOF\n\nJupyter server is running on: $(hostname)\nJob starts at: $(date)\nStep 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L $port:localhost:$port $USER@login01.tartan.njit.edu\n\nStep 2: Connect to Jupyter\n\nKeep the terminal in the previouse step open. Now open browser, find the line with\n\nOr copy and paste one of these URLs:\n\nthe URL will be something like:\n\nhttp://localhost:${port}/?token=XXXXXXXX\n\nyou should be able to connect to Jupyter Notebook running remotely on a Wulver compute node with the above url\n\nEOF\n\njupyter notebook --no-browser --port $port --notebook-dir=$(pwd)\n</code></pre> <p>Once you submit this job script, you will see an output file indicating the port number that you need to use to connect to Wulver in a new terminal window. Please follow further instructions from the output file.</p>"},{"location":"Software/programming/python/jupyter/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/slurm/slurm/","title":"SLURM","text":"<p>Slurm (Simple Linux Utility for Resource Management) is an open-source workload manager and job scheduler designed for high-performance computing clusters. It is widely used in research, academia, and industry to efficiently manage and allocate computing resources such as CPUs, GPUs, memory, and storage for running various types of jobs and tasks. Slurm helps optimize resource utilization, minimizes job conflicts, and provides a flexible framework for distributing workloads across a cluster of machines. It offers features like job prioritization, fair sharing of resources, job dependencies, and real-time monitoring, making it an essential tool for orchestrating complex computational workflows in diverse fields.</p>"},{"location":"Software/slurm/slurm/#availability","title":"Availability","text":"Software Module Load Command slurm <code>module load wulver</code> <p>Please note that the module <code>wulver</code> is already loaded when a user logs in to the cluster. If you use <code>module purge</code> command, make sure to use <code>module load wulver</code> in the slurm script to load SLURM.</p>"},{"location":"Software/slurm/slurm/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of SLURM is available at SLURM manual. </p>"},{"location":"Software/slurm/slurm/#managing-and-monitoring-jobs","title":"Managing and Monitoring Jobs","text":"<p>SLURM has numerous tools for monitoring jobs. Below are a few to get started. More documentation is available on the SLURM website.</p> <p>The most common commands are: </p> <ul> <li>List all current jobs: <code>squeue</code></li> <li>Job deletion: <code>scancel [job_id]</code></li> <li>Run a job: <code>sbatch [submit script]</code></li> <li>Run a command: <code>srun &lt;slurm options&gt; &lt;command name&gt;</code></li> </ul>"},{"location":"Software/slurm/slurm/#slurm-user-commands","title":"SLURM User Commands","text":"Task Command Job submission: <code>sbatch [script_file]</code> Job deletion: <code>scancel [job_id]</code> Job status by job: <code>squeue [job_id]</code> Job status by user: <code>squeue -u [user_name]</code> Job hold: <code>scontrol hold [job_id]</code> Job release: <code>scontrol release [job_id]</code> List enqueued jobs: <code>squeue</code> List nodes: <code>sinfo -N OR scontrol show nodes</code> Cluster status: <code>sinfo</code>"},{"location":"Software/slurm/slurm/#using-slurm-on-wulver","title":"Using SLURM on Wulver","text":"<p>In Wulver, SLURM submission will have new requirements, intended for a more fair sharing of resources without impinging on investor/owner rights to computational resources.  All jobs must now be charged to a PI-group (Principal Investigator) account.</p> <ol> <li>To specify the job, use <code>--account=PI_ucid</code>.  You can specify <code>--account</code> as either an <code>sbatch</code> or <code>#SBATCH</code> parameter. If you don't know the UCID of PI, use<code>sacctmgr show user $LOGNAME</code>, and you can find the PI's UCID under <code>Def Acct</code> column.</li> </ol> <p><pre><code>   [ab1234@login01 ~]$ sacctmgr show user $LOGNAME\n      User   Def Acct    Admin\n---------- ----------  ---------\n    ab1234     xy1234      None\n</code></pre> 2. Wulver has three partitions, differing in GPUs or RAM available:</p> <p> Partition Nodes Cores/Node CPU GPU Memory Service Unit (SU) Charge <code>--partition=general</code> 100 128 2.5G GHz AMD EPYC 7763 (2) NA 512 GB 1 SU per hour per cpu <code>--partition=debug</code> 1 4 2.5G GHz AMD EPYC 7763 (2) NA 512 GB No charges, must be used with <code>--qos=debug</code> <code>--partition=gpu</code> 25 128 2.0 GHz AMD EPYC 7713 (2) NVIDIA A100 GPUs (4) 512 GB 3 SU per hour per GPU node <code>--partition=bigmem</code> 2 128 2.5G GHz AMD EPYC 7763 (2) NA 2 TB 1.5 SU per CPU hour  3. Wulver has three levels of \u201cpriority\u201d, utilized under SLURM as Quality of Service (QoS):  QoS Purpose Rules Wall time limit, hours Valid Users <code>--qos=standard</code> Normal jobs, similar to Lochness \u201cpublic\u201d access SU charges based on node type (see partitions table above), jobs can be preempted by high QoS enqueued jobs 72 Everyone <code>--qos=low</code> Free access, no SU charge jobs can be preempted by high or standard QoS enqueued jobs 72 Everyone <code>--qos=high</code> Only available to owners/investors Highest Priority Jobs, no SU Charges. 72 owner/investor PI Groups <code>--qos=debug</code> Intended for debugging and testing jobs No SU Charges, maximum 4 CPUs are allowed, must be used with <code>--partition=debug</code> 8 Everyone  4. Check Quota Faculty PIs are allocated 300,000 Service Units (SU) per year upon request at no cost, which can be utilized via <code>--qos=standard</code> on the SLURM job. It's important to regularly check the usage of SUs so that users can be aware of their consumption and switch to <code>--qos=low</code> to prevent exhausting all allocated SUs. Users can check their quota using the <code>quota_info UCID</code> command.  <pre><code>[ab1234@login01 ~]$ module load wulver\n[ab1234@login01 ~]$ quota_info $LOGNAME\nUsage for account: xy1234\n   SLURM Service Units (CPU Hours): 277557 (300000 Quota)\n   PROJECT Storage: 867 GB (of 2048 GB quota)\n     User ab1234 Usage: 11 GB (No quota)\n   SCRATCH Storage: 791 GB (of 10240 GB quota)\n     User ab1234 Usage: 50 GB (No quota)\nHOME Storage ab1234 Usage: 0 GB (of 50 GB quota)\n</code></pre> Here, 'xy1234' represents the UCID of the PI, and 'SLURM Service Units (CPU Hours): 277557 (300000 Quota)' indicates that members of the PI group have already utilized 277,557 CPU hours out of the allocated 300,000 SUs. Please ensure that you load the 'wulver' module before running the 'quota_info' command. This command also displays the storage usage of directories such as $HOME, /project, and /scratch. Users can view both the group usage and individual usage of each storage. In the given example, the group usage from the 2TB project quota is 867 GB, with the user's usage being 11 GB out of that 867 GB. For more details file system quota, see Wulver Filesystem.</p>"},{"location":"Software/slurm/slurm/#example-of-slurm-script","title":"Example of slurm script","text":""},{"location":"Software/slurm/slurm/#submitting-jobs-on-cpu-nodes","title":"Submitting Jobs on CPU Nodes","text":"Sample Job Script to use: submit.sh <pre><code>    #!/bin/bash -l\n    #SBATCH --job-name=job_nme\n    #SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n    #SBATCH --error=%x.%j.err\n    #SBATCH --partition=general\n    #SBATCH --qos=standard\n    #SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n    #SBATCH --nodes=1\n    #SBATCH --ntasks-per-node=8\n    #SBATCH --time=59:00  # D-HH:MM:SS\n    #SBATCH --mem-per-cpu=4000M\n</code></pre> <ul> <li>Here, the job requests 1 node with 8 cores, on the <code>general</code> partition with <code>qos=standard</code>. Please note that the memory relies on the number of cores you are requesting. </li> <li>As per the policy, users can request up to 4GB memory per core, therefore the flag  <code>--mem-per-cpu</code> is used for memory requirement. </li> <li>In this above script <code>--time</code> indicates the wall time which is used to specify the maximum amount of time that a job is allowed to run. The maximum allowable wall time depends on SLURM QoS, which you can find in QoS. </li> <li>To submit the job, use <code>sbatch submit.sh</code> where the <code>submit.sh</code> is the job script. Once the job has been submitted, the jobs will be in the queue, which will be executed based on priority-based scheduling. </li> <li>To check the status of the job use <code>squeue -u $LOGNAME</code> and you should see the following  <pre><code>  JOBID PARTITION     NAME     USER  ST    TIME    NODES  NODELIST(REASON)\n   635   general     job_nme   ucid   R   00:02:19    1      n0088\n</code></pre> Here, the <code>ST</code> stands for the status of the job. You may see the status of the job <code>ST</code> as <code>PD</code> which means the job is pending and has not been assigned yet. The status change depends upon the number of users using the partition and resources requested in the job. Once the job starts, you will see the output file with an extension of <code>.out</code>. If the job causes any errors, you can check the details of the error in the file with the <code>.err</code> extension.</li> </ul>"},{"location":"Software/slurm/slurm/#submitting-jobs-on-gpu-nodes","title":"Submitting Jobs on GPU Nodes","text":"<p>In case of submitting the jobs on GPU, you can use the following SLURM script </p> Sample Job Script to use: gpu_submit.sh <pre><code>    #!/bin/bash -l\n    #SBATCH --job-name=gpu_job\n    #SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n    #SBATCH --error=%x.%j.err\n    #SBATCH --partition=gpu\n    #SBATCH --qos=standard\n    #SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n    #SBATCH --nodes=1\n    #SBATCH --ntasks-per-node=8\n    #SBATCH --gres=gpu:2\n    #SBATCH --time=59:00  # D-HH:MM:SS\n    #SBATCH --mem-per-cpu=4000M\n</code></pre> <p>This will request 2 GPUS per node on the <code>GPU</code> partition.</p>"},{"location":"Software/slurm/slurm/#submitting-jobs-on-debug","title":"Submitting Jobs on <code>debug</code>","text":"<p>The \"debug\" QoS in Slurm is intended for debugging and testing jobs. It usually provides a shorter queue wait time and quicker job turnaround. Jobs submitted with the \"debug\" QoS have access to a limited set of resources (Only 4 CPUS on Wulver), making it suitable for rapid testing and debugging of applications without tying up cluster resources for extended periods. </p> Sample Job Script to use: debug_submit.sh <pre><code>    #!/bin/bash -l\n    #SBATCH --job-name=debug\n    #SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n    #SBATCH --error=%x.%j.err\n    #SBATCH --partition=debug\n    #SBATCH --qos=debug\n    #SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n    #SBATCH --nodes=1\n    #SBATCH --ntasks-per-node=4\n    #SBATCH --time=7:59:00  # D-HH:MM:SS, Maximum allowable Wall Time 8 hours\n    #SBATCH --mem-per-cpu=4000M\n</code></pre>"},{"location":"Software/slurm/slurm/#interactive-session-on-a-compute-node","title":"Interactive session on a compute node","text":"<p>Interactive sessions are useful for tasks that require direct interaction with the compute node's resources and software environment. To start an interactive session on the compute node, use the following after logging into Wulver</p> CPU NodesGPU NodesDebug Nodes <pre><code>   srun -p general -n 1 --ntasks-per-node=8 --qos=standard --account=PI_ucid --mem-per-cpu=2G --time=59:00 --pty bash\n</code></pre> <pre><code>   srun -p gpu -n 1 --ntasks-per-node=8 --qos=standard --account=PI_ucid --mem-per-cpu=2G --gres=gpu:2 --time=59:00 --pty bash\n</code></pre> <pre><code>   srun -p debug -n 1 --ntasks-per-node=4 --qos=debug --account=PI_ucid --mem-per-cpu=2G --gres=gpu:2 --time=59:00 --pty bash\n</code></pre> <p>Replace <code>PI_ucid</code> with PI's NJIT UCID. </p> <p>Warning</p> <p>Login nodes are not designed for running computationally intensive jobs. You can use the head node to edit and manage your files, or to run small-scale interactive jobs. The CPU usage is limited per user on the head node. Therefore, for serious computing either submit the job using <code>sbatch</code> command or start an interactive session on the compute node.</p> <p>Note</p> <p>Please note that if you are using GPUs, check whether your script is parallelized. If your script is not parallelized and only depends on GPU, then you don't need to request more cores per node. In that case use <code>--ntasks-per-node=1</code>, as this will request 1 CPU per GPU. It's important to keep in mind that using multiple cores on GPU nodes may result in unnecessary CPU hour charges. Additionally, implementing this practice can make service unit accounting significantly easier.</p>"},{"location":"Software/slurm/slurm/#additional-resources","title":"Additional Resources","text":"<ul> <li>SLURM Tutorial List</li> </ul>"},{"location":"Software/utilities/apptainer/","title":"Apptainer","text":"<p>Apptainer (formerly Singularity) s a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Apptainer on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don\u2019t have to worry about how to install all the software you need on each different operating system.</p> <p>Apptainer was created to run complex applications on HPC clusters in a simple, portable, and reproducible way. First developed at Lawrence Berkeley National Laboratory, it quickly became popular at other HPC sites, academic sites, and beyond. Apptainer is an open-source project, with a friendly community of developers and users. The user base continues to expand, with Apptainer now used across industry and academia in many areas of work.</p> <p>Many container platforms are available, but Apptainer is focused on:</p> <ul> <li>Verifiable reproducibility and security, using cryptographic signatures, an immutable container image format, and in-memory decryption.</li> <li>Integration over isolation by default. Easily make use of GPUs, high-speed networks, parallel filesystems on a cluster or server by default.</li> <li>Mobility of computing. The single file SIF container format is easy to transport and share.</li> <li>A simple, effective security model. You are the same user inside a container as outside, and cannot gain additional privilege on the host system by default. Read more about Security in Apptainer.</li> </ul>"},{"location":"Software/utilities/apptainer/#use-cases","title":"Use Cases","text":"<ul> <li>BYOE: Bring Your Own Environment!</li> <li>Reproducible science</li> <li>Commercially supported code requiring a particular environment</li> <li>Static environments (software appliances)</li> <li>Legacy code on old operating systems</li> <li>Complicated software stacks that are very host specific</li> <li>Complicated work-flows that require custom installation and/or data</li> </ul>"},{"location":"Software/utilities/dmtcp/","title":"DMTCP","text":""},{"location":"Software/utilities/dmtcp/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command DMTCP 3.0.0 - <code>module load DMTCP/3.0.0</code> DMTCP 2.6.0 - <code>module load DMTCP/2.6.0</code> </p> <p> Software Version Dependent Toolchain Module Load Command DMTCP 2.6.0 - <code>module load DMTCP/2.6.0</code> </p>"},{"location":"Software/utilities/dmtcp/#related-applications","title":"Related Applications","text":""},{"location":"Software/utilities/dmtcp/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/utilities/parallel/","title":"GNU Parallel","text":"<p>GNU Parallel is a shell tool for executing jobs in parallel using one or more computers. A job can be a single command or a small script that has to be run for each of the lines in the input. The typical input is a list of files, a list of hosts, a list of users, a list of URLs, or a list of tables. A job can also be a command that reads from a pipe. GNU parallel can then split the input and pipe it into commands in parallel.</p> <p>Tip</p> <p>You can use GNU Parallel on your Mac OSX laptop/desktop to run applications in parallel. Just install using either brew or Macports</p> <pre><code>sudo port install parallel\nbrew install parallel\n</code></pre>"},{"location":"Software/utilities/parallel/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command </p> <p> Software Version Dependent Toolchain Module Load Command parallel 20200422 foss/2020a <code>module load foss/2020a parallel/20200422</code> </p>"},{"location":"Software/utilities/parallel/#usage","title":"Usage","text":"Usage <pre><code>module load parallel/20200422\nparallel echo ::: 1 2 3 ::: 4 5 6\n</code></pre>"},{"location":"Software/utilities/parallel/#purpose","title":"Purpose","text":"<p>GNU Parallel is a tool for running a series of jobs, mostly serial jobs, in parallel. This tool is best suited for running a bunch of serial jobs that may not run for the same simulation time. For example, the most easiest way to run a series of serial jobs in parallel on n cpus within one submit script is as follows</p> <pre><code>./job_1 &amp;\n./job_2 &amp;\n...\n./job_n &amp;\nwait\n./job_(n+1) &amp;\n./job_(n+2) &amp;\n...\n./job_2n &amp;\nwait\n./job_(2n+1) &amp;\n./job_(2n+2) &amp;\n...\n./job_3n &amp;\nwait\n</code></pre> <p>This works most efficiently when all jobs have the same or almost the same run time. If run times for jobs are unequal, then n jobs are run simultaneously and the cpus remain idle until all n jobs are completed before looping through the next n jobs. This will lead to idle time and inefficient consumption of cpu time.</p> <p></p> <p>GNU Parallel solves this issue by first launching n jobs. When one job completes, then the next job in sequence is started. This permits efficient use of cpu time by reducing the wait time and letting a number of small jobs to run while some cpus work on longer jobs.</p> <pre><code>parallel job_{1} ::: $(seq 1 3n)\n</code></pre> <p></p>"},{"location":"Software/utilities/parallel/#single-node-example","title":"Single Node example","text":""},{"location":"Software/utilities/parallel/#multi-node-example","title":"Multi Node example","text":""},{"location":"Software/utilities/parallel/#related-links","title":"Related Links","text":"<ul> <li>https://www.gnu.org/software/parallel/parallel_tutorial.html</li> <li>https://www.biostars.org/p/63816/</li> <li>http://www.shakthimaan.com/posts/2014/11/27/gnu-parallel/news.html</li> <li>https://www.msi.umn.edu/support/faq/how-can-i-use-gnu-parallel-run-lot-commands-parallel</li> <li>https://github.com/LangilleLab/microbiome_helper/wiki/Quick-Introduction-to-GNU-Parallel</li> <li>https://davetang.org/muse/2013/11/18/using-gnu-parallel/</li> <li>https://sites.google.com/a/stanford.edu/rcpedia/parallel-processing/gnu-parallel-examples</li> <li>http://phili.pe/posts/free-concurrency-with-gnu-parallel/</li> </ul>"},{"location":"Software/utilities/parallel/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/ase/","title":"ASE","text":"<p>ASE (Atomic Simulation Environment) is an open-source Python library for performing atomic-scale simulations of materials. It provides a collection of tools and interfaces for setting up, running, and analyzing simulations of atoms, molecules, and solids using a variety of simulation packages.</p> <p>ASE is designed to be flexible and modular, allowing users to easily switch between different simulation packages and methods without having to modify their code. It currently supports a number of popular simulation packages, including Quantum ESPRESSO, LAMMPS, and GROMACS etc.</p> <p>ASE provides a wide range of functionality for working with atomic-scale simulations, including geometry optimization, molecular dynamics simulations, electronic structure calculations, and vibrational analysis. It also includes a number of built-in tools for analyzing simulation output, such as calculating radial distribution functions, computing surface area and volume, and visualizing simulation trajectories.</p> <p>ASE is actively developed and maintained by a community of researchers and developers from around the world, and is available as a free download under an open-source license.</p>"},{"location":"Software/visualization/ase/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command </p> <p> Software Version Dependent Toolchain Module Load Command ASE 3.21.1-Python-3.8.2 foss/2020a <code>module load foss/2020a ASE/3.21.1-Python-3.8.2</code> </p>"},{"location":"Software/visualization/ase/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/ase/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/gnuplot/","title":"GNUPlot","text":""},{"location":"Software/visualization/gnuplot/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command gnuplot 5.4.2 foss/2021b <code>module load foss/2021b gnuplot/5.4.2</code> gnuplot 5.4.6 foss/2022b <code>module load foss/2022b gnuplot/5.4.6</code> </p> <p> Software Version Dependent Toolchain Module Load Command gnuplot 5.2.8 foss/2020a <code>module load foss/2020a gnuplot/5.2.8</code> gnuplot 5.4.2 foss/2021b <code>module load foss/2021b gnuplot/5.4.2</code> </p>"},{"location":"Software/visualization/gnuplot/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/gnuplot/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/ovito/","title":"OVITO","text":"<p>OVITO is a scientific visualization and analysis software tool designed for the analysis of atomistic simulation data. It is used primarily in the field of materials science and computational chemistry, and is particularly well-suited for the analysis of molecular dynamics simulations and other atomistic simulation techniques.</p> <p>OVITO provides a graphical user interface (GUI) that allows users to interactively visualize and analyze large datasets of atomistic simulation data. It supports a wide range of file formats commonly used in molecular dynamics simulations, including LAMMPS, GROMACS, and DL_POLY.</p> <p>In addition to its visualization capabilities, OVITO also provides a set of built-in analysis tools that allow users to compute various properties of the simulated systems. These include calculation of radial distribution functions, Voronoi tessellation, and common structural analysis metrics like bond order parameters and coordination numbers.</p>"},{"location":"Software/visualization/ovito/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command OVITO 3.7.11-basic foss/2021b <code>module load foss/2021b OVITO/3.7.11-basic</code> </p> <p> Software Version Dependent Toolchain Module Load Command OVITO 3.7.11-basic foss/2021b <code>module load foss/2021b OVITO/3.7.11-basic</code> </p>"},{"location":"Software/visualization/ovito/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/ovito/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/paraview/","title":"Paraview","text":"<p>ParaView is an open-source, cross-platform data visualization and analysis tool that allows users to create visualizations and analyze large datasets. It was developed to process and visualize scientific and engineering data, such as computational fluid dynamics (CFD) simulations, seismic data, medical imaging, and climate data.</p> <p>ParaView provides a graphical user interface (GUI) that enables users to interactively explore and visualize data. It supports a wide range of data formats and allows users to customize and manipulate visualizations to suit their needs. Additionally, ParaView provides a scripting interface that allows users to automate repetitive tasks and create custom analysis pipelines.</p> <p>ParaView is widely used in a variety of scientific and engineering fields, including aerospace, automotive, biomedical, energy, and geosciences, among others. It is actively developed and maintained by Kitware, a software company that specializes in open-source solutions for scientific computing and data analysis.</p>"},{"location":"Software/visualization/paraview/#availability","title":"Availability","text":"WulverLochness <p> Software Version Dependent Toolchain Module Load Command ParaView 5.11.2-egl - <code>module load ParaView/5.11.2-egl</code> ParaView 5.11.0-osmesa - <code>module load ParaView/5.11.0-osmesa</code> ParaView 5.11.0-egl - <code>module load ParaView/5.11.0-egl</code> ParaView 5.9.1-mpi foss/2021b <code>module load foss/2021b ParaView/5.9.1-mpi</code> ParaView 5.11.0-mpi foss/2022b <code>module load foss/2022b ParaView/5.11.0-mpi</code> </p> <p> Software Version Dependent Toolchain Module Load Command ParaView 5.8.0-Python-3.8.2-mpi foss/2020a <code>module load foss/2020a ParaView/5.8.0-Python-3.8.2-mpi</code> ParaView 5.9.1-mpi foss/2021b <code>module load foss/2021b ParaView/5.9.1-mpi</code> </p>"},{"location":"Software/visualization/paraview/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of ParaView is available at ParaView manual. To use ParaView on the cluster, users need to use the same version of ParaView on their local machine. You can download the ParaView from ParaView official download page</p>"},{"location":"Software/visualization/paraview/#using-paraview","title":"Using ParaView","text":"<p>ParaView supports GPU acceleration, which can significantly improve performance and reduce processing times for certain types of data and operations. GPU acceleration is particularly useful for large datasets with many points or cells, as well as for operations such as volume rendering and streamlines. You can use ParaView with GPU acceleration, but you need to use GPU nodes on our cluster. ParaView is also designed to work in parallel environments, and it supports the Message Passing Interface (MPI) standard for distributed computing. With MPI support, ParaView can be used to visualize and analyze large-scale datasets on clusters. </p> Sample Batch Script to Run ParaView with MPI support: pvserver_cpu.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n#SBATCH --job-name=pvserver_cpu\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load the slurm, easybuild \nmodule load ParaView/5.11.0-osmesa \n################################################\n#\n# Open an ssh tunnel to the login node\n#\n################################################\n# Run on random port\nport=$(shuf -i 6000-9999 -n 1)\nHOST=$(hostname)\nif [ $(hostname) == $HOST ]; then\n  /usr/bin/ssh -N -f -R $port:localhost:$port login01.tartan.njit.edu\nfi\n################################################\ncat&lt;&lt;EOF\n\npvserver is running on: $(hostname)\nJob starts at: $(date)\n\nStep 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L $port:localhost:$port $USER@login01.tartan.njit.edu\nEOF\n################################################\n# Run MPI pvserver\n#\n################################################\nhost_list=$(srun hostname -s | sort | uniq | paste -s -d, -)\nmpiexec -np $SLURM_NTASKS -rmk slurm -hosts $host_list pvserver --server-port=$port --force-offscreen-rendering\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=pvserver_cpu\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=public # Modify the partion name to PI's partition\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=0 # Adjust as necessary\n#SBATCH --time=24:00:00  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load ParaView/osmesa/5.9.1\n################################################\n#\n# Open an ssh tunnel to the login node\n#\n################################################\n# Run on random port\nport=$(shuf -i 6000-9999 -n 1)\nHOST=$(hostname)\nif [ $(hostname) == $HOST ]; then\n  /usr/bin/ssh -N -f -R $port:localhost:$port login-1.tartan.njit.edu\nfi\n################################################\ncat&lt;&lt;EOF\n\npvserver is running on: $(hostname)\nJob starts at: $(date)\n\nStep 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L $port:localhost:$port $USER@login-1.tartan.njit.edu\nEOF\n################################################\n# Run MPI pvserver\n#\n################################################\nmpiexec -rmk slurm pvserver --server-port=$port --force-offscreen-rendering\n</code></pre> <p>To use ParaView with GPU, you need to use the following job script</p> Sample Batch Script to Run ParaView with GPU support: pvserver_gpu.submit.sh WulverLochness <pre><code>#!/bin/bash -l\n#SBATCH --job-name=pvserver_gpu\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load ParaView/5.11.0-egl\n################################################\n#\n# Open an ssh tunnel to the login node\n#\n################################################\nport=$(shuf -i 6000-9999 -n 1)\nHOST=$(hostname)\nif [ $(hostname) == $HOST ]; then\n     /usr/bin/ssh -N -f -R $port:localhost:$port login01.tartan.njit.edu\nfi\n################################################\ncat&lt;&lt;EOF\n\npvserver is running on: $(hostname)\nJob starts at: $(date)\n\nStep 1: Create SSH tunnel\n\nOpen a new terminal window, and run:\n(If you are off campus you will need a VPN running)\n\nssh -L $port:localhost:$port $USER@login01.tartan.njit.edu\nEOF\n################################################\n#\n# Run MPI pvserver\n#\n################################################\n\nhost_list=$(srun hostname -s | sort | uniq | paste -s -d, -)\n\nmpiexec -np $SLURM_NTASKS -rmk slurm -hosts $host_list pvserver --server-port=$port --force-offscreen-rendering\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=pvserver_test\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=datasci\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=0 # Adjust as necessary\n#SBATCH --time=24:00:00  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load ParaView/egl/5.9.1\n################################################\n#\n# Open an ssh tunnel to the login node\n#\n################################################\nport=$(shuf -i 6000-9999 -n 1)\nHOST=$(hostname)\nif [ $(hostname) == $HOST ]; then\n     /usr/bin/ssh -N -f -R $port:localhost:$port login-1.tartan.njit.edu\nfi\n################################################\ncat&lt;&lt;EOF\n\npvserver is running on: $(hostname)\nJob starts at: $(date)\n\nStep 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L $port:localhost:$port $USER@login-1.tartan.njit.edu\nEOF\n################################################\n#\n# Run MPI pvserver\n#\n################################################\n\nmpiexec -rmk slurm pvserver --server-port=$port --force-offscreen-rendering --displays=0,1\n</code></pre> <p>Submit the job script using the sbatch command: <code>sbatch pvserver_gpu.submit.sh</code> or <code>sbatch pvserver_cpu.submit.sh</code>. Once you submit the job, please open the output file with <code>.out</code> extension, and get the port number from the output file. Once you open the output file (with <code>.out</code> extension) and go to the end of the file, you should see the following </p> WulverLochness <pre><code>Step 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L 1234:localhost:1234 user@login01.tartan.njit.edu\nWaiting for client...\nConnection URL: cs://n0003:1234\nAccepting connection(s): n0003:1234\n</code></pre> <pre><code>Step 1: Create SSH tunnel\n\nOpen a new terminal window, and run:\n(If you are off campus you will need a VPN running)\n\nssh -L 1234:localhost:1234 user@login-1.tartan.njit.edu\nWaiting for client...\nConnection URL: cs://node801:1234\nAccepting connection(s): node801:1234\n</code></pre> <p>Next, open a new terminal and type</p> WulverLochness <p><code>ssh -L $port:localhost:$port $USER@login01.tartan.njit.edu</code></p> <p><code>ssh -L $port:localhost:$port $USER@login-1.tartan.njit.edu</code></p> <p>where <code>$port</code> corresponds to the port number. Once you open ParaView from your local machine go to <code>File --&gt; Connnect</code>, and you will see a dialogue box with the name <code>Choose Server Configuration</code>. You need to select Add Server option and there you need to use the following as shown below.</p>    Your browser does not support the video tag.  <p>Make sure to use the same port number in Port option. Once you add the server, you need to select Connect to connect ParaView to the cluster.</p> <p>Note</p> <p>The port number may change every time you submit the job. In that case, you need to modify the port number by selecting the Edit Server option. The step to modify the server is shown in the above tutorial.</p>"},{"location":"Software/visualization/paraview/#related-applications","title":"Related Applications","text":"<ul> <li>Tecplot</li> </ul>"},{"location":"Software/visualization/paraview/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"about/","title":"ARCS Leadership","text":"Name Title Phone Email Office Alex Pacheco Associate CIO - Advanced Research Computing (973) 596-2672 alex.pacheco@njit.edu GITC 5401 Gedaliah Wolosh Director, High Performance Research Computing (973) 596-5436 gwolosh@njit.edu GITC 5203 Kate Cahill Associate Director, High Performance Research Computing (973) 596-2721 katharine.cahill@njit.edu GITC 2203 Kevin Walsh Assistant Director, Advanced Computing Infrastructure (973) 596-5747 walsh@njit.edu GITC 2202"},{"location":"about/#high-performance-research-computing","title":"High Performance Research Computing","text":"Name Title Phone Email Office Gedaliah Wolosh Director, High Performance Research Computing (973) 596-5436 gwolosh@njit.edu GITC 2203 Kevin Walsh Assistant Director, Advanced Computing Infrastructure (973) 596-5747 walsh@njit.edu GITC 2202 Kate Cahill Associate Director, High Performance Research Computing (973) 596-2721 katharine.cahill@njit.edu GITC 2203 Abhishek Mukherjee Research Computing Facilitator (973) 642-4132 abhishek.mukherjee@njit.edu GITC 2316B Hui(Julia) Zhao Research Computing Facilitator (973) 596-2727 hui.zhao@njit.edu GITC 2316B"},{"location":"about/#research-and-academic-technologies","title":"Research and Academic Technologies","text":"Name Title Phone Email Office Rick Gaine Research Technology System Administrator (973) 596-5441 rgaine@njit.edu GITC 2316B Charles Yan Research Technology Support Specialist (973) 596-2907 cyan@njit.edu GITC 2301"},{"location":"about/#alex-pacheco","title":"Alex Pacheco","text":"<p>Alex Pacheco joined NJIT on August 2, 2022 as the Associate CIO of Advanced Research Computing. Alex has over twenty years combined experience as an HPC user, user support consultant and Manager of HPC resources. He helped develop a strategic vision for a shared HPC resource and served as a co-PI on an NSF Campus Cyberinfrastructure award at Lehigh University.  </p>"},{"location":"about/#gedaliah-wolosh","title":"Gedaliah Wolosh","text":"<p>Dr. Wolosh has been at NJIT for over twenty years working in research computing. He has been the lead architect for all of the HPC resources at NJIT. He specializes in building scientific software stacks.  </p> <p>Schedule an appointment with Gedaliah </p>"},{"location":"about/#kate-cahill","title":"Kate Cahill","text":"<p>Kate started at NJIT in September 2023. Previously, she was the Education &amp; Training Specialist at the Ohio Supercomputer Center for 8 years, where she led the training programs for OSC as well as external education programs related to HPC and computational science for the XSEDE project as well as other grant-funded efforts.</p>"},{"location":"about/#kevin-walsh","title":"Kevin Walsh","text":""},{"location":"about/#abhishek-mukherjee","title":"Abhishek Mukherjee","text":"<p>Abhishek Mukherjee is a computational scientist and has experience in multidisciplinary projects, providing engineering solutions for computational fluid dynamics problems. He has expertise in HPC software installation and is an active contributor to EasyBuild that allows to manage software on HPC systems in an efficient way.  </p> <p>You can schedule appointments with Abhishek to consult on problems or questions you are encountering related to your work using the high-performance computing and big data resources at NJIT. Please, before making appointments with Abhishek Mukherjee, send your query to hpc@njit.edu, so that an incident number will be created which is required to schedule an appointment.</p> <p>Schedule an appointment directly on Abhishek's calendar from </p>"},{"location":"about/#huijulia-zhao","title":"Hui(Julia) Zhao","text":"<p>Hui(Julia) joined NJIT in December 2023, following twenty years in a similar role at Memorial Sloan Kettering Cancer Center. Julia's experience includes roles as a Bioinformatician, Unix System Administrator and High Performance Computing Engineer, with expertise in system and application configuration and user support. She helps user apply high-performance computing techniques to address diverse challenges in scientific computing. </p>"},{"location":"about/contact/","title":"Contact Us","text":"<p>Requests for assistance with HPC resources have been integrated into the campus-wide ServiceNow system. To have a service ticket created automatically and assigned to the HPC staff, email hpc@njit.edu. Before requesting assistance, we encourage you to take a look at the relevant documentation on this site. </p> <p>For prompt response, please consider the following when submitting a ticket. </p> <ul> <li> <p>Email Subject</p> <ul> <li>Very short summary of the issue as a subject, examples:<ul> <li>Lochness: Unable to submit jobs to public partition</li> <li>Lochness: Job error - no space on disk</li> <li>Lochness: Jobs are getting killed</li> </ul> </li> </ul> </li> <li> <p>Email Content</p> <ul> <li>JOB NUMBER</li> <li>System, Software and Partition where you are having an issue.</li> <li>Detailed description of your issue including modules loaded, and location of input, output and error files.</li> <li>If you have an error to report, paste the error message. Do not take a snapshot and upload the image.</li> <li>An image is not helpful when a long string of characters including directory path or file names needs to be copied.</li> <li>Anything else that you think may be necessary for us to understand and find a resolution to your issue.</li> </ul> </li> <li> <p>Software requests</p> <ul> <li>Submit a request for HPC Software Installation by visiting the Service Catalog </li> <li>Note: Users should consider installing software in their home directory especially Python and R packages. See Building your own for instructions on using conda or mamba. If you encounter issues, consider asking for help above rather than requesting a software install.</li> <li>You will need to enter the following information<ul> <li>Name of software and URL to download.</li> <li>If software is behind a paywall or account, please download the software, copy it over to lochness and provide location from where we can copy it for installation.</li> </ul> </li> </ul> </li> <li> <p>Account requests</p> <ul> <li>Requests for an account must come from faculty advisors by sending an email to hpc@njit.edu including the UCIDs for the accounts to be created.</li> <li>Students making requests should add their faculty advisor to the request. Requests will only be approved after confirmation by faculty advisor.</li> <li>To use HPC in courses, the course instructor should submit a request and include UCID's for the accounts to be created.</li> <li>Guest Accounts: Faculty can use Service Now to request Guest Access on HPC resources only for their external collaborators for a period of 1 year (can be renewed annually). </li> </ul> </li> <li> <p>Storage Requests</p> <ul> <li>Requests for storage must come from faculty advisors by sending an email to hpc@njit.edu. The request should include the amount of storage needed.</li> </ul> </li> <li> <p>Consultations</p> <ul> <li>During the migration, HPC leadership will host office hours for faculty and PIs. These can be virtual or in person at GITC 2200. Please sign up here.</li> <li>We also provide one-on-one consultations for HPC related issues. Schedule an appointment directly from . Please note that, when requesting assistance provide the information described above (where applicable), so we can most effectively assist you. Before requesting one-on-one consultations, please send you query to hpc@njit.edu, so that an incident number will be created which is required to schedule an appointment. </li> </ul> </li> </ul>"},{"location":"clusters/","title":"Current cluster","text":"<ul> <li>Wulver is NJIT's newest High Performance Cluster made available to users in Jan 2024.</li> </ul>"},{"location":"clusters/#old-cluster","title":"Old cluster","text":"<ul> <li>Lochness</li> </ul>"},{"location":"clusters/Wulver_filesystems/","title":"Wulver Filesystems","text":"<p>The Wulver environment is quite a bit like Lochness, but there are some key differences, especially in filesystems and SLURM partitions and priorities.</p> <p>Wulver Filesystems are deployed with more attention to PI ownership / group efforts:</p> <ol> <li>The <code>$HOME</code> directory is not intended for primary storage and has a 50GB quota. The main location for storing files is the group project directory which has 2TB of storage per PI group. To run the simulations, compilations, etc., users need to use a project directory which has 2TB of storage per PI group. Students can store their files under their corresponding PI\u2019s UCID in the <code>/project</code> directory.  For example, if PI\u2019s UCID is <code>doctorx</code>, then students need to use the <code>/project/doctorx/</code> directory. </li> <li>Users can also store temporary files under the <code>/scratch</code> directory, likewise under a PI-group directory. For example, PI\u2019s UCID is <code>doctorx</code>, so students need to use the <code>/scratch/doctorx/</code> directory.  Please note that the files under <code>/scratch</code> will be periodically deleted. To store files for longer than computations, please use the <code>/project</code> directory.  Files under <code>/scratch</code> are not backed up. For best performance simulations should be performed in the <code>/scratch</code> directory. Once the simulation is complete, the results should be copied into the <code>$HOME</code> or <code>/project</code> directory.  Files are deleted from <code>/scratch</code> after they are 30 days old.</li> <li>The <code>/research</code> and <code>$HOME</code> directory from Lochness will be mounted on Wulver as <code>/research</code> and <code>/oldhome</code> respectively. For details, see Migration.</li> </ol> Filesystem Purpose Characterics Backup policy Total Size Default Quota Deletion Policy Cost per TB per five years <code>/home</code> Non-research user files, such as profile, history, etc., files not intended for sharing.  Not for actual research files. Kalray Pixstor / GPFS (expensive) Daily ~1 PB 50GB per user (locked) One year after owner leaves NJIT Not possible to increase size, use <code>/project</code> or <code>/research</code> instead <code>/project</code> Active research by groups. Deployed as <code>/project/$PI_UCID/$LOGIN/</code> Kalray Pixstor / GPFS (expensive) Daily ~1 PB 2TB per group TBD $200 per TB for a duration of 5 years, minimum storage allocation of 5TB is required <code>/scratch</code> Temporary space for intermediate results, downloads, checkpoints, and such. MOVE YOUR RESULTS &amp; IMPORTANT FILES TO <code>/project</code> or <code>/research</code> Nvme  (very expensive) NEVER ~ 150 TB NONE Files deleted after 30 days or sooner if 80% full No charge, but files are automatically deleted <code>/tmp</code> Very high speed temporary storage Node-local SSD or NVME (very expensive) NEVER 1 TB per node shared by all users NONE Files deleted after job completion No charge, but files are automatically deleted <code>/research</code> Long term archive. Users can buy as much as they need. The users need to buy this space.  Existing purchases/quotas will be kept over from Lochness. NFS (inexpensive) Daily 8 TB Whatever the PI purchases. TBD TBD <code>/oldhome</code> Lochness <code>/home</code> is mounted as read-only on Wulver login nodes NFS CSO 8 TB NA All to be deleted one year after last login is migrated from Lochness to Wulver. NA"},{"location":"clusters/cluster_access/","title":"Access to NJIT Clusters","text":"<p>The following instructions are provided to access NJIT HPC clusters from a local computer.</p>"},{"location":"clusters/cluster_access/#getting-a-login","title":"Getting a Login","text":"<p>Faculty can obtain a login to NJIT's HPC by sending an email to hpc@njit.edu. Students can obtain a login either by taking a class that uses one of the systems or by asking their faculty adviser to contact on their behalf. Your login and password are the same as for any NJIT AFS system.</p>"},{"location":"clusters/cluster_access/#access-to-clusters","title":"Access to Clusters","text":"<p>Make sure the user is connected to <code>NJITsecure</code> if the user is on campus. If working off campus, NJIT VPN is required. Please find the details here. Here we will provide instructions for connecting to NJIT HPC on Mac/Linux and Windows OS.</p> Mac/LinuxWindows <p>Open terminal from Launchpad and select terminal. Type the following in the terminal by substituting <code>wulver</code> or <code>lochness</code> for <code>HPC_HOST</code> and <code>ucid</code> with your NJIT UCID. <pre><code>  localhost&gt; ssh -X -Y ucid@HPC_HOST.njit.edu  \n</code></pre> If you don\u2019t yet have a public SSH key for your local machine, you need to initialize one. The process of doing so differs across operating systems. The Linux and Mac system users simply need to run the command <code>ssh-keygen</code>, which will store the keys in the <code>~/.ssh</code> folder.</p> <p>Users will be prompted for your password. Enter your NJIT UCID password. Users can omit the <code>-X -Y</code> if you are not using a graphic interface. Once the password is provided, the user will see the following</p> <pre><code>The authenticity of host 'user@lochness.njit.edu' cannot be established.\n DSA key fingerprint is 01:23:45:67:89:ab:cd:ef:ff:fe:dc:ba:98:76:54:32:10. \n Are you sure you want to continue connecting (yes/no)?\n</code></pre> <p>Answering <code>yes</code> to the prompt will cause the session to continue. Once the host key has been stored in the known_hosts file, the client system can connect directly to that server again without the need for any approvals. </p> <p>Download the MobaXterm from this link.  Open MobaXterm after installation is completed.  Select Start local terminal to open the terminal.</p> <p></p> <p>Type <code>ssh ucid@HPC_HOST.njit.edu</code>. Replace <code>ucid</code> with your NJIT UCID and substitute <code>wulver</code> or <code>lochness</code> for <code>HPC_HOST</code>.</p> <p>. </p> <p>User will be prompted to type the password. The password is NJIT UCID password. After successful login, user will see the following in the terminal, which means the user is now connected to HPC.</p> <pre><code>Last login: Tue Oct 25 12:37:31 2022 from 10.192.228.138\nStarting /home/a/user/.bash_profile ... standard AFS bash profile\n\n========================\nHome directory : /home/u/user is not in AFS -- skipping quota check\n========================\n\nOn host login-1 :\n         12:38:05 up 315 days, 22:36, 35 users,  load average: 10.32, 10.10, 10.63\n\n=== === === Your Kerberos ticket and AFS token status === === ===\nKerberos :  Renew until 10/26/22 12:38:01, Flags: FRIA Renew until 10/26/22 12:38:01, Flags: FRA\nAFS      :  User's (AFS ID 105631) tokens for afs@cad.njit.edu [Expires Oct 25 22:38]\n\nRunning your /home/u/user/.modules file:\n\nTo see your aliases, enter \"alias\"\n\nlogin-1-41 ~ &gt;:\n</code></pre>"},{"location":"clusters/cluster_access/#key-based-authentication-to-the-njit-cluster","title":"Key based Authentication to the NJIT Cluster","text":"Mac/LinuxWindows <p>To access to NJIT cluster without the password, you need to have a public ssh key on your Mac or Linux system. If you don\u2019t yet have a public SSH key for your local machine, you need to initialize one. The process of doing so differs across operating systems. The Linux and Mac system users simply need to run the command <code>ssh-keygen</code>, which will store the keys in the <code>~/.ssh</code> folder. The public key is typically <code>id_rsa.pub</code> which is located in <code>~/.ssh/</code>. Once you have a public SSH key, copy it to the set of authorized keys on the computing cluster. Since you\u2019ve already connected to the cluster in the previous step, simply navigate to the <code>/.ssh</code> folder on the computing cluster and open the file <code>~/.ssh/authorized_keys</code> and paste the content of your public key from your local machine. Double-check that the pasted key begins with <code>ssh-rsa</code>.</p> <p>Windows users can save the public key through MobaXterm settings.</p>"},{"location":"clusters/cluster_access/#transfer-the-data-from-the-local-machine-to-clusters-or-vice-versa","title":"Transfer the Data from the Local Machine to Clusters or vice versa","text":"Mac/LinuxWindows <p>Users need to use the command in the terminal to transfer in and out the data </p> <p><pre><code>rsync -avzP /path/to/local/machine ucid@HPC_HOST.njit.edu:/path/to/destination\n</code></pre> Replace <code>HPC_HOST</code> with <code>lochness</code> or <code>wulver</code>. This will transfer the data from the ocal machine to HPC cluster.  To transfer the data from HPC cluster to local machine use</p> <pre><code>rsync -avzP ucid@HPC_HOST.njit.edu:/path/to/source /path/to/local/machine\n</code></pre> <p>User needs to select the <code>follow terminal folder</code> of the left pane of the MobaXterm terminal. </p> <p></p> <p>Next, to transfer the data from the local machine to Lochness user needs to select the <code>Upload to current folder</code> option, as shown below. Selecting this option will open a dialogue box where user needs to select the files to upload.</p> <p></p> <p>For transferring the data from Lochness to the local machine user needs to select the directory or the data from the left pane and then select <code>Download selected files</code>.</p>"},{"location":"clusters/get_started_on_Wulver/","title":"Access to Wulver","text":"<p>Wulver is a new cluster which will be available by the end of 2023. To see the specifications of Wulver cluster, please see the Wulver Documentation. If you already have access to existing clusters such as Lochness, you can log in to Wulver using <code>ssh ucid@wulver.njit.edu</code>. Replace <code>ucid</code> with your UCID. If you don't have prior access to NJIT cluster, you need to request for access. Faculty can obtain a login to NJIT's HPC &amp; BD systems by sending an email to hpc@njit.edu. Students can obtain a login either by taking a class that uses one of the systems or by asking their faculty adviser to contact on their behalf. Your login and password are the same as for any NJIT AFS system.</p>"},{"location":"clusters/get_started_on_Wulver/#login-to-wulver","title":"Login to Wulver","text":"<p>Please see the documentation on cluster access for details.</p>"},{"location":"clusters/get_started_on_Wulver/#general-linux-commands","title":"General Linux Commands","text":"<p>Since the operating system (OS) HPC cluster is linux based RHEL 8, users need to know linux to use the cluster. In the table below, you can find the basic linux commands required to use the cluster. For more details on linux commands, please see the Linux commands cheat sheets.</p> Linux Commands Description <code>cd [directory]</code> Change directory <code>cd ..</code> Change to one directory up <code>mkdir [directory]</code> create a directory <code>mkdir -p [directory]</code> create directories as necessary, if the directories exist, no error is specified <code>pwd</code> Print current working directory <code>ls</code> lists directory contents of files and directories <code>ls -ltra</code> all the files in long format with the newest one at the bottom <code>cp /path/to/source/file1 /path/to/destination/file2</code> Copy files from source to destination <code>cp \u2013r /path/to/source/dir1 /path/to/destination/dir2</code> Copy directory from source to destination <code>mv /source/file1 /source/file2</code> move directories or files  and rename them <code>cat filename1</code> display the content of text files <code>rm -rf /path/to/dir</code> used to delete files and directories"},{"location":"clusters/get_started_on_Wulver/#wulver-filesystems","title":"Wulver Filesystems","text":"<p>See Wulver Filesystems for details.</p>"},{"location":"clusters/get_started_on_Wulver/#software-availability","title":"Software Availability","text":"<p>The software can be loaded via <code>module load</code> command. You see the following modules are loaded once you log in to the Wulver (Use the <code>module li</code>) command to see the modules.  <pre><code>   1) easybuild   2) slurm/wulver   3) null\n</code></pre></p>"},{"location":"clusters/get_started_on_Wulver/#slurm-configuration","title":"Slurm Configuration","text":"<p>Please see SLURM for details in the slurm configuration.  </p>"},{"location":"clusters/lochness/","title":"Lochness","text":"<p>This very heterogeneous cluster is a mix of manufacturers, components, and capacities as it was built up in incremental purchases spanning several years. </p> <p>Much of lochness will be incorporated into the new Wulver cluster 1Q 2024.</p>"},{"location":"clusters/lochness/#specifications","title":"Specifications:","text":"Current Partitions Number of Nodes Processor Family Cores Per Node Total Cores Number of GPU per node Total GPUs Model of GPU bader 1 Sky Lake 20.0 20 4.0 4 Titan V cld 11 Sandy Bridge 20.0 220 0.0 9 cld-gpu 2 Sandy Bridge 20.0 40 2.0 4 K20Xm datasci 7 Sandy Bridge 20.0 140 2.0 14 P100 datasci 1 Sky Lake 20.0 20 4.0 4 Titan RTX datasci3 1 Sky Lake 20.0 20 4.0 4 Titan RTX datasci4 1 Sky Lake 20.0 20 4.0 4 Titan RTX davidsw 1 Cascade Lake 32.0 32 2.0 2 A100 ddlab 1 Sandy Bridge 20.0 20 0.0 0 ddlab 1 Cascade Lake 20.0 20 2.0 2 V100 esratoy 2 Cascade Lake 32.0 64 0.0 0 fahmadpo 14 Cascade Lake 64.0 896 0.0 0 fahmadpo-gpu 1 Cascade Lake 24.0 24 4.0 4 Titan RTX gor 28 Broadwell 20.0 560 0.0 0 gperry 3 Cascade Lake 36.0 108 0.0 0 hrjin 1 Sky Lake 32.0 32 4.0 3 Titan XP jyoung 3 Cascade Lake 96.0 288 0.0 0 phan 1 Cascade Lake 20.0 20 4.0 4 1 Titan X,3 Ge Force project 1 Cascade Lake 32.0 32 0.0 0 public 37 Cascade Lake 32.0 1184 0.0 0 samaneh 16 Cascade Lake 40.0 640 0.0 0 shakib 20 Cascade Lake 24.0 480 0.0 0 singhp 1 Ice Lake 56.0 56 0.0 0 smarras 4 Cascade Lake 48.0 192 0.0 0 smarras 2 Cascade Lake 48.0 96 2.0 2 A100 solarlab 1 Cascade Lake 48.0 48 2.0 2 A100 solarlab 4 Cascade Lake 48.0 192 0.0 0 solarlab 1 Ice Lake 48.0 48 1.0 1 A100 solarlab 2 Ice Lake 48.0 96 0.0 0 solarlab 1 Ice Lake 48.0 48 0.0 0 xt3 11 Cascade Lake 32.0 352 0.0 0 xye 1 Cascade Lake 16.0 16 5.0 5 3 types of GPUs zhiwei 1 Ice Lake 56.0 56 4.0 4 A40 183 6080 72 <ul> <li>All nodes have:<ul> <li>Ethernet network interface (1GigE or 10GigE)</li> <li>Infiniband network interface (mix of HDR100, EDR, and FDR speeds)</li> <li>1TB local storage (mostly SSD but a few HD)</li> </ul> </li> <li>All nodes have network accessible storage:<ul> <li><code>/home/</code>: 26 TB</li> <li><code>/research/</code>: 97 TB</li> <li><code>/afs/cad/</code>: 50 TB </li> </ul> </li> </ul> <p>The cluster also features</p> <ul> <li>\"CentOS Linux 7 (Core)\" operating system</li> <li>Virtualized login and control nodes</li> <li>SLURM job scheduler</li> <li>Warewulf stateless node provisioning</li> <li>Managed entirely by NJIT personnel</li> </ul> <p>Roughly half under warranty; rest are time and materials</p>"},{"location":"clusters/wulver/","title":"Wulver","text":"<p>Wulver is NJIT's newest cluster, which went into production in early 2024. To get started with Wulver, please check the details in get started on Wulver</p>"},{"location":"clusters/wulver/#specifications","title":"Specifications:","text":"Regular Bigmem GPU Partition regular bigmem gpu Nodes 100 2 25 CPU Type AMD EPYC 7753 AMD EPYC 7753 AMD EPYC 7713 CPU Speed (GHZ) 2.45 2.45 2.0 CPUs/Socket 64 64 64 Sockets/Node 2 2 2 RAM/Node (GB) 512 2048 512 GPU Type - - nVIDIA A100 GPUs/Node - - 4 GPU Memory (GB) - - 80 Total CPUs 12800 256 3200 Total RAM (TB) 50 4 12.5 Total SUs/year 112128000 2242560 28032000 Peak Performance (TFLOPs) 501.76 10.035 1072.4 HPL (TFLOPs/Node) - - - <ul> <li>100 GB High throughput/low latency Infiniband network</li> <li>Arcastream Storage<ul> <li>Parallel filesystem allowing multiple read/write operations</li> <li>Max performance 20 GB/s</li> <li>Hierarchical storage; NVME scratch, SAS, Cloud</li> <li>User transparent transfer from high-speed to medium to archive storage</li> <li>1 PB total storage</li> </ul> </li> <li>Virtualized Support and login nodes</li> <li>Management and deployment overseen by NJIT:<ul> <li>Nvidia Bright Cluster Manager</li> <li>X-ISS system administration</li> <li>SLURM scheduler with full support</li> <li>InfiniBand support via Dell or directly from Mellanox</li> <li>Will be deployed in the Databank data center in Piscataway</li> </ul> </li> </ul>"},{"location":"clusters/decommissioned/stheno/","title":"Stheno","text":"<p>Stheno was decommissioned on October 6<sup>th</sup>, 2023. Stheno users can access their data from Lochness when Lochness is returned to service.</p>"},{"location":"clusters/decommissioned/stheno/#specifications","title":"Specifications","text":"<ul> <li>GPU nodes: 2</li> <li>Total GPUs: 4</li> <li>GPU models:</li> <li>2x  node(s) w/ 2x K20m GPU(s)</li> <li>Total Nodes: 22</li> <li>Total Cores: 1224</li> <li>Total GB RAM: 2481667</li> <li>Nodes:  GB:<ul> <li>15x     128</li> <li>7x      96</li> </ul> </li> <li>Processor models:<ul> <li>15x Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz</li> <li>7x Intel(R) Xeon(R) CPU E5649  @ 2.53GHz</li> </ul> </li> </ul>"},{"location":"facilities/facilities/","title":"Introduction","text":"<p>As New Jersey\u2019s Science and Technology University, New Jersey Institute of Technology (NJIT) has developed a local cyberinfrastructure well positioned to allow NJIT faculty and students to collaborate at local, national, and global levels on many issues at the forefront of science and engineering research.</p>"},{"location":"facilities/facilities/#cyberinfrastructure","title":"Cyberinfrastructure","text":"<p>NJIT\u2019s Information Services and Technology (IST) resources provide members of the university community with universal access to a wealth of resources and services available over the NJIT network. NJIT's multi-100 gigabit backbone and multi-gigabit user network provides access to classrooms, laboratories, residence halls, faculty and staff offices, the library, student organization offices and others. 50% of these locations are provided with speeds of 5Gb/s or more. The campus wireless network 2,900+ access points blanket the university\u2019s public areas, classrooms, offices, collaboration spaces and outdoor areas. Over 60% of the Wi-Fi network is Wi-fi 6 capable, enabling NJIT\u2019s mobile users connectivity with multiple devices at increasing speeds. NJIT\u2019s connectivity to NJEdge, NJ\u2019s state-wide higher education network, provides access to the Internet and Internet 2. Students have the opportunity to work closely with faculty and researchers as new families of advanced applications are developed for an increasingly networked and information-based society. NJIT is also directly connected to cloud service providers such as AWS (Amazon Web Services) to provide low latency high speed access to cloud resources. A redundant diverse 100Gb network connection is being provisioned to support NJIT\u2019s new HPC co-location facility in Piscataway NJ.</p>"},{"location":"facilities/facilities/#compute-resources","title":"Compute Resources","text":"<p>NJIT\u2019s Advanced Research Computing Services (ARCS) group presently maintains two HPC clusters, a 224 node heterogenous condominium cluster Lochness. In addition, multiple IST teams are working on deploying Wulver, a 127 node heterogeneous condominium cluster at the new HPC co-location facility, Databank in Piscataway NJ, expected to be available for general use by the end of 2023.</p> Category Lochness Wulver Total Nodes 224 127 Total Cores 5600 16256 Total RAM / GB 65291 68096 Total GPUs 64 100 Total GPU RAM / GB 888 8000 Access General General Network (Infiniband; Ethernet) mix of HDR100, EDR, and FDR HDR100 and 10Gig Theoretical Peak Performance (TFLOPs) 514.79 1584.19"},{"location":"facilities/facilities/#storage-resources","title":"Storage Resources","text":"<p>Storage on Wulver will be provided by a 1PB arcastream high-performance storage that combines flash, disk, tape, and cloud storage into a unified single namespace architecture. Data moves seamlessly through various tiers of storage - from fast flash to cost-effective, high capacity object storage, all the way out to the cloud.</p> <p>The ARCS team provides 50TB of research and academic storage via the Andrew File System (AFS) distributed file system. AFS is backed up daily via IST\u2019s enterprise backup system. The current AFS implementation, OpenAFS, which is open source, will be replaced with a commercial implementation, AuriStor, during the 2022-2023 academic year, providing important enhancements in performance, security,capacities, authorization, permissions, and administration as well as bug fixes and technical support.</p>"},{"location":"facilities/facilities/#afs-storage","title":"AFS Storage","text":"<p>AFS is a distributed file system. Its principal components are :</p> <p>Database servers : provide information on authorization, and directory and file locations on file servers File servers : store all data in discrete \"volumes\", with associated quotas Client software : connects AFS clients to database servers and file servers over a network. Every client has access to every file in AFS, subject to the permissions attached to the identity of the user logged into the client. Client software is available for Linux, MacOS, and Windows. Single global name space for all clients : all clients see the identical path names and permissions.</p> <p>An AFS \"cell\" is a group of database servers and file servers sharing the same cell name.</p> <p>AFS was designed for highly-distributed wide area network (WAN) use. A cell can be concentrated in one physical location, or be widely geographically dispersed. A client in one cell can be given fine-grained levels of access to the data in other cells.</p> <p>The NJIT cell name is \"cad.njit.edu\"; all file and directory paths begin with <code>/afs/cad.njit.edu/</code>(abbreviated to <code>/afs/cad/</code>). This cell currently contains about 27TB of research data, 4TB of user data,and 1.4TB of applications data, in about 47,700 volumes.</p> <p>The current AFS implementation, OpenAFS, which is open source, will be replaced with a commercial implementation during the 2022-2023 academic year, providing important enhancements in performance, security,capacities, authorization, permissions, and administration as well as bug fixes and technical support.</p> <p>All of <code>/afs/cad/</code> is backed up daily via IST enterprise backup.</p>"},{"location":"facilities/facilities/#cloud-computing","title":"Cloud Computing","text":"<p>Access to Cloud Computing is provided via Rescale, a cloud computing platform combining scientific software with high performance computing. Rescale takes advantage of commercial cloud computing vendors such as AWS, Azure and Google Cloud to provide compute cycles as well as storage. The Rescale services also include applications setup, and billing and provides a pay-as-you-go method for researchers to use commercial cloud services - e.g., Amazon Web Services, Azure, Google Cloud Platform.</p>"},{"location":"facilities/facilities/#data-center-space-power-and-cooling","title":"Data Center: Space, Power and Cooling","text":"<p>NJIT\u2019s recent purchase of the Wulver cluster exceeds the capacity of NJIT\u2019s current computing facilities in terms of power and cooling. To accommodate Wulver and future expansion, NJIT has partnered with Databank, a leader in colocation facilities. Databank has more than 65 datacenters in over 27 metropolitan areas, supporting many industries including very large HPC deployments. The Databank location in Piscataway NJ will provide NJIT with 100% uptime SLA due to redundant power, cooling, and network facilities. The facility also provides water-cooling instead of traditional air-conditioned cooling in order to support far denser equipment needed for modern HPC.</p>"},{"location":"facilities/facilities/#services","title":"Services","text":"<p>NJIT employs nine FTE staff members to administer and support research computing resources. Services available to the user community include system design, installation, and administration of research computing resources, application support, assistance with software purchasing and consulting services to faculty members, their research associates, and students.</p>"},{"location":"migration/","title":"Important Announcement: HPC Cluster Migration from Lochness to Wulver","text":"<p>We are excited to share important news about the upcoming migration of users from the old HPC cluster, Lochness, to the new and improved cluster, Wulver. This migration is scheduled to commence on 1/16/2024.</p>"},{"location":"migration/#migration-details","title":"Migration Details:","text":"<ul> <li>Start Date: January 16, 2024</li> <li>Priority: Migration will be carried out in PI groups, with PIs who own nodes in Lochness being migrated first.</li> <li>Communication: PIs will receive prior communication from our team to discuss specific details tailored to their groups.</li> <li>Lochness Complete Shutdown: At the conclusion of the migration, Lochness will undergo a complete shutdown to facilitate its merger with Wulver. You will be advised of any necessary preparations needed on your end for a seamless transition.</li> <li>Restrictions on Lochness: As of the migration start date, no new user accounts or software installations will be permitted on Lochness. We appreciate your cooperation in adhering to these restrictions to ensure a smooth migration process.</li> <li>HPC Usage in Spring Courses: For coursework during the Spring semester, HPC usage will be exclusively on Wulver. Faculty members planning to integrate Wulver into their Spring courses are encouraged to contact us for testing and any necessary support. This proactive approach will help ensure a successful experience for both faculty and students.</li> </ul>"},{"location":"migration/#action-required","title":"Action Required:","text":"<p>PIs: Expect communication from our team before the migration date, providing essential details and discussing the migration plan tailored to your group's needs. We will need a list of your current students and software in use by your group.</p>"},{"location":"migration/#important-note-regarding-documentation","title":"Important Note Regarding Documentation:","text":"<p>To facilitate a smooth transition, please refer to FAQs, and other resources specific to the migration process.</p>"},{"location":"migration/#benefits-of-migration","title":"Benefits of Migration:","text":"<p>Wulver is equipped with enhanced capabilities and improved performance, ensuring a more efficient and streamlined high-performance computing experience for all users.</p> <p>If you have immediate concerns or questions, please feel free to reach out to the group via Contact US.</p>"},{"location":"migration/Lochness_node_owners/","title":"Lochness Node Owners Information","text":""},{"location":"migration/Lochness_node_owners/#overview","title":"Overview","text":"<p>As a node owner on the Lochness high-performance computing (HPC) cluster, we would like to provide you with important information regarding the upcoming migration to the new cluster, Wulver.</p>"},{"location":"migration/Lochness_node_owners/#node-migration-to-wulver","title":"Node Migration to Wulver","text":""},{"location":"migration/Lochness_node_owners/#privately-owned-nodes","title":"Privately Owned Nodes","text":"<p>Most privately owned nodes on Lochness will be migrated to Wulver as part of the cluster upgrade. Owners of these nodes will be considered \"condo investors\" and will receive higher priority access on an equivalent amount of resources for a period of time to be determined. This ensures that your investment in HPC resources continues to be recognized and prioritized during the transition.</p>"},{"location":"migration/Lochness_node_owners/#decommissioning-of-out-of-warranty-nodes","title":"Decommissioning of Out-of-Warranty Nodes","text":"<p>Nodes that are out of warranty and deemed too old to become part of Wulver's infrastructure will be decommissioned. We will contact owners of such nodes for further discussions and potential considerations.</p>"},{"location":"migration/Lochness_node_owners/#node-exclusions-from-wulver-migration","title":"Node Exclusions from Wulver Migration","text":""},{"location":"migration/Lochness_node_owners/#nodes-with-consumer-grade-gpus","title":"Nodes with Consumer Grade GPUs","text":"<p>Nodes equipped with consumer-grade GPUs, such as Titan, RTX, etc., will not be migrated to Wulver. Instead, these nodes will be relocated to the GITC 4320 datacenter and will be individually managed by ARCS.</p> <p>This decision is made to ensure the optimal performance and compatibility of the Wulver cluster while providing an alternative solution for nodes with specialized hardware configurations.</p>"},{"location":"migration/Lochness_node_owners/#action-required","title":"Action Required","text":"<ol> <li>Privately Owned Node Owners: Await further communication regarding the migration process and priority access details.</li> <li>Owners of Out-of-Warranty Nodes: Await further communication for discussions on potential considerations and decommissioning.</li> <li>Owners of Nodes with Consumer Grade GPUs: Expect your nodes to be relocated to the GITC 4320 datacenter, managed individually by ARCS.</li> </ol>"},{"location":"migration/Lochness_node_owners/#timeline","title":"Timeline","text":"<p>The migration timeline and the duration of higher priority access for condo investors will be communicated to you in advance.</p>"},{"location":"migration/faqs/","title":"Faqs","text":""},{"location":"migration/faqs/#the-following-are-questions-we-have-received-over-the-last-several-weeks-regarding-the-migration-from-lochness-to-wulver","title":"The following are questions we have received over the last several weeks regarding the migration from Lochness to Wulver.","text":""},{"location":"migration/faqs/#what-is-the-timeline-for-the-migration-from-lochness-to-wulver","title":"What is the timeline for the migration from Lochness to Wulver?","text":"Answer <p>We anticipate the migration process to be complete by the end of February 2024. The migration will commence on January 16, 2024, and our team is dedicated to ensuring a smooth transition for all users. We will keep you informed about any updates or changes to the timeline as the migration progresses. Your cooperation and understanding during this period are greatly appreciated. If you have any specific concerns about the timeline, please feel free to reach out to our support team for further clarification.</p>"},{"location":"migration/faqs/#how-will-the-migration-impact-my-existing-workflows-and-computations","title":"How will the migration impact my existing workflows and computations?","text":"Answer <p>The research facilitation team is committed to assisting you during the migration process. Our team will work closely with you to ensure that your existing workflows and computations are seamlessly transferred to Wulver. We understand the importance of minimizing disruptions to your research activities, and our experts will provide guidance and support to address any compatibility issues that may arise. You can expect personalized assistance to make the transition as smooth as possible. If you have specific concerns about your workflows, please don't hesitate to reach out to our team for tailored support.</p>"},{"location":"migration/faqs/#what-specific-steps-should-i-take-to-prepare-for-the-migration","title":"What specific steps should I take to prepare for the migration?","text":"Answer <p>To prepare for the migration, there are a few crucial steps:</p> <ul> <li> <p>Provide a List of Current Students and Postdocs: </p> <ul> <li>Please share with us an updated list of current students, postdocs, and external collaborators who are actively using the HPC resources on Lochness. This information will ensure that user accounts are accurately migrated to Wulver, and access is maintained for the relevant individuals.</li> </ul> </li> <li> <p>List of Required Software: </p> <ul> <li>Compile a list of software applications that are essential for your research. This includes both commonly used software and any specialized tools unique to your work. Knowing your software requirements enables us to ensure that the necessary applications are available and properly configured on Wulver. You can find the list of software applications installed on Wulver in Software. If you don't find your applications in the list, submit a request for HPC Software Installation by visiting the Service Catalog.</li> </ul> </li> <li> <p>Planning for Former Students: </p> <ul> <li>If you have former students who may still have data or files on Lochness, it's essential to plan for their data migration or archival. We recommend reaching out to former students to coordinate any necessary data transfers or backups to avoid potential data loss.</li> </ul> </li> </ul> <p>These steps will contribute to a successful migration, allowing us to tailor the process to your specific needs. If you have any questions or need assistance with these preparations, please contact our support team.</p>"},{"location":"migration/faqs/#will-there-be-any-downtime-during-the-migration-and-how-will-it-affect-my-research-activities","title":"Will there be any downtime during the migration, and how will it affect my research activities?","text":"Answer <p>Once the migration has started for your group, Lochness will be inaccessible. If access to Lochness is critical for specific tasks during this period, we strongly encourage you to reach out to us. We understand that some users may have time-sensitive activities or dependencies on Lochness, and we are committed to working with you to find solutions that meet your needs. Please contact our support team by sending an email to hpc@njit.edu to discuss your specific requirements, and we will do our best to accommodate your situation during the migration process. You can check Contact Us to see how to create tickets with us and what information is required to create tickets. </p>"},{"location":"migration/faqs/#is-there-a-plan-for-backing-up-and-restoring-data-during-the-migration-process","title":"Is there a plan for backing up and restoring data during the migration process?","text":"Answer <p>Yes, there is a plan in place for data continuity. The data on Lochness resides on a shared filesystem, and these same filesystems will be mounted and available on Wulver post-migration. This approach ensures that your data remains accessible and seamlessly transfers to the new cluster.</p> <p>There is no need for a separate backup and restoration process as the shared filesystem continuity facilitates a smooth transition. If you have specific data-related concerns or requirements, please feel free to reach out to our support team (Contact Us) for further clarification and assistance. Your data integrity and accessibility are our top priorities throughout the migration process.</p>"},{"location":"migration/faqs/#what-changes-will-occur-in-the-file-directory-structure-especially-regarding-the-research-and-home-directories","title":"What changes will occur in the file directory structure, especially regarding the <code>/research</code> and <code>/home</code> directories?","text":"Answer <p>With the migration to Wulver, there will be changes in the file directory structure:</p> <p>Filesystems on Wulver: Wulver will have three filesystems available for use: <code>/home</code>, <code>/project</code>, and <code>/scratch</code>.</p> <ul> <li> <p>Availability of <code>/research</code> Directory: The <code>/research</code> directory from Lochness will be mounted on Wulver and will be available for use. This ensures continuity for research-related files and data.</p> </li> <li> <p>Lochness <code>/home</code> Directory on Wulver: The Lochness <code>/home</code> directory will be mounted on the Wulver login node as <code>/oldhome</code>. Users will be able to read files in this directory and move files from this directory but will not be able to write files into this deirectory. This allows users to access their personal home directories from Lochness during the migration period. The Lochness files under <code>/oldhome</code> will be available until 1-Jan-2025. Users should move needed files into <code>/project/PI_UCID</code> directory (replace PI_UCID with the UCID of PI). </p> </li> </ul> <p>These changes are designed to optimize the file organization on Wulver while maintaining accessibility to critical research data. The research facilitation team will work closely with users to ensure a smooth transition of data and assist in adapting to the new file directory structure. If you have specific questions or require assistance with data migration, please don't hesitate to contact our support team.</p>"},{"location":"migration/faqs/#how-will-the-migration-impact-access-to-specialized-software-or-applications-that-i-currently-use-on-lochness","title":"How will the migration impact access to specialized software or applications that I currently use on Lochness?","text":"Answer <p>The research facilitation team is dedicated to ensuring a smooth transition for users in terms of software and applications:</p> <ul> <li>Installation Support: <ul> <li>The research facilitation team will handle the installation of necessary software on Wulver, ensuring that essential tools and applications are available for your research. You can request for HPC Software Installation by visiting the Service Catalog. Please visit Software to see the list of applications isntalled on Wulver.</li> </ul> </li> <li>Code Compilation Assistance: <ul> <li>If your research involves custom code that needs compilation, the research facilitation team will provide assistance to ensure a successful compilation on Wulver. This support extends to helping users adapt their code to the new environment.</li> </ul> </li> </ul> <p>Our goal is to minimize any disruptions to your research activities and provide the necessary support for a seamless transition. If you have specific software requirements or need assistance with code compilation, please reach out to the research facilitation team, and they will be happy to assist you.</p>"},{"location":"migration/faqs/#can-i-continue-using-lochness-for-specific-tasks-during-the-migration-or-will-it-be-completely-inaccessible","title":"Can I continue using Lochness for specific tasks during the migration, or will it be completely inaccessible?","text":"Answer <p>Once the migration has started for your group, Lochness will be inaccessible. If access to Lochness is critical for specific tasks during this period, we strongly encourage you to reach out to us. We understand that some users may have time-sensitive activities or dependencies on Lochness, and we are committed to working with you to find solutions that meet your needs. Please contact our support team to discuss your specific requirements, and we will do our best to accommodate your situation during the migration process.</p>"},{"location":"migration/faqs/#what-happens-to-my-existing-job-submissions-and-queued-jobs-during-the-migration-process","title":"What happens to my existing job submissions and queued jobs during the migration process?","text":"Answer <p>For the most part, the migration will start after all running jobs have been completed. We understand the importance of job completion for ongoing research activities. Accommodations will be made for long-running jobs that cannot be checkpointed.   Our aim is to minimize disruptions to your computational tasks and ensure a smooth transition. If you have specific concerns about job submissions or if you anticipate long-running jobs during the migration period, please communicate with our support team. We are here to work collaboratively and make necessary accommodations to facilitate the completion of your jobs during the migration process.</p>"},{"location":"migration/faqs/#are-there-any-adjustments-needed-for-code-or-scripts-to-ensure-compatibility-with-wulver","title":"Are there any adjustments needed for code or scripts to ensure compatibility with Wulver?","text":"Answer <p>Yes, adjustments will be required for code or submission scripts to ensure compatibility with Wulver:</p> <ul> <li>Submit Script Changes: <ul> <li>Submit scripts will need to be modified to accommodate changes in partitions, hardware configurations, policies, and filesystems on Wulver. The research facilitation team will provide guidance and support in updating your submit scripts for seamless job submissions. Check the sample submit scipts for Wulver in SLURM.</li> </ul> </li> <li>Code Recompilation: <ul> <li>Due to differences in hardware, code may need to be recompiled to ensure optimal performance on Wulver. The research facilitation team is ready to assist you in this process, offering support to recompile code and address any related issues.</li> <li>If you code is compiled based on FOSS Toolchain (GCC and OpenMPI), you need to compile the code the same way you did in Lochness. Just make sure all the dependency libraries are installed on Wulver. Please visit Software to check the list of applications installed on Wulver.</li> <li>If your code is based on the Intel toolchain, you need to add the following while configuring your code.      <pre><code>./configure CFLAGS=\"-march=core-avx2\"\n</code></pre></li> <li>When installing codes, ensure that you perform the installation on the compute node rather than the login node. Since the hardware architecture is different on the login node, it's best practice to compile your code on the compute node. You need to initiate an interactive session with compute node before compiling your code.</li> </ul> </li> </ul> <p>Assistance will be provided to help you adapt your code and scripts to the new environment on Wulver. If you have specific concerns or require support in making these adjustments, please reach out to our research facilitation team, and they will work with you to ensure a smooth transition.</p>"},{"location":"migration/faqs/#will-there-be-training-sessions-or-documentation-available-to-help-faculty-and-researchers-transition-smoothly-to-wulver","title":"Will there be training sessions or documentation available to help faculty and researchers transition smoothly to Wulver?","text":"Answer <p>While there are no official training sessions scheduled at this point, comprehensive documentation is available at NJIT HPC Documentation to assist faculty and researchers during the transition to Wulver.   In addition to documentation, the research facilitation team is committed to providing personal assistance to faculty and researchers. If you have specific questions, require hands-on support, or need guidance on using Wulver effectively for your research, please do not hesitate to reach out to the research facilitation team. We are here to ensure that you receive the assistance you need for a successful transition.</p>"},{"location":"migration/faqs/#how-can-i-request-additional-resources-or-discuss-specific-requirements-for-my-research-projects-on-wulver","title":"How can I request additional resources or discuss specific requirements for my research projects on Wulver?","text":"Answer <p>To request additional resources or discuss specific requirements for your research projects on Wulver, please reach out to us at hpc@njit.edu. Our team is ready to assist you with any inquiries related to resource allocation, project needs, or any other aspects that can enhance your experience on the Wulver cluster. Your requests will be promptly addressed, and we are committed to providing the support necessary for the success of your research endeavors.</p>"},{"location":"migration/faqs/#what-support-services-will-be-available-during-and-after-the-migration-to-address-any-issues-or-concerns","title":"What support services will be available during and after the migration to address any issues or concerns?","text":"Answer <p>The research facilitation team is committed to providing personalized assistance and support services during and after the migration:</p> <ul> <li> <ul> <li>The research facilitation team is dedicated to offering personal assistance to each user. Whether you need help with data migration, code adjustments, or understanding the new environment, our team is here to provide tailored support.</li> </ul> <p>Personal Assistance:</p> <ul> <li>Issue Resolution:</li> <li> <p>Any issues or concerns that arise during or after the migration will be promptly addressed by the research facilitation team. We aim to ensure a smooth transition for all users and are ready to tackle any challenges that may arise.</p> </li> <li> <p>Ongoing Support:</p> </li> <li>Support services will continue to be available after the migration to address ongoing needs, answer questions, and assist with any further optimizations or adjustments required for your research projects.</li> </ul> </li> </ul> <p>Your success is our priority, and the research facilitation team is here to guide you through the migration process and beyond. If you encounter any issues or have specific concerns, please reach out to the team for personalized assistance.</p>"},{"location":"migration/faqs/#can-i-test-my-applications-or-simulations-on-wulver-before-the-migration-to-ensure-compatibility","title":"Can I test my applications or simulations on Wulver before the migration to ensure compatibility?","text":"Answer <p>Yes, absolutely! We encourage users to proactively test their applications or simulations on Wulver before the migration to ensure compatibility and identify any potential issues. This testing phase allows you to familiarize yourself with the new environment and address any concerns in advance.</p> <p>If you encounter challenges or have questions during the testing process, please don't hesitate to reach out to us. Our team is here to provide guidance, answer queries, and assist you in ensuring a smooth transition for your research activities on Wulver. Your proactive testing will contribute to a successful migration experience.</p>"},{"location":"migration/faqs/#how-long-will-the-oldhome-directory-on-lochness-be-accessible-after-the-migration-and-what-actions-should-i-take-regarding-data-stored-there","title":"How long will the <code>/oldhome</code> directory on Lochness be accessible after the migration, and what actions should I take regarding data stored there?","text":"Answer <p>The <code>/oldhome</code> directory on Lochness will be accessible for 6 months after the migration is complete. During this period, users are advised to review and move their data to other locations on Wulver or archive it as needed. This timeframe provides a reasonable window for users to organize and transfer their data while ensuring a smooth transition.</p> <p>If you have specific questions about data migration or need assistance during this post-migration period, please reach out to our support team. We are here to help you with any further steps or considerations related to your data on Lochness.</p>"},{"location":"migration/faqs/#will-afs-be-available-on-wulver","title":"Will AFS be available on Wulver?","text":"Answer <p>The AFS will not be available on Wulver. However we will setup a self-serve procedure for researchers to move AFS files to <code>/research</code>. We can do this as part of the full migration process. Please reach out to us at hpc@njit.edu for any questions.</p>"},{"location":"migration/faqs/#i-have-several-conda-environments-on-lochness-should-i-perform-a-fresh-installation-or-copy-the-environment-to-wulver","title":"I have several Conda environments on Lochness. Should I perform a fresh installation or copy the environment to Wulver?","text":"Answer <p>We recommend a fresh installation since the hardware architecture is different on Wulver. However, you can export the existing Conda environment to a YAML file, transfer it to Wulver, and then install the environment using this YAML file. See Conda Export Environment for details.</p>"},{"location":"migration/lochness_filesystem/","title":"Lochness to Wulver Migration: File Directory Changes","text":""},{"location":"migration/lochness_filesystem/#overview","title":"Overview","text":"<p>As part of the HPC cluster migration from Lochness to Wulver, there will be changes in the file directory structure to ensure a smooth transition. This documentation provides details on how the <code>/research</code> and home directories will be handled on the new Wulver cluster.</p>"},{"location":"migration/lochness_filesystem/#research-directory","title":"<code>/research</code> Directory","text":"<p>The <code>/research</code> directory from Lochness will be mounted on Wulver as <code>/research</code>. This ensures that users can seamlessly access their research-related files and data in the familiar directory structure. All content, including subdirectories and files, within <code>/research</code> on Lochness will be avaialble on the same location on Wulver.</p>"},{"location":"migration/lochness_filesystem/#home-directories","title":"Home Directories","text":""},{"location":"migration/lochness_filesystem/#lochness-home-directory","title":"Lochness Home Directory","text":"<p>The home directories from Lochness will be available on Wulver under the directory <code>/oldhome</code>. Users will find their personal home directories within <code>/oldhome</code>, allowing them to access their files and configurations.</p>"},{"location":"migration/lochness_filesystem/#locking-of-oldhome-directory","title":"Locking of <code>/oldhome</code> Directory","text":"<p>After the migration is completed, the <code>/oldhome</code> directory on Lochness will be locked. This means that no additional files or directories can be created in <code>/oldhome</code>. The locking of this directory is a crucial step to maintain the integrity of the migrated data and to ensure a stable environment on both clusters.</p> <p>Users are advised to review and transfer any necessary files from <code>/oldhome</code> on Lochness to their new home directories on Wulver during the migration period.</p>"},{"location":"migration/lochness_filesystem/#important-notes","title":"Important Notes","text":"<ul> <li>Migration Date: The migration process is scheduled to start on January 16, 2024.</li> <li>Locking Date for <code>/oldhome</code>: After the migration, the <code>/oldhome</code> directory on Lochness will be locked to prevent further modifications.</li> <li>File Access: Users will have continued access to their research files under <code>/research</code> on Wulver and their home directories under <code>/oldhome</code> on Lochness until the specified locking date.</li> </ul> <p>Please make the necessary adjustments to your workflows and ensure a smooth transition by reviewing and moving your files as needed. If you have any questions or concerns, feel free to reach out to our support team.</p>"},{"location":"news/","title":"Cluster Maintenance Updates and News","text":""},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/","title":"Relocation of Lochness to Databank Datacenter","text":"<p>Dear Lochness Users,</p> <p>We hope this message finds you well. We want to inform you of an upcoming significant event regarding our HPC (High-Performance Computing) cluster, lochness.njit.edu. The GITC datacenter is scheduled for demolition on November 1, 2023. In order to maintain the operation of our computing infrastructure, we will be relocating the cluster to the Databank colocation facility in Piscataway.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#key-details","title":"Key Details:","text":"<ul> <li>Cluster Shutdown Date: October 6<sup>th</sup>, 2023 at Noon</li> <li>Anticipated Duration: Up to Seven Days</li> <li>Operational Continuity: After the move the cluster will remain operational until the end of the semester.</li> <li>User Migration: We are actively working on migrating all users to the new cluster, wulver.njit.edu, before the end of the semester.</li> </ul>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#cluster-relocation-details","title":"Cluster Relocation Details:","text":"<p>The scheduled shutdown of the lochness.njit.edu cluster will take place on October 6<sup>th</sup>, 2023. We have estimated that the relocation process will require no longer than five days to complete. During this time, the cluster will not be accessible. We understand the importance of uninterrupted access to computational resources, and we will make every effort to minimize downtime.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#operational-continuity","title":"Operational Continuity:","text":"<p>Rest assured that we are committed to maintaining cluster availability for your research and academic needs. The lochness.njit.edu cluster will remain operational until the end of the current semester. This means that you will have access to its computing power throughout your ongoing projects and coursework.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#user-migration","title":"User Migration:","text":"<p>Our team is actively working on the migration process to ensure a smooth transition for all users. We plan to migrate all users to the new cluster, wulver.njit.edu, well before the end of the semester. Detailed instructions and support will be provided to facilitate this transition, and we will keep you updated on the migration progress.</p> <p>We understand that this relocation may raise questions or concerns, and we are here to address them. Please feel free to reach out to hpc@njit.edu you have any specific inquiries or require further information.</p> <p>We appreciate your understanding and cooperation during this transitional period. The relocation of our HPC cluster is aimed at providing you with an improved and more reliable computing environment.</p> <p>Thank you for your ongoing support and contributions to our research community.</p>"},{"location":"news/2023/10/14/wulver-maintenance/","title":"Wulver Maintenance","text":"<p>GPFS Fileset Changes</p> <p>Wulver will be out of service Wed Oct 18<sup>th</sup> between 9:00 am-11:00 am for updates and configuration changes. The maintence will be conducted to fix the <code>stale file handle</code> error on <code>/scratch</code> while accessing files from login node.</p>"},{"location":"news/2023/10/14/wulver-maintenance/#maintencance-plans","title":"Maintencance Plans","text":""},{"location":"news/2023/10/14/wulver-maintenance/#recommendation","title":"Recommendation:","text":"<ul> <li>Each fileset gets it\u2019s own inode namespace</li> <li>Fileset names to automatically inherit pool policies</li> <li>Additional fileset settings for chmod to not conflict with ACLs</li> </ul>"},{"location":"news/2023/10/14/wulver-maintenance/#migration-plan","title":"Migration Plan:","text":"<ul> <li>Create new filesets and link under /mmfs1/Scratch and /mmfs1/Project<ul> <li>New fileset names with sata1-project_xx and nvme1-scratch_xx (no bearing on FS path)</li> <li>New fileset have own inode spaces</li> </ul> </li> <li>Rsync data from old to new location</li> <li>Job outage for final copy and change</li> <li>Final rsyncs<ul> <li>Remove symlink for /mmfs1/scratch and create /mmfs1/scratch</li> <li>Unlink/relink filesets in new location</li> <li>Resolve any links remaining on nodes/images</li> </ul> </li> </ul>"},{"location":"news/2023/10/18/lochness-maintenance-updates/","title":"Lochness Maintenance Updates","text":"<p>Lochness is Back Online!</p> <p>Lochness is mostly back up after being moved to a new facility.  The move required complete disassembly and reassembly of the entire cluster. There are 8 nodes down as they were damaged in the move, repairs forthcoming. Infiniband network issues affect 50 nodes, these are in \"drain\" state. Currently 120 nodes are fully functional. You can use <code>sinfo</code> to see the exact states of nodes accessible to you. Please email hpc@njit.edu for assistance.</p>"},{"location":"news/2024/01/22/wulver-maintenance/","title":"Wulver Maintenance","text":"<p>Wulver Monthly Maintenance</p> <p>Beginning Feb 1, 2024, ARCS HPC will be instituting a monthly maintenance downtime on all HPC systems on the second Tuesday from 9AM - 9PM. Wulver and the associated GPFS storage will be taken out of service for maintenance, repairs, patches and upgrades. During the maintenance downtime, logins will be disabled, users will not have access to their stored data in <code>/project</code>, <code>/home</code> and <code>/scratch</code>. All jobs that do not end before 9AM will be held by the scheduler until the downtime is complete and the systems are returned to service.</p> <p>We anticipate maintenance to be completed by the scheduled time. However, occasionally the maintenance may be completed earlier than scheduled or could be extended to the following days. A notification will be sent to the user mailing list when the systems are returned to service or the maintenance window is extended. Additionally, users will encounter the cluster service information upon logging in to Wulver during maintenance. Please pay attention to the Message of the Day when logging in, as it will serve as a reminder for upcoming downtimes or other crucial cluster-related information. Users should take into account the maintenance window when scheduling jobs and developing plans to meet various deadlines. Please do not contact the help desk, HPC staff or open SNOW tickets for access to the cluster or data during the maintenance downtime.</p>"}]}